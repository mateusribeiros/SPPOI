Title: Toward Deep Semi-Supervised Continual Learning: A Unified Survey for Scalable and Adaptive AI

Abstract:
The integration of Deep Semi-Supervised Learning (DSSL) with Continual Learning (CL) holds significant promise for advancing artificial intelligence systems capable of learning from limited labeled data while continuously adapting to new tasks. This review explores recent progress in combining DSSL and CL, referred to as Deep Semi-Supervised Continual Learning (DSCL), focusing on the potential to develop models capable of learning efficiently in dynamic environments through both labeled and unlabeled data while mitigating catastrophic forgetting. Our analysis highlights several critical applications including image classification, cybersecurity, and natural language processing, while also identifying key challenges that prevent its broader adoption in real-world scenarios. Integration challenges such as catastrophic forgetting, handling noisy, unlabeled, and imbalanced data, and managing the stability-plasticity trade-off are discussed in detail. Moreover, the importance of open-world learning, lightweight architectures for on-device learning, enhanced scalability, and interpretable models is emphasized to ensure DSCL’s applicability in real-world, high-stakes domains. However, the current reliance on benchmark datasets, while valuable for evaluation, may limit generalization to complex tasks like medical imaging. To address these challenges, future research should prioritize leveraging domain-specific datasets to enhance real-world applicability, integrating transfer learning for better adaptability, and developing domain-agnostic frameworks and task-free continual learning. Additionally, exploring techniques like Reinforcement Learning from Human Feedback (RLHF) could enhance interpretability and trustworthiness. By addressing these gaps, DSCL can evolve to provide more flexible, scalable, and reliable solutions, contributing significantly to the development of adaptable and intelligent systems across diverse domains.

Graphical overview of key challenges and corresponding solution strategies in Deep Semi-Supervised Continual Learning (DSCL). The figure highlights five major integration challenges including catastrophic forgetting, stability-plasticity trade-off, handling unlabeled/noisy data, imbalanced data, and transferability alongside mitigation approaches proposed in recent research. This visual summary captures the core obstacles and emerging solutions for developing DSCL systems in dynamic environments.
Graphical overview of key challenges and corresponding solution strategies in Deep Semi-Supervised Continual Learning (DSCL). The figure highlights five major integration...Show More
Published in: IEEE Access ( Volume: 13)
Page(s): 60903 - 60929
Date of Publication: 31 March 2025 
Electronic ISSN: 2169-3536
DOI: 10.1109/ACCESS.2025.3556569
Publisher: IEEE
CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles and stipulations in the API documentation.
SECTION I.Introduction
The rapid advancements in deep learning (DL) have revolutionized numerous domains by achieving exceptional results, particularly in supervised learning tasks that rely on large, high-quality labeled datasets [1]. However, in many real-world applications, the availability of labeled data is often scarce, despite the continuous generation of vast amounts of unlabeled data. The process of obtaining labeled datasets is resource-intensive, requiring expert knowledge, substantial time, and considerable costs. Furthermore, the growing concerns surrounding data privacy and the high volume of unstructured data exacerbate this challenge, creating a bottleneck for the broader adoption of machine learning (ML) models. This challenge is particularly pronounced in deep learning due to the complexity of models that rely heavily on large datasets for generalization and performance.

To mitigate this data scarcity issue, Semi-Supervised Learning (SSL) has emerged as a promising paradigm that leverages both labeled and unlabeled data to improve model performance. SSL utilizes the latent structures within unlabeled data, reducing reliance on large labeled datasets, and thereby bridging the gap between limited data availability and the high model complexity [2] typical of DL models. Expanding on this foundation, Deep Semi-Supervised Learning (DSSL) has gained attention for its ability to scale SSL methodologies to deeper architectures, allowing for more efficient learning from unlabeled data.

However, real-world environments often require models not only to learn from limited labeled data but also to adapt continuously to new tasks without forgetting previous knowledge. This is where Continual Learning (CL) becomes crucial, as it enables models to evolve over time, addressing the problem of catastrophic forgetting. This research aims to survey the integration of DSSL with CL, with the goal of developing unified frameworks that enhance adaptability, generalization, and robustness in dynamic environments. This integration is essential for real-world applications where data distributions change over time, such as business intelligence and medical image classification, where models must continuously learn and evolve without sacrificing performance.

Despite the progress in DSSL and CL, significant research gaps remain. Existing studies primarily focus on either DSSL or CL independently, with limited exploration of their integration. The lack of unified frameworks capable of addressing dynamic learning challenges in real-world scenarios, such as evolving data distributions and resource constraints, hinders their practical adoption. Furthermore, scalability to large datasets, improving interpretability for high-stakes applications, and developing efficient strategies for open-world scenarios and on-device computing are underexplored areas. These gaps highlight the need for comprehensive research that bridges DSSL and CL, providing insights into scalable, adaptive, and robust learning systems.

In this survey, we explore the integration of DSSL and CL, with a focus on key advancements, methodologies, challenges, and unresolved issues in combining these learning paradigms. Our review highlights their contributions to addressing dynamic learning challenges and identifies research gaps and future directions in dynamic environments where data evolves over time. We explore the applicability of these methodologies across various domains, including medical image classification and business intelligence, while discussing the potential of DSSL and CL to address scalability, generalization, and adaptability in real-world scenarios. Specifically, this review highlights the potential of DSSL to mitigate catastrophic forgetting in CL models and investigate emerging frameworks that could enable continuous learning from both labeled and unlabeled data streams.

To the best of our knowledge, this is the first comprehensive review that explores the intersection of DSSL and CL, and the challenges associated with continual adaptation in real-world scenarios.

The main contributions of this paper are as follows: (i) We review the key methods for DSSL and CL approaches and highlight their strengths and limitations; (ii) we provide a detailed discussion on the integration of DSSL and CL, emphasizing how their complementary nature can be leveraged to develop more adaptive learning systems; (iii) we examine the latest datasets used to evaluate integrated DSCL models, discussing their characteristics and suitability for different tasks, including open-world and dynamic learning scenarios; (iv) we discuss the challenges of scalability, class imbalance, and catastrophic forgetting in integrated DSCL models, and review techniques such as transfer learning and model regularization to address these issues; and (v) we investigate emerging areas of research, such as lightweight frameworks for on-device learning and domain-agnostic models, to provide insights into future research directions.

This paper is organized as follows: Section II presents the research methodology, including the research questions, search strategy, inclusion and exclusion criteria, and article selection process; Section III discusses the core concepts of SSL and DSSL, including an overview of the key methods and approaches in DSSL; Section IV explores CL, covering its fundamental principles, the main approaches, and scenarios for real-world implementation; Section V provides an analysis of recent studies that integrate DSSL and CL, discussing the complementary nature of these paradigms and recent advances in this emerging field; Section VI identifies the challenges in integrating DSSL and CL and proposes future directions to address these issues. Section VII addresses the research questions and section VIII concludes the paper by summarizing the main findings and contributions.

SECTION II.Research Methodology
The methodology for this review paper follows a systematic approach designed to identify, select, and analyze relevant research on the integration of DSSL and CL. The following steps were undertaken to ensure a rigorous and transparent review process.

A. Research Questions
We began by formulating a set of research questions (RQs), followed by defining keywords and constructing search queries tailored to address these questions. The following RQs were formulated to guide the review:

RQ1: What are the key advancements and methodologies in DSSL?

RQ2: What are the fundamental concepts, methods, and scenarios that define CL, and how do they contribute to addressing dynamic learning challenges?

RQ3: How does integrating DSSL with CL benefit real-world scenarios, and what rationale supports their integration?

RQ4: What challenges are encountered when combining DSSL and CL, and what are the critical approaches used to address them in recent studies?

RQ5: What open issues and future directions remain in the development of DSCL systems, especially in dynamic learning environments?

B. Search Strategy
A comprehensive search strategy was implemented using combinations of keywords tailored to the research questions. Examples of keywords include: “semi-supervised learning,” “deep semi-supervised learning,” “continual learning,” “class imbalance,” “transfer learning,” “catastrophic forgetting,” and “dynamic data streams.”

Boolean operators (AND, OR) were used to construct queries targeting titles, abstracts, and keywords. To gather relevant literature, primary academic digital databases were utilized for their extensive coverage of machine learning and deep learning research to ensure the retrieval of high-quality literature including Google Scholar, IEEE Xplore, ACM Digital Library, SpringerLink, ScienceDirect, and Scopus.

C. Inclusion and Exclusion Criteria
To ensure the selection of relevant and high-quality studies and comprehensive coverage of the literature, we applied the following inclusion criteria:

Peer-reviewed journal articles and conference proceedings, along with the contributions that have shaped the development of this field.

Studies addressing DSSL, CL, or their integration.

Research presenting empirical studies, theoretical frameworks, or reviews related to the research questions.

Articles written in English.

Research published within the last ten years, with the inclusion of seminal works predating this timeframe for foundational context.

Unpublished data under review, conference abstracts, and grey literature such as short surveys were excluded. Additionally, we excluded studies lacking methodological detail or relevance to the research questions.
D. Article Selection Process
The PRISMA framework was applied for the process of article selection and the refinement of the research corpus to ensure transparency and replicability. The initial search yielded 642 articles across the selected databases. After removing duplicates and applying exclusion criteria including a preliminary title and abstract screening, 392 articles remained. A more in-depth full-text review resulted in the exclusion of 251 articles, leaving 141 research papers for detailed analysis. The final set of articles includes key contributions that directly address the research questions related to the integration of DSSL and CL. Fig. 1 illustrates the PRISMA flow diagram that summarizes the search process.

FIGURE 1. - PRISMA flow diagram summarizing the search and screening process.
FIGURE 1.
PRISMA flow diagram summarizing the search and screening process.

Show All

Fig. 2 provides an overview of the final set of articles, with 63 focused on DSSL, 67 on CL, and 11 on DSCL, offering insights into the current research landscape.

FIGURE 2. - Distribution of articles among research categories.
FIGURE 2.
Distribution of articles among research categories.

Show All

The studies in each category were systematically analyzed to examine the challenges they addressed, the models and frameworks they proposed, the datasets utilized, and the outcomes achieved. A structured analysis within each category of methods examines the strengths and limitations of the approaches and, where relevant, their potential applicability across different domains. This in-depth analysis also identified gaps in the existing literature, forming the basis for discussions on future research directions. The findings and insights derived from this analysis are presented in detail in the following sections.

SECTION III.Semi-Supervised Learning: From SSL to DSSL
This section addresses the first research question (RQ1) by examining the key advancements and methodologies in DSSL, beginning with an overview of SSL. It provides a structured review of progress in this area, with findings systematically presented to guide future research directions and potential integration scenarios with continual learning, which is further explored in subsequent sections.

A. Overview of Semi-Supervised Learning (SSL)
Consider a training dataset of input-target pairs of (x,y)∈D , sampled from an unknown joint distribution p(x,y) . In supervised learning, the goal is to produce a prediction model that produces the correct target y for previously unseen samples from p(x) . Now, suppose we also have a collection of unlabeled data points of x sampled from p(x) . The objective of SSL is to utilize the unlabeled data to enhance the prediction model by incorporating information about the structure of p(x) . One significant advantage of SSL is its potential to provide insights into the shape of the data distribution, leading to a more accurate estimation of the decision boundary between different target values.

Since the emergence of the idea of SSL in studies ([3]; [4]), a wide variety of SSL methods has been developed, including generative models, semi-supervised support vector machines, and graph-based methods [1]. As the field has progressed, the popularity of deep learning has naturally expanded into SSL, giving rise to deep semi-supervised learning (DSSL). DSSL methods build on the foundations of deep learning by incorporating both labeled and unlabeled data into the training process.

B. Deep Semi-Supervised Learning (DSSL) Methods
In this section, a comprehensive review of the key methods and approaches in DSSL is provided, with these methods broadly classified into five main categories. Fig. 3 illustrates a taxonomy of these categories.

FIGURE 3. - Taxonomy of key DSSL methods in the literature.
FIGURE 3.
Taxonomy of key DSSL methods in the literature.

Show All

1) Deep Generative Methods
Deep generative methods are designed to model the underlying distribution of the training dataset. By accurately capturing this distribution, these methods can generate synthetic samples that augment the existing data. This augmentation not only enhances the diversity of the training set but also improves the model’s generalization capabilities. The integration of generated data into DSSL is achieved in two primary ways: (1) Synthetic samples are utilized to augment labeled datasets, improving diversity and mitigating overfitting, particularly when labeled data is scarce. (2) Generated samples are aligned with the distribution of unlabeled data, enhancing the model’s ability to learn robust feature representations and meaningful decision boundaries. Prominent techniques in this category include Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs), both of which have demonstrated success in generating samples that capture the inherent variability and complexity of the input data.

The main structure of GANs, as introduced by Goodfellow et al. [5] in 2014, is composed of two core components: a generator (G) and a discriminator (D), both modeled as multilayer perceptrons (MLPs). G is responsible for learning the distribution pg over real data x and generating new synthetic samples G(z) that closely mimic the real data. It starts with a random noise z sampled from the prior distribution pz(z) and transforms it into a sample that looks like real data. Conversely, the discriminator’s role is to differentiate between real samples from the training data and the fake samples generated by the generator. It outputs a probability indicating whether a given sample is real. The generator and discriminator engage in a two-player game where the generator tries to fool the discriminator by creating samples that are as close as possible to real data. Meanwhile, the discriminator tries to get better at telling real and fake samples apart. This competition between G and D is modeled mathematically through a minimax game and represented by V(D,G) [5], as shown in (1). In this setup, the generator’s objective is to minimize a loss function, while the discriminator’s goal is to maximize it, leading to an adversarial process that improves the performance of both networks over time.
minGmaxDV(D,G)=Ex∼pdata(x)[logD(x)]+Ez∼pz(z)[log(1−D(G(z)))](1)
View SourceRight-click on figure for MathML and additional features.

GANs learn the distribution of real data, p(x), using unlabeled samples. This capability makes them particularly valuable for SSL, as it allows them to effectively leverage both labeled and unlabeled data in model development. Several adaptations of semi-supervised GANs have been proposed, each introducing distinct approaches. In the following discussion, we provide a comprehensive review of these methods.

Categorical GAN (CatGAN)[6] extends the traditional GAN framework by requiring the discriminator to not only distinguish between real and fake data but also to classify real data into predefined categories with high confidence. This approach incorporates an objective function that maximizes the mutual information between observed samples and their predicted categorical class distribution, while also enhancing the robustness of the classifier against adversarial perturbations.

Context-Conditional GAN (CCGAN)[7] employs in-painting with adversarial loss. In this approach, the generator fills in random missing patches within images, while the discriminator evaluates whether the in-painted images are real or fake. This process acts as a regularizer for the discriminator during supervised training. Additionally, the generator is conditioned to produce class-specific samples, and the discriminator assesses both their authenticity and class alignment.

Semi-supervised GAN (SGAN)[8] extends the traditional GAN framework to SSL by modifying the discriminator to output class labels. The discriminator is trained to classify inputs into one of the N+1 classes, where the additional class represents the generator’s synthetic outputs. This method improves traditional GANs by incorporating techniques such as feature matching, virtual batch normalization, and introducing a synthetic class specifically for generated samples.

GoodLearningBadGAN[9] addresses the challenge of simultaneously optimizing both the generator and discriminator in SSL. It introduces a “bad” generator that deliberately produces less realistic samples, which helps the discriminator improve its classification performance by refining the decision boundary between classes. This approach is grounded in the theoretical insight that a suboptimal generator can improve the discriminator’s ability to generalize.

Bidirectional GANs (BiGANs)[10] build upon the traditional GANs by incorporating an encoder that maps data back into the latent space z. This bidirectional approach allows the model to simultaneously learn both generation (from z to data space) and inference (from data space back to z), improving feature learning, particularly in unsupervised settings. The learned feature representations are also useful for auxiliary supervised tasks, making BiGANs competitive with contemporary unsupervised and self-supervised learning methods [10].

Localized GAN (LGAN)[11] learns local geometric transformations on a data manifold, unlike traditional GANs that parameterize the entire manifold globally. LGAN uses local coordinate charts to capture transformations at specific points, with local generators adapting to the geometry without needing global inversion. By enforcing orthonormality between tangent directions, LGAN prevents the manifold from collapsing into lower dimensions, reducing mode collapse. This approach trains a locally consistent classifier, with a regularizer closely related to the Laplace-Beltrami operator.

ConsistencyTerm GAN (CT-GAN)[12] integrates consistency regularization with Wasserstein GAN (WGAN) principles. It applies gradient penalties and uses dropout in the discriminator’s hidden layers to enforce 1-Lipschitz continuity, ensuring smoothness on the real data manifold and its surrounding regions. Additionally, CT-GAN incorporates temporal self-ensembling into the semi-supervised learning framework, using consistency regularization to improve the discriminator’s output stability.

Triple GAN (Triple-GAN)[13] introduces a third player, a classifier, to resolve the mismatch between generator and discriminator loss functions in traditional GANs. In this three-player minimax game, the classifier labels real data, the generator produces samples conditioned on class labels, and the discriminator distinguishes between real and fake image-label pairs. Several extensions of Triple-GAN have been proposed: Enhanced TGAN [14] improves semantic consistency by redesigning the generator loss and adding collaborative classifiers; MarginGAN [15] optimizes the margin between real and fake samples to improve semi-supervised classification accuracy; Triangle GAN [16] enables cross-domain learning with two generators and two discriminators to model relationships between domains; and Structured GAN [17] conditions generation on two independent latent variables to capture different aspects of joint data distributions.

VAEs [18] combine deep autoencoders with generative latent-variable methods. Unlike GANs, which focus on generating realistic samples that are indistinguishable from real ones, VAEs aim to capture the underlying data distribution to generate new data points that fit within this distribution. The VAE architecture consists of two main components: an encoder and a decoder. The encoder maps the input data x into the parameters of a latent distribution, typically producing the mean and variance. A latent variable z is then sampled from this distribution, which is guided by a prior distribution p(z) . The decoder subsequently attempts to reconstruct the original data by generating a distribution over possible data points p(x∣z) . During training, the model is optimized to ensure that the latent variable distribution q(z∣x) remains close to the prior p(z) . Evidence Lower Bound (ELBO), as shown in (2), is a key objective in VAEs that the model tries to maximize during training.
ELBO=logp(x)≥Eq(z|x)[logp(z)p(x|z)q(z|x)]=Eq(z|x)[logp(z)+logp(x|z)−logq(z|x)](2)
View SourceRight-click on figure for MathML and additional features.

In the following, the proposed methods, building upon VAEs, are presented that extended to SSL through deep generative modeling techniques.

M1[19] model focuses on learning a latent-feature discriminative model where the latent variable z (often modeled as Gaussian or Bernoulli) is learned from the data. This learned latent variable serves as a lower-dimensional feature representation for classification tasks. In the M2 [19] model introduces both a latent class variable y (for class labels) and a continuous latent variable z to describe the data. This allows for joint modeling of both labeled and unlabeled data. The class labels are treated as latent variables when missing, thereby the model can infer the missing labels by marginalizing them over the possible class labels. The M1+ M2 [19] stacked model combines M1 and M2, using the latent representation learned from M1 as input for M2, which further refines the generative process.

Auxiliary Deep Generative Models (ADGM)[20] extend the M1 and M2 models by introducing an auxiliary variable a to improve the variational approximation. This auxiliary variable enhances the flexibility of the inference model q(z∣x) by making the posterior distribution more expressive. This addition improves the model’s representational power by introducing a class-specific latent distribution between input data x and class labels y. Skip Deep Generative Models (SDGM) [20] build on ADGM by adding skip connections between the stochastic layers and labels that result in improving training efficiency via stochastic gradient descent. The model allows the auxiliary variable a to influence both the latent variable z and the observed data x, thereby increasing the model’s representational power.

Infinite Variational Autoencoder (Infinite VAE)[21] integrates non-parametric Bayesian methods through a Dirichlet process, forming an infinite mixture of VAEs. This approach enables the model to dynamically adjust the number of components which allows it to capture the complexity of the data more effectively. Using a combination of Gibbs sampling and variational inference, infinite VAE enhances the model’s flexibility in representing diverse latent structures.

Disentangled Variational Autoencoder (Disentangled VAE)[22] aims to learn disentangled representations by separating different factors of variation in the data across distinct latent variables. Through importance sampling and a partially specified graphical model, this method maximizes the likelihood for both supervised and semi-supervised data, providing a clearer separation of the factors that influence the observed data.

Semi-supervised Disentangled VAE (SDVAE)[23] builds on Disentangled VAE by incorporating label information into the disentangled latent representations. It separates the latent space into categorical (disentangled) and non-interpretable components, ensuring that these variables independently capture different information from the data.

Reparameterized VAE (ReVAE)[24] introduces auxiliary variables to capture label-specific information more effectively. By partitioning the latent space, ReVAE isolates label-relevant features from other generative factors, enhancing classification performance while preserving generative capabilities. This structured design enables ReVAE to generate diverse conditional samples and perform fine-grained interventions, achieving a balance between discriminative accuracy and generative quality in SSL tasks.

Analysis. Deep generative methods effectively utilize unlabeled data by modeling complex distributions, generating realistic synthetic data, and enhancing feature representations. Semi-supervised GANs, for instance, incorporate generated data by introducing an additional “fake” class, enhancing discriminator training. In contrast, models like M1+ M2 VAEs utilize generative modeling to structure the latent space, improving label inference from unlabeled data. These approaches improve model robustness and generalization, particularly when labeled data is limited. However, they often come with high computational costs and can be challenging to train due to issues such as mode collapse in GANs and the complexity of optimizing variational bounds in VAEs. Furthermore, the quality of the generated data can significantly impact the performance, and poor-quality samples might introduce noise rather than improve learning.

2) Consistency Regularization
Consistency regularization methods aim to ensure that models produce stable predictions when subjected to various perturbations in the input, model parameters, or network structure [1], while simultaneously reducing uncertainty in these predictions. By incorporating consistency constraints into the loss function, these methods improve the model’s robustness to real-world variations in the input distribution. Typically, these techniques follow a Teacher-Student framework, where the teacher model generates targets for the student. The objective is to minimize the difference in student’s predictions across different perturbations, thereby enhancing stability and generalization under various conditions. Consistency-based approaches incorporate labeled data by applying perturbations (e.g., by adding noise or applying augmentations), and the model is trained to produce stable predictions across these variations. Similarly, unlabeled data is subjected to perturbations, and the model minimizes the difference between predictions on the original and perturbed inputs. This process allows the model to leverage unlabeled data effectively, improving its understanding of the underlying data distribution.

Ladder Network[25] utilizes a denoising autoencoder architecture, where noise is introduced into the hidden layers of the encoder. The decoder then reconstructs the clean activations, and the consistency loss is computed by comparing these reconstructions with the original clean inputs, ensuring robust and consistent feature representations across the network.

Π -Model[26] applies random noise or augmentations to both labeled and unlabeled data, introducing perturbations through data augmentation and dropout. The model then minimizes the difference between predictions for these augmented inputs.

Temporal Ensembling[27] improves the Π Model by addressing the instability of target predictions through the use of an Exponential Moving Average (EMA) of past predictions as the consistency target. The model is encouraged to produce similar predictions over time for the same unlabeled data. Building on this concept, Mean Teacher [28] further refines the approach by averaging model weights over training steps using EMA. This results in a more reliable teacher model, with a consistency constraint applied between the student model’s predictions and those of the EMA-weighted teacher.

Virtual Adversarial Training (VAT)[29] enhances model robustness by approximating minimal perturbations to input data x that significantly alter the model’s output to challenge the model. Inspired by adversarial examples, VAT creates virtually imperceptible changes to data points that can cause misclassification.

Dual Student[30] extends the Mean Teacher framework by replacing the teacher with another student model, each optimized independently, thereby avoiding the performance bottleneck associated with a coupled EMA Teacher-Student model.

Stochastic Weight Averaging (SWA)[31] improves generalization by averaging model weights across different stages of training with cyclical learning rates, approximating the Teacher-Student framework within a single model.

Virtual Adversarial Dropout (VAdD)[32] integrates adversarial training with dropout, applying an adversarial dropout mask to create challenging perturbations, and enforcing consistency between the predictions from randomly and adversarially dropped networks.

Worst-case Perturbation (WCP)[33] focuses on enhancing network resilience by applying additive noise and DropConnect perturbations to identify and strengthen the model’s most vulnerable weights and connections.

Unsupervised Data Augmentation (UDA)[34] leverages advanced data augmentation techniques like AutoAugment and Back-Translation to introduce high-quality perturbations, maintaining consistency in the model’s predictions across these augmented inputs.

Analysis. Consistency regularization methods are strong in that they enhance the generalization ability of models by enforcing consistency between predictions under different perturbations. These methods are particularly effective when labeled data is scarce, as they reduce dependency on large labeled datasets while improving generalization. However, their weakness lies in their sensitivity to the choice of perturbations and hyperparameters, which can be challenging to tune. Additionally, if the model is overly confident in incorrect predictions, consistency regularization may reinforce these errors, leading to suboptimal performance.

3) Graph Semi-Supervised Learning (GSSL)
Graph-based methods construct a similarity graph from raw datasets, where each node represents a training sample, and edges are weighted based on the similarity between node pairs. The core principle of these methods is the manifold assumption, which suggests that data points close to one another in high-dimensional space are likely to share the same label. This graph enables label propagation from labeled to unlabeled samples, facilitating label inference for the latter. These methods are particularly effective in scenarios where data exhibits a natural clustering structure. Deep embedding approaches within this framework can be broadly divided into two main categories, AutoEncoder-based methods and Graph Neural Network (GNN)-based methods as detailed below.

AutoEncoder-based methods aim to preserve both local and global graph structures by learning embeddings. Each node is represented by a high-dimensional vector derived from the graph’s similarity matrix. The autoencoder encodes this vector into a lower-dimensional latent space and then attempts to reconstruct it, intending to minimize the reconstruction error between the original and reconstructed vectors. This approach incorporates local structural information into the embeddings, which shallow methods are often unable to capture.

Structural Deep Network Embedding (SDNE)[35] is designed to capture highly non-linear structures in a network using a deep autoencoder. It preserves both first-order proximity, which reflects the direct similarity between node pairs, and second-order proximity, which represents the similarity in their neighborhood structures. The unsupervised component of the model focuses on global structure by reconstructing each node’s neighborhood, while the supervised component uses Laplacian Eigenmaps to ensure that connected nodes remain close in the embedding space.

Deep Neural Graph Representations (DNGR)[36] combine random surfing with autoencoders. The model comprises three key components: random surfing, Positive Pointwise Mutual Information (PPMI) estimation, and stacked denoising autoencoders. The random surfing process generates a stochastic matrix similar to the similarity measure matrix used in HOPE [37], which is then transformed into a PPMI matrix. This matrix is input into a stacked denoising autoencoder to derive the final node embeddings, effectively capturing higher-order proximities and making the model robust to noise.

Graph Auto-Encoders and Variational Graph Auto-Encoders (GAE&VGAE). GAE and VGAE [38] are inspired by traditional autoencoders. GAE learns node representations by encoding the graph’s structure into a latent space and reconstructing it via a decoder. The encoder uses graph convolutional layers (GraphConv) to process the adjacency matrix VGAE extends this by optimizing the variational lower bound, directly reconstructing the adjacency matrix while regularizing the latent space through a Kullback-Leibler divergence term.

Deep Recursive Network Embedding (DRNE). DRNE [39] introduces a recursive approach to node embedding by using LSTM-based Auto-Encoders. Unlike previous methods that utilize MLPs, DRNE captures the recursive structure of the graph by aggregating the embeddings of a node’s neighbors, preserving local structures over iterative updates.

Adversarially Regularized Graph Auto-Encoders (ARGA). ARGA and their variational counterpart ARVGA [40] employ a variant of Graph Convolutional Networks (GCN) [41] as encoders. These models incorporate adversarial training principles by enforcing a prior distribution over the latent space, typically Gaussian or uniform, to regularize the embeddings.

Various deep embedding strategies have been developed to address the limitations of autoencoder-based methods by focusing on the local structure of nodes rather than the entire graph. GNN-based methods typically rely on two core operations: the aggregation operation and the update operation [1]. The foundational GNN framework employs an iterative message-passing mechanism, where each node gathers information from its neighbors and updates its embedding based on this aggregated data. This process allows for the integration of local neighborhood information into the node’s representation. Several GNN-based deep embedding methods are outlined below.

Graph Convolutional Network (GCN)[41] employs symmetric normalization to efficiently aggregate features from a node’s neighbors, capturing the uniform neighborhood structure. This approach facilitates the learning of smooth representations that generalize well across similar nodes.

Simple Graph Convolution (SGC)[42] simplifies the GCN model by eliminating non-linearities between layers, focusing instead on linear transformations. This reduction in complexity allows for faster computations while maintaining the effectiveness of feature aggregation.

MixHop[43] enhances GCN by learning complex neighborhood relationships through the repeated mixing of feature representations from neighbors at varying distances. By incorporating higher-order message passing, MixHop captures a broader range of connections within the graph.

GraphSAGE[44] mitigates the over-smoothing issue in GNNs by employing skip connections that preserve information from earlier layers. This approach concatenates previous layer representations with newly aggregated information, ensuring richer node embeddings.

Deep Graph Networks (DGN)[45] introduce a novel approach to reducing over-smoothing by normalizing groups of nodes independently across successive layers. This group-specific normalization helps in retaining diverse node features while enhancing layer-wise learning.

Graph Attention Network (GAT)[46] incorporates an attention mechanism that assigns varying weights to a node’s neighbors based on their importance. This weighted aggregation allows for more nuanced and informative node representations.

Jumping Knowledge Network (JK Net)[47] improves node representations by combining the outputs from all layers of the GNN, rather than relying solely on the final layer. This aggregation across layers captures a wider range of features, enhancing the model’s overall performance.

Contrastive Graph Poisson Networks (CGPN)[48] leverages a flexible label propagation approach that utilizes both structural and feature information for label inference. By integrating contrastive learning within a variational framework, CGPN effectively exploits unlabeled data to improve model training.

Analysis. GSSL methods excel in handling graph-structured data, offering robust performance for tasks like label inference in SSL. Their strengths lie in effectively capturing relationships within the graph, allowing for improved generalization and the incorporation of local and global structures. However, these methods face challenges with scalability, especially for large and evolving graphs, and can be computationally expensive [49]. They can struggle with noise or missing attributes. Additionally, many GSSL methods lack interpretability and are vulnerable to adversarial attacks, limiting their reliability in real-world applications.

4) Pseudo-Labeling
Pseudo-labeling methods assign labels to unlabeled samples based on the predictions of a trained model with high confidence. In some approaches, label propagation is employed, where labels are propagated from labeled to unlabeled samples by using strategies such as nearest-neighbor matching to assign labels based on feature space similarity [50]. However, In this study, we focus on model prediction-based approaches. Pseudo-labels are then treated as ground truth in subsequent training iterations, effectively converting the SSL task into a supervised one. This process can be seen as a form of bootstrapping, where the model iteratively refines its predictions and leverages the unlabeled data to enhance performance. When the model’s predictions are accurate, this approach is highly advantageous, as it enables the efficient utilization of large volumes of unlabeled data. These methods are broadly categorized into Disagreement-based Models and Self-training Models.

Zhou and Li [51] were the first to propose the use of multiple learners for a task, where the disagreements among the learners are leveraged to enhance the SSL process.

Deep Co-Training[52] employs two complementary views of the data to train separate classifiers. By enforcing consistency between their predictions and incorporating constraints based on the differences between the views, this approach enhances model robustness and improves overall performance.

Tri-Net[53] employs three classifiers, each trained on different bootstrap samples. It incorporates output smearing to smooth predictions and enforces consistency among predictions, which helps augment the training set and enhance model diversity.

Self-training models generate pseudo-labels for unlabeled data by relying on the model’s confident predictions, which are subsequently used to further refine the model. Key techniques within this approach are outlined below.

Pseudo-Label. This technique [54] uses labeled samples to train a model and subsequently assigns pseudo-labels to unlabeled data based on the model’s maximum confidence predictions. These pseudo-labeled samples are then combined with the labeled data for further supervised training.

Noisy Student. This training approach [55] improves model performance by combining self-training, knowledge distillation (KD), and noise injection. The process begins with a teacher model trained on labeled data, which then generates pseudo-labels for the unlabeled data. A larger student model is trained using both the labeled and pseudo-labeled data. Noise is added to the student model during training through techniques like dropout, stochastic depth, and data augmentation. These noise mechanisms help the student model generalize better than the teacher. The process is iterative, with the student becoming the new teacher in each cycle.

Self-Supervised Semi-Supervised Learning (S4L). This framework [56] incorporates two self-supervised learning techniques, such as image rotation and exemplar learning, to learn useful feature representations from unlabeled data. These techniques are integrated into a semi-supervised learning approach, improving performance with both labeled and unlabeled data.

Meta Pseudo Labels (MPL)[57] uses a teacher model to generate conditional class distributions for training a student model. Pseudo-labels are iteratively refined based on the student’s validation performance, improving overall model accuracy. Unlike traditional pseudo-labeling, where the teacher remains fixed, MPL continuously updates the teacher based on feedback from the student’s performance on labeled data. This dynamic feedback loop helps the teacher generate more accurate pseudo-labels.

Ensemble of Auto-Encoding Transformations (EnAET)[58] combines spatial and non-spatial transformations within an autoencoder framework to learn robust feature representations and generate pseudo-labels. By applying a variety of transformations, such as projective, affine, and color adjustments, EnAET enforces consistency in predictions and enhances the generalization of SSL models. This framework acts as a regularization mechanism that can be integrated into existing semi-supervised approaches to improve their performance.

Analysis. Pseudo-labeling methods offer several advantages, including simplicity, scalability, and the ability to effectively leverage large volumes of unlabeled data. By iteratively refining the model with pseudo-labels, performance can be incrementally improved, and the approach can be easily combined with other techniques like data augmentation. However, they carry the risk of confirmation bias, where incorrect pseudo-labels can reinforce model errors and rely heavily on the quality of the initial model. It also depends on careful tuning of confidence thresholds to ensure reliable label generation. Furthermore, the iterative training process can be computationally intensive, particularly when applied to large-scale datasets.

5) Hybrid Methods
Hybrid methods integrate multiple techniques such as consistency regularization, entropy minimization, pseudo-labeling, data augmentation, and distribution alignment. By blending these approaches, hybrid methods enhance model robustness and adaptability that can effectively leverage both labeled and unlabeled data. This integration helps mitigate the limitations inherent in individual techniques, improves generalization, and allows models to adapt to diverse data conditions, making hybrid methods particularly effective in scenarios with limited labeled data.

Interpolation Consistency Training (ICT)[59] encourages smooth and consistent model predictions by applying the principle of interpolation to unlabeled data. ICT mixes two unlabeled data points and trains the model to predict the label of the interpolated point as a blend of the predictions of the original points. This method enforces the decision boundary to lie in low-density regions of the data distribution. By doing so, ICT improves model generalization and reduces overfitting, especially in scenarios where labeled data is limited, by leveraging the structural information from the unlabeled data.

MixMatch[60] combines consistency regularization and entropy minimization. It applies data augmentation to both labeled and unlabeled data and generates pseudo-labels by averaging the model’s predictions over multiple stochastic augmentations. These averaged pseudo-labels are then “sharpened,” adjusting the distribution, to reduce entropy, encouraging more confident predictions on unlabeled data. Additionally, MixMatch employs the MixUp technique, which blends labeled and unlabeled examples to create virtual training data. This integration of methods results in improved generalization.

ReMixMatch[61] builds on MixMatch by introducing two key enhancements: distribution alignment and augmentation anchoring. Distribution alignment ensures that the marginal distribution of predictions on unlabeled data matches the true label distribution. Augmentation anchoring replaces the consistency regularization component of MixMatch by generating multiple strongly augmented versions of each unlabeled example and anchoring their predictions to the model’s output on a weakly augmented version. These augmentations improve the model’s robustness and generalization, making ReMixMatch more data-efficient and able to achieve high accuracy with significantly fewer labeled examples compared to previous approaches.

DivideMix[62] further refines the MixMatch framework by addressing the challenge of noisy labels in training data. It introduces two key components: co-refinement and co-guessing, which allow two networks to simultaneously filter out noisy samples and refine labels. A Gaussian Mixture Model (GMM) is used to dynamically separate clean labeled samples from noisy unlabeled ones, ensuring that the model trains on reliable data. This approach helps to avoid overfitting to incorrect labels and improves generalization by leveraging noisy samples in a semi-supervised manner.

FixMatch[63] simplifies the SSL pipeline by combining two key techniques: consistency regularization and pseudo-labeling. In FixMatch, pseudo-labels are generated from the model’s predictions on weakly augmented unlabeled data. These pseudo-labels are retained only if the model’s prediction surpasses a confidence threshold. The model is then trained to match its predictions on a strongly augmented version of the same image to the pseudo-label, minimizing the discrepancy between the weakly and strongly augmented versions.

FlexMatch[64], an extension of FixMatch, enhances the pseudo-labeling process by introducing Curriculum pseudo-labeling (CPL), which dynamically adjusts the thresholds for each class based on the model’s learning progress. This adaptive thresholding mechanism ensures more effective utilization of unlabeled data by calibrating the learning process to suit different classes, allowing the model to focus on harder-to-learn classes at different stages of training.

Analysis. While hybrid models offer significant advantages by integrating multiple techniques, they also introduce added complexity. These models often require sophisticated and computationally intensive training pipelines, which can be challenging to optimize. Achieving a balance between the different components is crucial, as the combination of techniques can introduce noise, instability, or even overfitting if not carefully managed. Effective regularization and tuning strategies are essential to mitigate these risks and ensure the model’s overall performance and stability.

C. Comparative Analysis of DSSL Methods
Table 1 presents a comparative evaluation of key DSSL methods, highlighting their advantages, limitations, and relevance to DSCL.

TABLE 1 Comparative Analysis of DSSL Methods
Table 1- Comparative Analysis of DSSL Methods
SECTION IV.Continual Learning (CL)
This section addresses the second research question (RQ2) by examining the fundamental concepts, methods, and scenarios that define CL. The discussion explores how these approaches facilitate adaptive learning in dynamic environments and help overcome challenges associated with evolving tasks and data distributions. Key findings are systematically presented to highlight the role of CL in addressing these challenges.

A. Fundamental Concepts of CL
To effectively address the complexities of real-world environments, AI systems are required to dynamically and incrementally acquire, update, and utilize knowledge and information, a process known as continual learning (CL). Continual learning enables systems to learn from new data over time without forgetting previously learned information, a challenge often referred to as catastrophic forgetting. This adaptive ability allows intelligent systems to seamlessly adjust to changes in real-world scenarios. In the literature, continual learning is also referred to as incremental learning or lifelong learning, reflecting the ongoing and cumulative nature of knowledge acquisition [65].

In practice, continual learning involves adapting to dynamic and evolving data distributions, where training samples are presented sequentially. A basic approach to mitigating catastrophic forgetting in continual learning is to retain and reuse all previous training samples. However, this strategy comes with substantial computational and storage overheads, along with potential privacy concerns. As highlighted in the literature [65], a CL model needs to learn from incoming batches of training samples (Xt,Yt)∈Dt corresponding to task t, with minimal to no access to previous training samples, while still maintaining strong performance on their respective test sets. Under more realistic conditions, the data labels yt and task identity t may not always be available, which presents additional challenges for CL approaches in maintaining accuracy and robustness. To address these challenges, continual learning can be integrated with various learning paradigms depending on the availability labeled and unlabeled data in each dataset Dt . For instance, zero-shot learning [66] allows the model to handle entirely new tasks without prior exposure. Similarly, few-shot learning [67] enhances the system’s ability to learn from a small number of samples, where acquiring large labeled datasets may be impractical or costly. Semi-supervised learning [68] further strengthens continual learning by making efficient use of limited labeled data while utilizing abundant unlabeled data. Unsupervised learning [69] and self-supervised learning [70] do not require labeled data. These methods offer flexible strategies for adapting continual learning to a wide range of tasks and data constraints.

B. Methods in CL
In recent years, various methods have been proposed to address the challenges of continual learning. Fig. 4 illustrates the most representative approaches, organized into distinct categories based on their underlying mechanisms in the literature.

FIGURE 4. - Taxonomy of key CL approaches in the literature.
FIGURE 4.
Taxonomy of key CL approaches in the literature.

Show All

1) Architectural-Based Methods
Architectural-based methods primarily involve modifying or expanding the network structure to prevent task interference and preserve knowledge.

Parameter Isolation. These methods allocate a distinct subset of parameters for each task to prevent interference. Fixed approaches, like WSN [71], use binary masks to freeze specific neurons or parameters after learning a task. This prevents overwriting by future tasks while keeping the rest of the network adaptable. The isolated parameters remain nearly static for previous tasks, ensuring that learned knowledge is preserved. Dynamic approaches expand the network in response to the need for additional capacity and give rise to Dynamic Architecture Expansion techniques. Techniques such as reinforcement learning, neural architecture search, and variational Bayes [65] dynamically adapt the network’s architecture, adding layers or neurons as task complexity increases to enhance parameter efficiency and improve knowledge transfer. However, these methods must balance expansion rates to avoid overgrowth. The challenge lies in ensuring scalability while maintaining sparsity and reusability, as over-expansion can lead to inefficient models.

Modular Networks utilize parallel sub-networks or sub-modules to learn tasks independently, promoting efficient knowledge transfer while preventing interference. Expert Gates [72] assign tasks to specialized experts through a gating mechanism, utilizing task-specific autoencoders to enhance knowledge transfer. By comparing reconstruction errors, the gating mechanism directs samples to the most appropriate expert, reducing task interference while enabling knowledge sharing among related tasks. RPSNet [73] pre-allocates multiple parallel modules within each layer and dynamically selects an optimal path of modules for each task using a random path selection strategy. These paths are progressively combined across tasks, allowing shared use of previously learned paths to facilitate forward knowledge transfer. LMC [74] optimizes configurations by integrating functional and structural components within each module, dynamically selecting and expanding sub-modules based on their relevance and performance for the given task. CAF [75] introduces a multi-learner architecture inspired by biological intelligence, where multiple learning modules are coordinated to balance memory stability and learning plasticity.

Model Decomposition. These methods decompose the model into shared and task-specific components. The parallel branching technique [76] introduces separate branches for different tasks, allowing each branch to learn task-specific features independently. Another approach employs adaptive layers [77], where layers adjust dynamically to meet the specific needs of each task. It uses intermediate feature masks, which apply task-specific masks to feature representations at intermediate layers, distinct from binary parameter-space masks. Moreover, some methods use parameter decomposition techniques that offer another layer of flexibility, breaking down parameters into shared and task-specific elements. Methods like additive decomposition, singular value decomposition, filter atom decomposition, and low-rank factorization [65] facilitate this separation. While these decomposition methods can expand with the number of tasks, their scalability is contingent on efficient resource management and computational cost.

Analysis. Architectural-based methods excel at preventing task interference and preserving learned knowledge in CL systems. They offer adaptability and facilitate knowledge transfer. However, these methods face challenges with scalability and resource efficiency, as dynamic expansion can lead to inflated architectures, and managing multiple sub-networks or components increases computational complexity. Balancing flexibility with model efficiency and avoiding overgrowth remains a key limitation in their practical application.

2) Regularization-Based Methods
Regularization-based approaches in continual learning aim to balance the learning of new tasks while retaining old knowledge by adding explicit regularization terms to the loss function. These methods often require a frozen copy of the old model as a reference to guide updates.

Penalization Strategies (Weight Regularization). The first approach is Weight Regularization. This approach selectively penalizes changes in network parameters that are important for previously learned tasks. These methods add a regularization term to the loss function, typically in the form of a quadratic penalty, where the penalty weight is determined by the importance of each parameter. One of the well-known approaches in this category is Elastic Weight Consolidation (EWC) [78] which calculates parameter importance using the Fisher Information Matrix (FIM). However, EWC can be computationally expensive due to the complexity of FIM approximation, and more recent methods aim to mitigate this issue. Synaptic Intelligence (SI) [79] approximates parameter importance by evaluating each parameter’s contribution to the overall loss during training. Memory Aware Synapses (MAS) [80] accumulate importance measures by evaluating the sensitivity of predictive results to variations in model parameters. This allows the model to identify and prioritize parameters that are more critical to its predictions. RWalk [81] combines elements of EWC and SI, offering a hybrid approach that incorporates both regularization methods for improved task retention. Some methods refine the quadratic penalty implementation. For instance, R-EWC [82] improves EWC by factorizing and rotating the parameter space to better preserve old task information. XK-FAC [83] further considers inter-example relations in FIM approximation to accommodate batch normalization.

Function Regularization. This method focuses on preserving the functionality of the model by ensuring that the output predictions for old tasks are retained while learning new tasks. This approach typically employs Knowledge Distillation (KD), where a previously trained model “teacher” provides soft labels or predictions that guide the learning of the new model “student.” Learning without Forgetting (LwF) [84] is a foundational method in this category. It preserves knowledge from previous tasks by using the output of the old model as a distillation target for the new task. More advanced versions, such as LwF.MC [85] and Learning without Memorizing (LwM) [86] improve this process by incorporating attention maps or additional feature-based distillation to better align the new model with the old one. This method is particularly effective when combined with replay methods, where a subset of old data is retained and used during training to further reinforce the knowledge of old tasks. Another form of function regularization is Sequential Bayesian inference over the function space, which also aims to retain knowledge from previous tasks but does so by constructing a posterior distribution over task-specific functions rather than distilling outputs from a previous model. Unlike KD, which relies on transferring soft labels, Bayesian inference-based methods like FRCL [87], FROMP [88], and S-FSVI [89] store a compressed summary of past tasks (such as inducing points) and regularize the function space directly through posterior distributions. This probabilistic approach allows for a more principled way of mitigating catastrophic forgetting, especially when handling uncertainty in task-specific functions. These methods can complement or replace replay strategies by maintaining functional knowledge without needing to store large amounts of previous data.

Some regularization methods extend beyond basic weight or function regularization. Incremental Moment Matching (IMM) [90] aligns posterior distributions of old and new tasks to balance stability and plasticity, while ResCL [91] refines this by learning a combination coefficient. Progress & Compress (P&C) [92] distills knowledge from separate task-specific networks into a main model. In Neuron Importance and Selective Freezing, methods like NPC [93] adjust learning rates based on neuron importance, and UCL [94] and AGS-CL [95] go further by freezing key parameters to preserve past knowledge.

Analysis. While weight regularization methods are effective at reducing catastrophic forgetting, they often struggle with scalability, especially when applied to models with a large number of tasks or parameters. Function regularization approaches focus on preserving model outputs, offering better scalability.

3) Replay-Based (Rehearsal) Methods
These methods store a subset of previous data and replay it during training on new tasks to mitigate catastrophic forgetting. They aim to approximate and recover old data distributions and can be divided into three subcategories.

Experience (Exact) Replay. This approach stores a fixed number of old training samples in a memory buffer to retain past knowledge. The key challenge lies in how to construct and manage this memory buffer efficiently [65]. Memory Construction: Early methods like Reservoir Sampling [96] randomly stored a fixed number of samples from each batch. More advanced techniques employ optimized strategies to enhance sample diversity, task performance, and robustness. For example, AQM [97] compresses data using a VQ-VAE framework, while MRDC [98] uses determinantal point processes (DPPs) for efficient compression. Augmentation techniques like RM [99] and RAR [100] further increase the diversity of stored samples. Memory Utilization: Effective utilization of stored samples is crucial for preventing forgetting and improving knowledge transfer. Methods such as GEM [101] and A-GEM [102] use old samples to constrain gradient updates, ensuring that learning new tasks does not increase the loss on previous tasks. MER [103] applies meta-learning to align gradients for better learning across tasks, while MIR [104] prioritizes samples that are more prone to forgetting. iCaRL [85] and EEIL [105] integrate knowledge distillation with experience replay to balance data from old and new tasks, addressing data imbalance and representation shifts.

Generative Replay. Instead of storing real data, generative replay uses models like GANs or VAEs to generate synthetic data that mimics previously learned tasks. This approach maintains past knowledge without requiring actual old samples. DGR [106] is a foundational method, where a generative model replays old tasks by generating samples from the previous task’s distribution. MeRGAN [107] builds on this by ensuring consistency between old and new generative models. Generative replay can be combined with other continual learning techniques, such as weight regularization or experience replay, to reduce catastrophic forgetting [65]. For instance, DGMa/DGMw [108] uses task-specific binary masks to reduce interference.

Feature Replay. Feature replay reduces the complexity of generative replay by focusing on replaying feature-level representations instead of raw data. This method leverages semantic information to maintain task performance. Methods like GFR [109] employ conditional GANs to generate features after the feature extractor, while BI-R [110] incorporates context-modulated feedback to replay internal representations.

Analysis. Experience replay is straightforward and effective, allowing models to learn from past tasks, but its main limitation lies in memory constraints, where managing the diversity and relevance of stored samples becomes challenging. Generative replay avoids storing real data by generating past examples, which can be beneficial for privacy and efficiency. However, it is resource-intensive and often struggles with complex datasets, as maintaining high-quality generation across tasks is difficult. Hybrid models like L-VAEGAN [111] attempt to balance high-quality generation with accurate inference but still face challenges in scalability. Feature replay is more efficient than generative replay, focusing on preserving feature-level representations, but it suffers from representation drift as the feature extractor is updated sequentially. This issue can be mitigated with techniques like feature distillation [109] or using pre-trained models, which provide more stable representations, improving efficiency and reducing forgetting.

In addition to these approaches, recent studies [65] introduce new categories, namely optimization-based and representation-based methods which are detailed in the subsequent sections.

4) Optimization-Based Methods
These methods aim to prevent forgetting by directly influencing the optimization process to maintain performance on previously learned tasks.

Gradient-Based Methods encompass Gradient Projection approaches that adjust gradient directions during training to avoid interference with past tasks. For instance, LOGD [112] aligns updates with experience replay to preserve both the input and gradient spaces of previous tasks, while OWM [113] modifies updates to be orthogonal to the input space, reducing interference with past knowledge. Similarly, OGD [114] ensures that new gradient directions are orthogonal to previous ones to minimize task overlap, and FS-DGPM [115] maintains the subspace important to old tasks and selectively releases unimportant bases to enhance learning flexibility. AdamNSCL [116] defines regions or null spaces in the parameter space to selectively reuse or project parameter updates, ensuring stability across tasks.

Meta-Learning and Task Adaptation. These approaches aim to learn a meta-learner that can adapt quickly to new tasks by leveraging prior knowledge. ANML [117] reduces task interference through meta-training strategies, often resulting in sparse, task-specific representations. AIM [118] enhances this with a mixture of experts, improving prediction efficiency and sparsity. iTAML [119] combines meta-learning with experience replay to align gradient directions, while ARI [120] optimizes task-specific models through meta-learning, sometimes using adversarial techniques for robustness.

Parameter Space and Loss Optimization. These methods focus on finding optimal paths or regions in the loss landscape to ensure stability and minimize catastrophic forgetting. Stable-SGD [121] adapts training parameters to converge to flat local minima, making solutions more robust to task shifts. Other approaches [122] identify and connect low-error paths in the loss landscape to ensure smooth transitions between task-specific solutions.

Analysis. These methods offer strong control over task interference and flexibility in learning but can be computationally intensive and may struggle with task diversity.

5) Representation-Based Methods
This category focuses on developing robust and transferable feature representations that can adapt to new tasks while mitigating catastrophic forgetting.

Self-Supervised Contrastive Learning. These methods improve representation robustness to maintain stability across tasks. Methods like LUMP [123] enhance representations by interpolating between old and new task instances to strengthen resilience to forgetting. CaSSLe [124] converts self-supervised learning loss into a distillation strategy to map the current state of representations to previous states. Co2L [125] combines supervised and self-supervised contrastive losses to distill knowledge between old and new models, while CL-SLAM [126] uses a dual-network architecture, where one network is trained with supervised loss and the other with self-supervised loss, ensuring that representations remain both stable and plastic.

Pre-Trained Models and Transfer Learning. Pre-training on large datasets helps create strong, transferable representations that are robust to forgetting when fine-tuned on downstream tasks [65]. Adapting pre-trained knowledge to new tasks is the key challenge, but solutions like DLCFT [127] fuse outputs from a lightweight network parallel to the pre-trained backbone, and TwF [128] trains a side network that distills knowledge layer by layer from the backbone network. ADA [129] employs adapters to adjust pre-trained transformers for new tasks to further improve adaptability.

Incremental and Continual Representation Learning. This merges Continual Pre-Training and Meta-Training and focuses on incrementally improving representations using large-scale, often unlabelled data. Techniques like [130] combine Barlow Twins and EWC to learn from incremental unlabeled data, while ORDER [131] manages dynamic task distributions by integrating out-of-distribution data and feature replay.

Analysis. Representation-based methods excel at creating transferable representations that facilitate task adaptation and help minimize forgetting. However, the challenge lies in adapting these representations to new tasks without disrupting those learned previously, which can be difficult, especially in the context of dynamic and evolving tasks.

C. Comparative Analysis of CL Methods
Table 2 presents a comparative overview of key CL methods, highlighting their strengths, limitations, and potential application in DSCL.

TABLE 2 Comparative Analysis of CL Methods
Table 2- Comparative Analysis of CL Methods
D. Scenarios in CL
Continual learning is traditionally categorized into three distinct scenarios based on how a model processes and adapts to new tasks and data over time: Task-Incremental Learning (Task-IL), Domain-Incremental Learning (Domain-IL), and Class-Incremental Learning (Class-IL) [132]. In Task-Incremental Learning, each new task introduces a new set of classes, and the model learns these classes while retaining knowledge of previously learned ones. Task boundaries are clearly defined, meaning the model is aware of which task or subset of classes it is working on during both training and testing. This scenario focuses on expanding the model’s classification abilities over time, with the task identifier always available that simplifies the management of task-specific data. The key challenge in this scenario is to balance performance and computational efficiency, while effectively utilizing knowledge from one task to improve performance on others [132]. In Domain-Incremental Learning, all classes from the initial task remain present across subsequent tasks, but the input distribution changes. The model must adapt to distribution shifts without task identifiers, meaning it cannot rely on knowing the domain of the current data during testing. As a result, preventing forgetting by design is inherently impossible in this scenario which makes the mitigation of catastrophic forgetting a significant and still unsolved challenge [132]. In Class-Incremental Learning, new tasks introduce either additional data for existing classes or entirely new classes, but task boundaries are not explicitly defined. The model must integrate new data incrementally and retain previous knowledge without knowing which task the data belongs to. This setup is more challenging, as the model must retain previously learned information while integrating new data [132]. Additionally, the model must learn to distinguish between both old and new classes.

While these divisions help in studying the specific challenges of each scenario in isolation, they fail to fully capture the non-stationary and unpredictable nature of real-world environments. Recent research has shifted toward more realistic and practical setups that better reflect the complexities and dynamics of real-world applications. Online Continual Learning [133] presents a learning paradigm where data is presented sequentially in small batches, and once processed, previously seen batches are no longer accessible for retraining or further evaluation. This scenario closely aligns with real-time learning, where models must update continuously without revisiting old data. Task-Free Learning [134] takes a step further by removing task identities during both training and inference. This setup better reflects dynamic environments where task boundaries are unclear or unknown. Continual Learning with Semi-supervised Data [135] adds another layer of realism by acknowledging that fully labeled datasets are often unavailable in real-world, real-time settings. Semi-supervised Incremental Learning methods combine unsupervised feature learning with supervised classification methods, improving continual learning in scenarios with limited labeled data. On a broader scale, Semi-supervised Continual Learning extends this approach by allowing models to continuously learn from streams of both labeled and unlabeled data over time, without clearly defined task boundaries. This ongoing learning process helps maintain performance across tasks, adapting to new information while minimizing the risk of catastrophic forgetting, making it especially valuable in dynamic environments where data arrives continuously and labeling is limited.

SECTION V.Integration of DSSL and CL
To address the third research question (RQ3), this section explores how integrating DSSL and CL enhances their applicability in real-world scenarios. It investigates the rationale behind the DSCL integration, categorizes recent strategies, and highlights key applications and research directions in the literature.

A. Synergy of DSSL and CL
The integration of continual learning into DSSL frameworks results in a new paradigm called Deep Semi-supervised Continual Learning (DSCL) which presents a unique opportunity to address the inherent challenges in both fields. While DSSL leverages unlabeled data to improve learning efficiency, it often struggles with challenges like confirmation bias, computational inefficiency, and instability when dealing with evolving data distributions. Continual learning, with its adaptive nature, can make these methods more resilient and effective over time. It enables DSSL models to continually refine and adapt their knowledge as new data becomes available, without the need for retraining from scratch. This dynamic adaptability helps mitigate challenges, such as mode collapse in generative models and the confirmation bias in pseudo-labeling, by continuously updating the model’s understanding of the underlying data distribution.

The trade-off between retaining previously learned knowledge and incorporating new information is crucial in DSSL scenarios, as it reduces the risks of overfitting to initial data or reinforcing incorrect predictions, which are common challenges in techniques like pseudo-labeling and consistency regularization. By continuously adapting to new data, continual learning can improve the robustness of graph-based DSSL methods. Additionally, this approach helps mitigate the effects of concept drift and shifting in data distributions, especially in dynamic or large-scale environments.

On the other hand, a significant challenge in continual learning is catastrophic forgetting, where newly acquired knowledge can overwrite previously learned information. While recent strategies aim to balance memory stability with learning plasticity, managing these trade-offs across tasks remains an ongoing challenge. DSSL approaches can mitigate catastrophic forgetting as they are more resilient to this issue compared to traditional supervised learning paradigms. This resilience can be attributed to two factors. First, Robust Representations, by leveraging large amounts of unlabeled data, DSSL models generate more robust representations which means the learned features are more general and less prone to forgetting. The continuous learning of robust features helps the model maintain a stronger foundation for task retention. Second, Convergence to Wider Loss Basins, DSSL models tend to converge to broader regions of the loss landscape which suggests that the optimization process is less sensitive to small changes and perturbations, thus less likely to lose previous knowledge.

Moreover, DSCL enhances the scalability and efficiency of DSSL models by allowing them to evolve with incoming data, rather than being constrained by static training processes. This dynamic adaptability not only improves the generalization and performance of DSSL models but also aligns with the real-world need for AI systems capable of lifelong learning and adaptation. Therefore, the rationale for integrating continual learning into DSSL frameworks lies in creating a more resilient, adaptive, and efficient learning paradigm that is better equipped to tackle the complexities of real-world data and evolving tasks.

B. Recent Studies on Deep Semi-Supervised Continual Learning (DSCL)
1) Integration Strategies
In recent years, there has been considerable research interest in integrating DSSL techniques with continual learning, resulting in the development of various DSCL strategies. We categorize these strategies into four main groups, as illustrated in Fig. 5.

FIGURE 5. - Taxonomy of DSCL integration strategies in recent studies.
FIGURE 5.
Taxonomy of DSCL integration strategies in recent studies.

Show All

Model Architecture and Network Design. Some studies have introduced innovative architectures and frameworks to seamlessly integrate DSSL with continual learning. This category focuses on how models can be designed to balance task-specific learning with generalizable knowledge across tasks. For instance, Wang et al. [68] propose ORDisCo, a triple-network architecture consisting of a classifier, a generator, and a discriminator, interdependently trained on incremental semi-supervised data. This framework employs a conditional GAN to enable the classifier and generator to collaboratively learn and improve. By generating conditional samples based on both labeled and unlabeled data, the system exploits the underlying data distribution for continual learning and enhances learning capacity. Kou et al. [136] developed SSCL-TransMD, an architecture designed for malware detection. This model integrates multiple (five) layers, including memory replay, information mapping, similarity calculation, information encoding, and classification detection. It utilizes an improved LUMP (Lifelong Unsupervised Mixup) model for memory replay to balance historical and new data. Additionally, it leverages the LLGC (Learning with Local and Global Consistency) algorithm for similarity calculation to provide pseudo-labels and employs an MLP neural network for final classification.

Memory and Replay Mechanisms. This category includes methods that focus on how models store, recall, and utilize past data for learning. These strategies are essential for preserving information across tasks and managing catastrophic forgetting in continual learning. Boschini et al. [137] propose a method that integrates experience replay with MixMatch to mitigate forgetting while effectively utilizing unlabeled data through consistency regularization. Their approach incorporates a contrastive loss to differentiate examples from different tasks and a supervised loss to align examples of the same class. Additionally, the k-Nearest Neighbors algorithm is employed during inference to further support memory retention. Chen et al. [138] combine in-task self-training (ST) with cross-task episodic memory replay (EMR) to replay data across tasks and address data scarcity and forgetting. This approach also introduces a novel Soft Fusion Network (SFNet) that employs a Teacher-Student framework to manage current task optimization alongside overall task retention. Additionally, SPIDER [139] stores only unlabeled data to preserve privacy and introduces a gradient projection memory (GPM) technique, which projects gradient updates orthogonally to the gradient subspaces of previous tasks. This technique ensures minimal interference from past tasks which helps minimize interference and improve memory retention.

Loss Functions and Knowledge Distillation. Innovative designs of loss functions and knowledge distillation techniques between tasks have also been explored. These approaches allow models to transfer and retain knowledge from earlier tasks while simultaneously learning new ones, ensuring minimal performance degradation. Kang et al. [140] propose the NNCSL framework, which combines a deep semi-supervised continual learner with a Nearest-Neighbor Distillation (NND) loss. The NND loss improves model stability by distilling knowledge from previous tasks using nearest-neighbor relationships in feature space. Liang et al. [141] propose a semi-supervised distillation method integrated with a cross-domain contrastive objective. This approach ensures the model can distinguish between domain-specific characteristics while preserving prior knowledge. Smith et al. [142] combine local and hard global distillation methods with consistency regularization to utilize unlabeled data. The distillation framework ensures that knowledge from earlier tasks is transferred to new ones, thus preserving performance across tasks.

Optimization and Data Management. This category encompasses methods focused on optimizing the learning process and managing labeled and unlabeled data while employing techniques to handle noise or gradient updates. Luo et al. [143] introduce a gradient learner within a continual learning framework that predicts gradients for unlabeled data, which are subsequently used to update the model. Their optimization strategy normalizes predicted gradients to enhance robustness and employs a probabilistic sampling policy to manage the use of unlabeled data. Karim et al. [144] proposed a framework that stores incoming data in a delay/current buffer and separates it into clean and noisy buffers using a simple thresholding technique based on loss values. This method then incorporates SSL for fine-tuning, utilizing both clean and noisy samples to improve representation learning. Furthermore, Fan et al. [145] introduce Dynamic Sub-graph Distillation (DSGD), which employs graph-based optimization techniques to preserve the local structure of knowledge representations. This dynamic graph structure distillation mechanism ensures consistency in knowledge transfer between old and new tasks which facilitates efficient optimization over time.

Table 3 presents a detailed comparative analysis of various approaches in DSCL, highlighting key aspects of recent studies.

TABLE 3 Comparison of Recent DSCL Approaches: Challenges, Methodologies, and Performance
Table 3- Comparison of Recent DSCL Approaches: Challenges, Methodologies, and Performance
2) Applications and Research Directions
Recent research in DSCL has explored a variety of applications and domains that highlight its potential to tackle diverse challenges faced by real-world systems. This section discusses the current trends observed in the application and research directions in the recent literature. As evidenced in Table 4, The majority of studies focus on image classification tasks (e.g. See [68], [137], [142]). This focus is consistent with the broad use of CL in computer vision tasks, where models must continuously adapt to new object categories in dynamic environments such as autonomous driving, medical imaging, and robotics. However, they evaluate the performance of DSCL models on benchmark datasets such as CIFAR-10, CIFAR-100, SVHN, and TinyImageNet. For instance, Boschini et al. [135] integrated experience replay with MixMatch to mitigate catastrophic forgetting while leveraging unlabeled data in tasks using datasets like SVHN, CIFAR-10, and CIFAR-100. Similarly, Wang et al. [66] applied conditional GANs that incrementally learn from semi-supervised image data in datasets such as CIFAR-10, SVHN, and TinyImageNet. These approaches highlight the strengths of DSCL in handling the complex and high-dimensional nature of image data, where leveraging both labeled and unlabeled data can significantly boost performance. While general-purpose datasets like CIFAR are widely used, they may not suffice for specialized tasks such as medical imaging, where capturing fine details is critical. For medical domains, such as tasks involving the classification of abnormalities in microcirculation patterns [146], domain-specific datasets are essential. The successful application of DSCL in such domains requires more tailored datasets that reflect the unique complexities and requirements of medical imagery, further underscoring the need for innovation in DSCL approaches across varied sectors.

TABLE 4 Recent DSCL Studies Across Datasets, Applications, and CL Settings
Table 4- Recent DSCL Studies Across Datasets, Applications, and CL Settings
Although image classification is the dominant focus, recent research is extending DSCL into specialized domains such as cybersecurity, natural language processing (NLP), and network intrusion detection. Studies such as Kou et al. [136] and Amalapuram et al. [139] emphasize the applicability of DSCL to structured data in cybersecurity tasks. Specifically, Kou et al. [136] focus on malware detection, while Amalapuram et al. [139] explore network intrusion detection, highlighting the flexibility of DSCL in domains beyond computer vision. For instance, The SSCL-TransMD [136] model for malware detection leverages tabular labeled historical data, including system logs and access permissions, alongside new, unlabeled data. Other studies, such as [138] and [141], have also applied DSCL models to structured data streams in different domains, specifically in text-to-SQL tasks and machine translation. These models adaptively handle sequential task streams, allowing for the continual learning of complex language-based tasks while preventing performance degradation on earlier learned tasks.

In terms of CL scenarios and settings, Class-Incremental Learning (Class-IL) is the most prevalent framework explored in recent DSCL studies [66], [135]. In Class-IL, models learn new classes sequentially while retaining the ability to recognize previously learned classes which makes it suitable for dynamic environments such as visual recognition systems, where the number of object categories can grow over time. Another common scenario is Task-Incremental Learning (Task-IL), where models learn distinct tasks over time, typically with clear task boundaries. In this setting, the identity of the task is provided during both training and testing, as seen in NLP and cybersecurity applications. Despite the diversity of applications and CL methods reviewed, all these studies operate within a closed-world incremental learning paradigm, where the tasks or classes are predefined, and new categories emerge only within the bounds of the system’s known structure. These methods assume a controlled environment where the data distribution is either fixed or changes incrementally but within familiar, predefined classes. Techniques such as replay buffers and knowledge distillation are specifically tailored to this setting, as the model is aware of the complete task or class space during training and adapts incrementally. While these methods have shown considerable success, there remains a broader opportunity to extend DSCL frameworks to more dynamic and unpredictable environments where new classes and tasks are not predefined which is called open-world continual learning. The gap lies in developing DSCL models that can autonomously detect and adapt to entirely new categories or concepts that were not encountered during the initial training phase, without the explicit task boundaries or class definitions characteristic of closed-world learning.

From another perspective, while current research has mostly focused on applications in image classification, cybersecurity, and NLP, there remains a significant gap in exploring the potential of DSCL in other domains, such as business and finance, where structures and patterns undergo constant evolution. Business environments, particularly in areas like customer behavior analysis, financial forecasting, and supply chain management, are characterized by dynamic, non-stationary data streams [147]. These domains regularly experience shifts in market trends, consumer preferences, and external economic factors that make them good candidates for the application of DSCL.

Furthermore, while many studies focus on domain-specific solutions, the extension of DSCL into multi-domain learning could represent a valuable advancement. Multi-domain continual learning presents additional complexities as models are required to generalize across domains with different data distributions, tasks, and goals. For instance, integrating insights from multiple domains such as customer behavior, market analysis, and supply chain logistics could enable more comprehensive business decision-making systems. Current research, while effective within isolated domains, does not yet address how DSCL frameworks might handle such cross-domain learning, which could significantly enhance the adaptability and robustness of machine learning models in real-world applications. Thus, the gap lies in developing scalable, domain-agnostic DSCL frameworks that can simultaneously handle a variety of data types, adapt to dynamic environments, and efficiently leverage both labeled and unlabeled data streams. Solving these challenges is crucial for advancing DSCL toward practical, real-world applications across fields such as healthcare, cybersecurity, business, and autonomous systems.

SECTION VI.Challenges and Future Directions
The integration of DSSL with continual learning brings significant advancements while also presenting several challenges. This section addresses the fourth and fifth research questions (RQ4, and RQ5) by delving into these challenges, discusses their implications in real-world scenarios, and outlines potential directions for future research.

A. Challenges in Integration
Based on our review, we identified five primary challenges in integrating DSSL and CL, which studies have targeted to develop robust and effective DSCL systems. Fig. 6 illustrates these main challenges and the corresponding integration strategies employed in the literature.

FIGURE 6. - Overview of key challenges in DSCL and associated solution strategies from prior research.
FIGURE 6.
Overview of key challenges in DSCL and associated solution strategies from prior research.

Show All

In the following discussion, we explore each challenge, analyze its real-world implications, examine existing solution strategies, and identify potential future directions to enhance the development and adoption of DSCL across various domains.

One of the primary challenges in CL is catastrophic forgetting, where models tend to lose information from previous tasks when exposed to new ones. Methods such as experience replay [137] and episodic memory replay [138] have demonstrated promise in mitigating this issue. In DSCL, which leverages both labeled and unlabeled data, the integration of unlabeled data can further mitigate catastrophic forgetting by facilitating the learning of more generalizable representations. However, this challenge persists, particularly in scenarios with limited labeled data or when models encounter noisy or irrelevant unlabeled samples. In these cases, preserving previously learned knowledge while acquiring new information becomes increasingly complex, underscoring the need for advanced strategies to manage memory and task integration effectively. In high-stakes domains like healthcare or cybersecurity, catastrophic forgetting can have severe consequences. For instance, a healthcare model that forgets how to detect early symptoms of diseases while learning new tasks could compromise patient outcomes. Similarly, in cybersecurity, forgetting older attack patterns might fail the model to protect systems effectively. Future research should explore hybrid memory consolidation mechanisms that combine episodic memory with self-supervised representation learning to improve knowledge retention without imposing excessive storage requirements.

The stability-plasticity dilemma, balancing stability (the retention of past knowledge) and plasticity (the capacity to adapt to new tasks), represents a fundamental challenge in DSCL. While unlabeled data can enhance plasticity by generalizing the model’s understanding, it may also introduce noise or irrelevant patterns that make it difficult to retain useful knowledge (stability). This challenge is particularly pronounced in domains with continuously evolving data distributions and unclear task boundaries, such as autonomous systems and financial markets. Maintaining an optimal balance is critical, as excessive plasticity increases the risk of catastrophic forgetting, while excessive stability can restrict effective learning and adaptation. For instance, in autonomous vehicles, excessive plasticity could lead to forgetting essential safety-critical behaviors, while excessive stability might prevent adaptation to new environments. Methods such as gradient projection memory (e.g., SPIDER [139]) aim to minimize task interference but typically rely on well-defined task boundaries. Real-world applications require more flexible, task-free approaches, wherein models can autonomously detect and adapt to task shifts. Future research should focus on adaptive learning frameworks capable of autonomously detecting task shifts and dynamically adjusting the stability-plasticity trade-off. Additionally, meta-learning approaches, particularly gradient-based learners such as MAML [148], could be leveraged to dynamically calibrate this trade-off in DSCL systems, ensuring task-agnostic adaptation while preventing catastrophic forgetting.

HandlingUnlabeled andNoisy Data presents another significant challenge in DSCL. While unlabeled data offers valuable learning opportunities, it simultaneously introduces complexities, particularly in real-world settings where it often contains noise or out-of-distribution (OoD) samples that can negatively affect model performance. Approaches such as MixMatch [137] and GAN-based methods [68] aim to make effective use of unlabeled data but their ability to manage noisy or irrelevant samples remains limited. For instance, in domains such as e-commerce, where data originates from diverse sources, the inability to filter out irrelevant samples can degrade model performance. Recent studies, including Smith et al. [142], have investigated OoD detection mechanisms, but further refinement in filtering and selecting the most relevant samples is necessary to enhance the learning process and overall model efficiency. Future research should consider integrating confidence-calibrated self-supervised learning techniques with dynamic filtering frameworks to selectively prioritize high-confidence samples while minimizing the risk of noise propagation. Additionally, targeted manual corrections for critical samples may further enhance reliability, particularly in high-risk domains such as medical diagnosis.

Imbalanced Data and Representation Learning poses another critical challenge for DSCL, particularly in environments where classes are unevenly represented. Some existing approaches have addressed this issue; for instance, Boschini et al. [137] integrated MixMatch with experience replay. In this framework, MixMatch improves the diversity and representation of minority classes through data augmentation and pseudo-labeling, while experience replay helps maintain balanced knowledge across classes from previous tasks. Despite these efforts, further advancements are required to manage imbalance in dynamic, evolving learning environments. In applications such as disaster response systems, underrepresented events can lead to suboptimal decision-making during emergencies. Given the prevalence of this challenge in real-world applications, future research should prioritize developing adaptive balancing strategies that dynamically adjust to evolving data distributions. Techniques such as generative modeling for synthetic sample creation and reinforcement learning for prioritizing minority class representations could enhance model robustness in imbalanced scenarios.

Transferability and Generalization are essential for DSCL systems to effectively leverage knowledge across tasks and domains. Techniques such as local and global distillation with consistency regularization [142] focus on improving the model’s performance across domains and handling OoD data challenges. Liang et al. [141] explicitly address domain-incremental learning by focusing on transferability in machine translation tasks, wherein knowledge acquired from one domain is expected to be generalized and adapted to other languages or domains during real-time translation. Moreover, mitigating negative transfer, where knowledge from one task adversely affects performance on another, can further enhance model adaptability. This is particularly relevant in applications such as multilingual machine translation, where real-time adaptation to diverse languages and domains is critical. To address these challenges, hierarchical and cascade transfer learning [149] frameworks that disentangle shared and task-specific knowledge should be prioritized. Additionally, exploring few-shot learning and continual knowledge refinement approaches may enable DSCL systems to generalize effectively with minimal data and incrementally enhance representations across diverse environments.

B. Challenges in Recent Studies and Future Directions
Overfocus on General-Purpose Image Datasets. Many recent DSCL studies have primarily focused on benchmark datasets like CIFAR-10, CIFAR-100, and SVHN. Fig. 7 provides an overview of the image datasets most commonly used in these studies and their respective usage percentages. While these datasets are valuable for benchmarking, they may not fully represent the complexity and diversity encountered in real-world applications. Real-world tasks such as medical imaging often involve subtle and intricate data patterns that go beyond the capabilities demonstrated on benchmark datasets. Benchmarks are often relatively simple and controlled. They contain well-defined images of objects that do not vary significantly in terms of lighting, angles, or environmental conditions. These characteristics make them useful for comparing algorithms, but they may not capture the challenges present in domains such as medical imaging or autonomous driving, where detecting subtle patterns or rare anomalies is critical. Overfocusing on these benchmarks risks over-optimizing models for controlled settings while limiting their generalizability to more complex tasks. Future research should prioritize domain-specific datasets to better assess the adaptability and performance of DSCL frameworks in more specialized, high-stakes environments.

FIGURE 7. - Overview of image datasets utilized in recent studies and their respective usage percentages.
FIGURE 7.
Overview of image datasets utilized in recent studies and their respective usage percentages.

Show All

Narrow Range of Applications. Although recent research has concentrated on general-purpose image classification, the exploration of DSCL techniques in other domains remains limited. While initial efforts have been made in cybersecurity and NLP tasks, noticeable gaps exist in applying DSCL to domains such as financial fraud detection, customer behavior analysis, and recommender systems, where dynamic and evolving data distributions and semi-supervised data are prevalent. These fields can benefit considerably from DSCL’s continual adaptation capabilities, particularly for tasks requiring real-time learning and decision-making. The lack of research in these areas limits the broader adoption of DSCL in real-world applications, where its adaptive capabilities could offer significant benefits, such as real-time adaptation in fraud detection and dynamic personalization in recommender systems.

Adaptive Classifiers and Open-World Learning. In real-world environments, data frequently evolve, and new, previously unseen classes emerge over time, which poses a significant challenge for learning systems. While current research has made progress with class-incremental learning, where models can adapt to new tasks or classes incrementally, these approaches still typically operate within closed-world settings where the range of potential classes is predefined. For practical deployment in real-world scenarios, adaptive classifiers are required to autonomously detect and incrementally learn new classes as they emerge without explicit prior knowledge. Achieving this involves recognizing when new classes appear and incorporating few-shot learning techniques to rapidly learn from limited labeled examples. Future research should focus on developing open-world DSCL frameworks capable of continuous class discovery and learning in diverse and changing environments that ensure adaptability to unforeseen challenges.

Scalability to Large and Dynamic Datasets. A pressing challenge for DSCL is scaling effectively to large datasets typical of domains like cybersecurity, finance, and e-commerce. The size and complexity of the datasets introduce significant computational demands, not only for storing and recalling past tasks but also for ensuring efficient model training and inference over time. Techniques such as experience replay and episodic memory (e.g., Boschini et al. [137]) offer mechanisms to retain learned knowledge but may become computationally expensive and memory-intensive in large-scale applications. Future work should prioritize memory-efficient solutions, such as hierarchical knowledge distillation and compressed episodic memory storage, to enable DSCL models to process continuous data streams without overwhelming system resources. However, the high computational power required for continuous model updates remains a bottleneck, especially in real-time adaptation scenarios. Research into lightweight continual learning frameworks, adaptive sampling, and model compression, is needed to enhance DSCL scalability.

Lightweight Models for On-Device Learning and Edge Computing. With an increasing number of AI models targeting deployment in resource-constrained environments, such as smartphones and IoT devices, lightweight architectures become essential. On-device learning, combined with federated learning frameworks, allows these devices to train locally and periodically send updates (such as gradients or logits) to a central server for aggregation. This setup reduces the frequency of data transmission, minimizes computational costs, and enhances privacy by keeping sensitive data local. Despite progress in DSCL research, the design of lightweight architectures optimized for on-device learning in resource-constrained settings remains largely unexplored. Future studies should focus on developing DSCL models that maintain a balance between learning efficiency and performance under limited resources. Techniques such as model pruning and knowledge distillation can play a critical role in achieving this goal.

Interpretable and Trustworthy Models. In high-stakes applications, interpretability and trustworthiness are critical concerns. Current DSCL approaches often function as “black boxes,” providing limited insight into their decision-making processes. This lack of transparency poses significant challenges in domains like healthcare, where understanding model decisions is essential for clinical validation and trust. Future research should address this issue by developing models that are inherently more transparent and capable of providing human-understandable explanations for their decisions. One promising approach involves the incorporation of human-in-the-loop frameworks, such as Reinforcement Learning with Human Feedback (RLHF), into the training process. By integrating human input into the learning process, RLHF can help align model behavior with human expectations, especially in uncertain scenarios. RLHF not only indirectly supports interpretability by encouraging model outputs that are more consistent with human reasoning but it also contributes to trustworthiness by ensuring that the system learns in a way that reflects human judgment and priorities. Additionally, by continuously refining model behavior based on systematic human feedback, RLHF may help mitigate risks associated with adversarial attacks and biases.

Universal Domain-Agnostic Frameworks. A critical direction for future research lies in developing frameworks that work seamlessly across multiple domains, from vision tasks to structured data. While studies by Kang et al. [140] and Kou et al. [136] have shown promise within specific domains, their approaches often lack adaptability across diverse domains. To overcome this limitation, a universal DSCL framework should effectively accommodate the inherent differences in data types, such as the high dimensionality of images versus the diverse structures of tabular data. Future work should focus on advanced domain adaptation techniques that enable models to generalize across domains through shared representation learning. Methods such as contrastive pre-training and domain adversarial learning [150] could be leveraged to bridge domain gaps in DSCL models.

Robustness to Noisy Data. Real-world data are often noisy, incomplete, or mislabeled, which can significantly degrade model performance. While approaches such as noise-aware buffers [144] and gradient normalization [143] have shown some progress in mitigating these issues, further research is needed. Future methods should focus on advanced denoising techniques for unlabeled data, including dynamic data cleaning methods and strategies to selectively emphasize more reliable data sources. Improving DSCL’s robustness to noisy environments will increase the applicability of these models in settings where perfect data is often unavailable.

Task-Free Continual Learning, in which models autonomously detect shifts in data distribution or identify new tasks without explicit labels or boundaries, remains an underexplored area. Advancing research in this direction would allow DSCL models to operate with greater flexibility and autonomy that improves their utility in dynamic, real-world environments where tasks are not clearly defined or segmented.

Transfer Learning and DSCL. Leveraging transfer learning to enhance DSCL models has shown promise in recent studies. Pre-training on vast amounts of unlabeled data allows models to learn robust, transferable feature representations, thereby reducing the risk of catastrophic forgetting and enhancing generalization across tasks. DSSL methods, such as consistency regularization or pseudo-labeling, enable efficient learning from limited labeled data, while continual learning mechanisms facilitate the incremental adaptation of pre-trained models to new tasks. This combined approach offers a scalable, flexible solution for developing DSCL models that are both adaptable and capable of long-term knowledge retention in dynamic environments.

SECTION VII.Responses to the Research Questions
This section revisits the research questions outlined in Section II-A, summarizing key insights and drawing connections across the literature to provide a structured understanding of the DSCL space. While earlier sections address one or more RQs in detail, the goal here is to provide a synthesized discussion linking RQs to the main findings.

RQ1: What are the key advancements and methodologies in DSSL? Recent studies highlight significant progress in DSSL, evolving from traditional semi-supervised approaches to advanced deep-learning techniques. Key advancements include utilizing deep generative models such as GANs and VAEs, which enhance feature learning and data augmentation; consistency regularization methods, like Mean Teacher and VAT, which improve model robustness by enforcing stability under perturbations; and graph-based approaches, such as GCNs and VGAE, that extend DSSL to structured data for label propagation. Additionally, pseudo-labeling iteratively refines predictions on unlabeled data, effectively transforming semi-supervised learning into a supervised task, while hybrid models combine multiple strategies to enhance generalization. These methodologies together form the foundation of modern DSCL frameworks, optimizing the use of unlabeled data for scalable and adaptive learning.

RQ2: What are the fundamental concepts, methods, and scenarios that define CL, and how do they contribute to addressing dynamic learning challenges? CL addresses dynamic learning challenges by mitigating catastrophic forgetting and task interference through multiple complementary strategies. Architectural approaches modify model structures to better support sequential learning, while regularization-based methods selectively constrain parameter updates to preserve critical information. Replay-based strategies enable models to retain past knowledge by storing real or synthetic samples, and optimization-based techniques balance stability and plasticity to facilitate continual adaptation. In addition, representation-based approaches focus on maintaining robust feature representations across tasks. CL scenarios, including Task-IL, Domain-IL, and Class-IL, capture different adaptation requirements in response to evolving data distributions. Furthermore, emerging task-free CL methods enable models to continuously adapt without predefined task boundaries, establishing CL as a fundamental framework for dynamic, real-world learning.

RQ3: How does integrating DSSL with CL benefit real-world scenarios, and what rationale supports their integration? Integrating DSSL with CL results in DSCL, a paradigm that enhances model adaptability and scalability while addressing the limitations of both approaches. DSSL improves generalization by leveraging large volumes of unlabeled data, while CL enables models to retain previously learned knowledge over time. This integration reduces reliance on extensive labeled datasets, mitigates catastrophic forgetting, and enhances learning efficiency. Empirical studies, including ORDisCo, SSCL-TransMD, and memory replay-based DSCL frameworks, demonstrate that the incorporation of techniques such as semi-supervised generative modeling, pseudo-labeling, and consistency regularization significantly improves adaptation to evolving data streams. By enabling continual adaptation with limited supervision, DSCL proves effective in dynamic environments such as cybersecurity, healthcare, NLP, and autonomous systems, where models must learn continuously while preserving previously acquired knowledge.

RQ4: What challenges are encountered when combining DSSL and CL, and what are the critical approaches used to address them in recent studies? DSCL systems inherit challenges from both DSSL and CL, including catastrophic forgetting, the stability-plasticity trade-off, handling noisy and imbalanced data, and ensuring effective transferability across tasks and domains. Catastrophic forgetting is mitigated through experience replay, episodic memory mechanisms, and gradient projection methods like SPIDER. The stability-plasticity dilemma is addressed using dynamic task-adaptive learning frameworks and memory-efficient replay strategies. Handling noisy and unlabeled data remains challenging, particularly in real-world scenarios with OoD samples. Approaches such as contrastive self-supervised learning, confidence-calibrated pseudo-labeling, and noise-aware replay buffers have been proposed to refine learning from noisy data. Furthermore, ensuring transferability across tasks and domains requires structured knowledge distillation, hierarchical transfer learning, and reinforcement learning-based strategies to mitigate negative transfer effects.

RQ5: What open issues and future directions remain in the development of DSCL systems, especially in dynamic learning environments? Despite significant advancements, DSCL research still faces several open challenges and promising future directions. The over-reliance on benchmark datasets such as CIFAR and SVHN restricts the evaluation of DSCL models to controlled environments, thereby limiting their applicability in complex, high-stakes domains like medical imaging. Future research should prioritize domain-specific datasets to more accurately assess model performance in real-world settings. Moreover, most current DSCL methods operate within closed-world learning settings, where the number of tasks and classes is predefined. Extending DSCL to open-world learning, where models autonomously detect and adapt to novel categories, remains a critical research direction. Scalability to large and dynamic datasets is another challenge, requiring efficient memory compression, adaptive sampling, and computationally efficient continual learning techniques. Additionally, lightweight models optimized for on-device learning and edge computing are essential for deploying DSCL in resource-constrained environments. Robustness to noisy, incomplete, or mislabeled data is another key area that requires further attention, ensuring DSCL models maintain high performance in imperfect real-world conditions. Finally, enhancing model interpretability and trustworthiness is crucial for high-risk applications; integrating human-in-the-loop frameworks and explainable AI techniques can help bridge this gap. Combining DSCL with reinforcement learning, meta-learning, and self-supervised pretraining could further boost model adaptability, paving the way for truly lifelong learning solutions across diverse fields.

SECTION VIII.Conclusion
The integration of DSSL with CL into DSCL represents a pivotal step toward developing adaptable and efficient systems capable of addressing challenges in dynamic, real-world environments. This review has provided a comprehensive analysis of DSCL, examining DSSL and CL methodologies, categorizing existing DSCL approaches, identifying critical challenges, and proposing future research directions to bridge existing gaps and enhance DSCL’s potential in real-world applications. The findings demonstrate DSCL’s capacity to leverage both labeled and unlabeled data, improving performance in high-dimensional and evolving tasks across diverse domains such as image classification, cybersecurity, and natural language processing. Despite significant progress, challenges such as catastrophic forgetting, stability-plasticity trade-offs, transferability across diverse domains, and robustness to noisy and imbalanced data continue to impact its effectiveness in real-world applications.

A key insight is the gap between research focused on benchmark datasets and the need for solutions addressing real-world complexities. Current methods often operate within closed-world settings, limiting generalization to unpredictable, open-world scenarios. While advances in class-incremental and domain-specific DSCL have been notable, critical areas such as open-world learning, task-free continual learning, and the design of lightweight models for resource-constrained devices remain underexplored. Moreover, scalability, interpretability, and domain-agnostic frameworks remain critical areas for advancement. Addressing these challenges is essential to unlocking DSCL’s potential for high-stakes applications in fields such as healthcare and autonomous systems.

Future research should prioritize adaptive classifiers capable of open-world learning, scalable models for large dynamic datasets, and interpretable approaches aligned with human-centric requirements. These advancements are crucial for transforming DSCL into a practical and impactful framework for solving the challenges of ever-changing environments. This review highlights the need for continued innovation in DSCL, encouraging the research community to address these gaps and further advance the field toward its full potential.