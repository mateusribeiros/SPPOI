Title: Emotion-aware Design in Automobiles: Embracing Technology
Advancements to Enhance Human-vehicle Interaction

Abstract
The integration of emotion-aware systems in vehicles is accelerated
by new technologies, including advancements in AI and ubiquitous sensing technologies. As the automotive industry shifts from
technology-centred, feature-driven approaches to human-centred
design, this research focuses on how to efectively incorporate emotion features into user-centred design to enhance efective humanvehicle interaction in practices. By conducting an interview study
with 31 industrial design practitioners, supplemented by insights
from engineers and AI experts involved in the early-stage design
and development of novel in-vehicle user interfaces and systems,
we examined current practices, and sampled their challenges, attitudes and expectations related to emotion-aware systems. Our
fndings provide critical insights to the design space of emotionaware systems from both user and AI perspectives, inform eforts to
support design practices in this evolving area, and identify opportunities for future innovation in emotion-aware in-vehicle design.
Based on our fndings, we propose adaptations to design practices
and recommendations for further research.
CCS Concepts
• Human-centered computing → Empirical studies in interaction
design; Empirical studies in HCI.
Keywords
Human-centred Design, Emotion, Human-vehicle Interaction
ACM Reference Format:
Xingtong Chen, Xia Wang, Cong Fang, Le Fang, Wei Gong, Chengzhong
Liu, and Stephen Jia Wang. 2025. Emotion-aware Design in Automobiles:
Embracing Technology Advancements to Enhance Human-vehicle Interaction. In CHI Conference on Human Factors in Computing Systems (CHI
’25), April 26–May 01, 2025, Yokohama, Japan. ACM, New York, NY, USA,
18 pages. https://doi.org/10.1145/3706598.3713571
1 Introduction
With the next paradigm shift in automation and intelligence, the
automotive industry is turning traditional automotive cabins into
intelligent mobile places with an user-centric vision: intelligent
cockpits [22]. The design space of intelligent cockpits fourishes
with the acceleration in industrial transformation, responding to
the dynamic revolution and seamless fusion of technologies [70, 81].
Meanwhile, the recent wave of Artifcial Intelligence (AI) and multimodal sensing technologies extends its ability where the intelligent cockpits equipped with AI agents allow the implementation
of afective systems along the progress of sensors and actuators,
exemplifed by the launch of empathic voice assistants [19, 115].
That is, emotion-aware systems enable cockpits to "sense, interpret,
adapt, and potentially respond appropriately to human emotions"
[76, 124].
In fact, emotion-aware systems, especially in automobiles, are
often infuenced by the capacity of technologies [1, 64], refecting
a technology-centred view in applications [19]. However, arguably
these are being studied and developed without embracing the full
spectrum of end-user needs [5, 90]. Much of the literature focusing
on in-car afective technologies is devoted to improving road safety
and user experience in the car [38], through studies on the context
of emotions on the road [69, 71, 72, 124], the efects of emotions on
driving [91, 95, 111], or using interaction approaches to regulate
human emotions [19, 63]. This is questionable because, within a
production paradigm, user-centric orientation is the primary driver
propelling the shift in technologies [114, 119]. Juxtaposing initial
research ideas with how they have been applied in vehicles [16–18],
it becomes clear that technical concepts need to fnd their way into
applications and, to the end-user. To close the gap between afective
technologies and user needsfor efective human-vehicle interaction,
thus, the focus in this paper is not on exploring new technologies
or adapting to novel conditions; rather, it pushes forward the edge
of emotion features through improving the user-centred design of
in-car emotional experiences.
Yet, while there is ongoing research on emotion features, truly
emotion-aware systems in mass produced vehicles are still a thing
of the future—little in the way of exploration of how the systems
are designed in practice—as it were, in the ‘real world’. Given the
early status of emotion AI in automotive industry practices [79],
a lack of established means to uncover user needs related to their
emotional states was anticipated. As industry practitioners, designers are always undertaking the leading work supported by
engineers and researchers [9, 62]. They bridge user needs and technical capabilities to create efective and engaging features during
early-stage design and refnement of automotive interfaces and
systems. Understanding their practices, challenges and insights
involved in emotion-aware systems can ofer a glimpse into the
upcoming autonomous and intelligent society, and inspire research
on emotion-aware human-vehicle interaction.
2 RQs and Related Work
This section frst describes how emotion features play an role in
human-vehicle interaction. To understand the context and scope of
our work, human-centred design approach was highlighted. Then
we look at current research on emotion features associated with AI
and ML in intelligent vehicle cockpits. We fnish by identifying the
research gap this study seeks to address.
2.1 Impact of Emotion in Human-vehicle
Interaction
With the transformation of vehicles from simple modes of transport
to intelligent mobile places, the emotional experience of drivers
and passengers has gained heightened signifcance [19, 20]. The
primary motivation for afective systems in vehicles comes from
the fact that emotional states can signifcantly infuence a driver’s
operational control and thus has an efect on road safety [4, 19], underscoring the importance of emotion detection for immediate user
feedback [13]. By detecting and interpreting the emotional states
of drivers and occupants, cognitive load during system operation is
diminished, evidenced by existing research emphasising on driving
safety where the real-time surveillance and appraisal of the driver’s
state reduce accident risks [83, 95, 124].
As driving systems continue to advance, the diversifcation of
user demands and increased market competitiveness are driving the
evolution of intelligent cockpits into more diverse areas [80]. Most
notably that recently many automotive brands have been pushing
emotion features within their in-car functionalities. For example,
building upon previous "Caring Car" feature [12], BMW debuted
the i Vision Dee concept at CES 2023, which includes advanced
emotional interaction capabilities like personalised responses or
visual expressions based on the drivers’ mood [49]. Porsche and
BMW both introduces Afectiva’s Emotion AI innovated by the
MIT’s Media Lab [77, 78], which uses cameras to measure realtime emotional states of drivers and occupants [28]. Thus, their
systems aim to adjust in-car settings dynamically (e.g., climate control, adaptive music, ambient light, or empathic speech) to create a
more comfortable journey [86]. Similarly, Mercedes-Benz recently
updated their operating system with a new MBUX Virtual Assistant where the AI-driven system ofers a human-like interaction
between drivers and vehicles [50]. These systems capture the driver’s psycho-physiological signals, speech or facial expressions
through sensors, analyse emotional states, and provide feedback or
recommendations—such as relaxation guidance or environmental
adjustments—to improve passenger comfort and overall well-being
[8]. We can see that emotion features are currently becoming more
human-centred, but realistically speaking, these features are still
very limited or not yet feasible for real-world applications.
2.2 Human-centred Design in Vehicles
Human-centred design is an approach that involves a multidisciplinary perspective [46], emphasizing user interaction, empathy,
and the iterative refnement of design based on user feedback [74].
The evolution of traditional automotive processes is leading to a
growing user demand for sophisticated experiences [24]. Humancentred design, widely adopted by the automotive industry, efectively identifes user needs and conducts scenario analysis to design
functions and services that align with real-world user scenarios
[40]. These approaches aim to help people deal with the high degree of complexity in the driving environment and to help them
manage with unanticipated driving events that involve emotional,
psychological or sociological issues [45].
In intelligent cockpits, the Human-Machine Interface (HMI) [37],
including the central control screen and instrument display, organises information layout based on diferent felds of view in human
vision: direct, glance, and peripheral vision. The voice assistant
enhances user interaction by allowing control of various vehicle
functions through speech [55]. As it continuously learns and adapts
to users’ speech habits and preferences, it provides more accurate
and personalised services [18]. The application of human-centred
design in intelligent cockpits ensures all users a more intuitive,
efcient, and enjoyable driving experience. A recent paper from
Braun et al. presents a particularly thorough review of afective automotive user interfaces [19]. Through their review, the challenges
arising in the design of real-world emotion-aware applications
point to the way of human-vehicle interaction falling short. Instead of launching immature product innovations at the moment,
exploring concepts for natural interaction is the priority which can
be enhanced with emotion features once the technology is ready.
Thus, built upon human-centred design approach (also refers to
user-centred design throughout the paper), this paper is motivated
to develop new perspectives for understanding emotion-aware design where little is known about how emotion-aware systems are
designed in real-world practices.
2.3 Adaptation of AI in Automotive
Emotion-aware Practices
The traditional automotive industry emphasisesthe design of audiovisual aspects such as lighting, music and exterior styling to enhance the driving experience through presetscenes and programmed
responses [52]. However, cockpits operating without artifcial intelligence exhibit signifcant limitations in the realm of emotional interaction experiences [33], in which interactions between users and
the system are typically confned to a predefned set of commands
and their corresponding response mechanisms [14]. Such systems
lack of capability to learn from user preferences and behavioural
patterns, thereby being unable to ofer a highly personalised service
experience [117]. Moreover, the absence of emotional recognition
functionality in cockpit systems hampers their ability to discern
fuctuations in user emotions, resulting in an inability to make
timely response adjustments based on changes in user emotional
states [14].
The integration of AI has broadened the application of emotionaware design in intelligent vehicle cockpits by incorporating multimodal sensing [122, 124]. Physiological signals [23, 51, 88, 110],
such as movements, gestures, facial expressions, skin conductance,
eye movement, heart rate and electroencephalogram(EEG), are collected for joint training with deep learning, signifcantly improving
the accuracy of emotion recognition [38] and enabling personalised
feedback and services [117]. The wide availability of training and
datasets has accelerated the adoption of machine learning (ML) and
AI technologies within vehicles [21]. Moreover, Large Language
Models (LLMs) enhance afective analysis and multimodal data
fusion, facilitating more accurate recognition of users’ emotional
states and enabling more context-aware interaction in the vehicle
[29, 122]. Leveraging AI, automakers, some collaborated with tech
frms, have now employed it within the cockpit to engage with
passengers and adapt to their needs [29, 43, 87, 112].
While AI shows great potential in intelligent cockpits, several
practical challenges persist, such as delayed responses, limited
contextual memory, and inadequate perception of environmental background information [89]. To overcome the limitations of
large models in areas like understanding, reasoning, memory, and
environmental awareness, engineers have introduced solutions [58,
73] including AI agents [115], prompt engineering [75], retrievalaugmented generation (RAG) [56], and fne-tuning [100] to enhance system’s intelligence and adaptability [118]. Meanwhile,
tasks within the cockpit have been subdivided to improve AI performance across diverse dimensions, including natural language
responses [10], afective computing, driver assistance and safety
[47], and personalisation [3]. In this technological context, it is
crucial to re-evaluate and redesign the methods of emotional interaction within the cockpit to fully leverage AI technology, enhance
human-vehicle interaction and optimise the user experience [25].
2.4 Motivation and Research Questions
As the frst step towards understanding what is needed to address
user emotional needs with vehicles in real-world practices, in this
paper, we conducted an online interview study with industry practitioners especially in the intelligent cockpit sector when involving in
early-stage ideation, design, prototyping, evaluation and iteration.
These practitioners included designers, focusing on user-facing and
user-centred aspects of technologies (e.g., user experience (UX) designers, user interface (UI) designers, interaction designers, or UX
researchers); and engineers, primarily responsible for the technical implementation and development of in-car systems. These two
sets of practitioners were actively involved in addressing emotion
features, conceptually by designers and technically by engineers,
both in design and refnement of novel user interfaces and systems
within intelligent cockpits. A further emphasis in the presented
work is placed on the impact of AI on emotion-aware applications
and systems. Thus, AI professionals or specialists (whom we refer
to as “AI experts” throughout the paper), were recruited to ofer
the lens of how AI has been adopted into practices and its potential
implications for better human-vehicle interactions. We thus seek
to answer the following research questions:
• RQ1: How do industry practitioners currently understand
and address emotion features during early-stage design and
refnement within intelligent cockpits?
• RQ2: What challenges do industry practitioners encounter
in emotion-aware design during early-stage design and refnement within intelligent cockpits?
• RQ3: What opportunities do industry practitioners envision
in emotion-aware design in the near future?
Through a deductive/inductive hybrid thematic analysis of interviews with participants, our work contributes to better enhancing
emotion-aware design in the context of intelligent cockpits. We provide critical insights into how to incorporate emotion features into
human-centred design to enhance efective human-vehicle interaction, and suggest how user emotional needs could be understood,
interpreted and addressed with the role of AI. Furthermore, current
challengesfaced by practitionersin designing and developing in-car
emotion-aware systems are summarised, including safety and ethical concerns, unanticipated consequences of bad user experience
and balance between business values and market needs. We also
identify opportunities for future emotion-aware design in vehicles
to better satisfy user needs. Finally we refect on these fndings and
what they mean for the evolution of emotion-aware design in the
Discussion, within the lens of improving human-vehicle interaction
and AI development.
3 Online Interview Study
To address our research questions, we conducted an online interview study with 31 participants, all of whom worked professionally
in the realm of intelligent cockpits including product designers, engineers and AI experts. This study was approved by our university’s
research ethics board.
3.1 Study Design
A semi-structured interview approach was selected for our qualitative research, for the purpose of eliciting open-ended and in-depth
insights from each participant, and providing fexibility to look into
key points based on participant responses [67]. Given the confdentiality constraints within automotive industries, we hypothesised
that our participants would be more comfortable talking privately
and directly to a researcher as opposed to typing their responses
into surveys or having a multi-person workshop or focus group.
Finally, we chose to conduct our interviews online, as it allowed us
to recruit a diverse sample of participants from diferent companies
and job positions.
The outline of interview questions was developed based on our
reviews of existing literature and research questions. We noticed
that researchers studying emotions have yet to reach a broad consensus on how to defne emotion precisely [61]. In this paper, to
explore the design space of emotion features in automobiles, we
tended to include the emotional spectrum ranging around diferent
origins: from raw and brief afects, to direct and intense emotions,
to longer-lasting and difuse moods [6, 97, 99]. Participants were
prompted to refect more broadly on their experiences and insights
towards emotion-aware design without limiting interview discussions to certain prerequisites.
3.2 Recruitment
We identifed potential participants by disseminating recruitment
messages via social media and professional networks, and by reaching out to automotive companies that collaborate with our lab.
Participants were eligible for this study if they were employed
in designated roles aligned with intelligent cockpit development
within automotive industries and had relevant expertise, defned
as having at least three years of work experience (one year for
AI experts due to its novelty). Potential participants were further
screened through an initial registration form, assessing their suitability based on the inclusion criteria provided in Table 3 in Appendix. Inclusion criteria included designers who had worked on
or were currently working on the design, development or implementation of intelligent cockpits. For engineers, we included those
involved in developing hardware and software components, system
integration, and addressing required design specifcations related
to physical and digital interfaces for intelligent cockpits. To anticipate future fusion with AI, we also included AI specialists with
experience in developing or applying AI technologies to intelligent
cockpit applications or systems. Meanwhile, we specifcally sought
out those who either experienced integrating human emotions into
designs or considered emotions as part of their work potentially.
Table 1 shows a summary of, and Table 4 in Appendix provides
the complete interview participants’ demographic information. Our
sample consisted of 31 participants (13 female, 18 male), with an
average age of 36 (Min = 23, Max = 48). The 31 participants were
from 16 diferent companies ranging from startups (less than 50
employees) to global multinational corporations (over 50,000 employees), with an average of 5.7 years in the automotive industry
and 3.8 years specifcally in the domain of intelligent cockpits. An
overview of detailed information about participants’ academic background and job roles is presented in Table 2. To protect data privacy,
we ensure that participants’ jobs cannot be tracked back to their
identities.
3.3 Procedure and Interview Protocols
Potential participants were informed about the overall goals of this
study and provided with a general outline of interview questions by
email prior to the online interview. Upon preliminary agreement,
they were invited via a link provided in the email to complete a
registration form which included details on demographics, qualifcations and occupations, streamlining the interview process and
categorising basic information automatically.
At the beginning of each interview, participants were reminded
that they could choose to skip any questions or end the session at
any time. Verbal consent was then sought for the audio recording
of the interview, following by analysis of their transcripts, as well
as the use of anonymous quotes in the paper. Once participants
agreed, the interviewer began the recording and proceeded with
the interview where the interviewer and interviewees shared the
same native language. To ensure great consistency of the research
goal, each interview was conducted by the same interviewer (frst
author), while a second researcher (also author) was presented to
take notes.
We started the interview with an introduction given by the interviewer to ensure each participant understand the purpose of this
research. And for each participant group, we describe the interview
protocol below and they are provided in Supplementary Materials.
In case the participant expressed a lack of knowledge of or diverged
from the topic, we diverted to show a collection of emotion-related
technologies and existing design practices of emotion features from
carmakers as probes at this point and continue to delve into envisioned roles of emotions for future. The interview process lasted
approximately 60 minutes and each participant was compensated
with a $100 coupon for their participation.
3.3.1 Designer Protocol. In the interviews with product designers,
participants were frst asked to generally describe their design role
and workfow during the cockpit design process, and then dive into
more details by having them introduce one or two specifc projects
they had undertaken related to the behaviours or conditions of
drivers and passengers. The focus was placed on understanding
how human emotions were integrated in their design practices,
the challenges faced during the process, and the strategies used to
address these challenges. Based on current designs, designers were
asked to refect on limitations and opinions towards emotion-aware
cockpit design, and to share their thoughts on how human emotions
could be better incorporated in intelligent cockpits. The interviews
concluded with an exploration of designers’ perspectives on how
emotion-aware cockpit design practices might evolve in the near
future.
3.3.2 Engineer Protocol. In the interviews with automotive engineers, we asked them about their experiences and responsibilities
in the development of cockpit functionalities, particularly focusing on the integration of human emotions into intelligent cockpit
designs. Engineers were then prompted to talk through specifc
projects they had worked on, highlighting the engineering tasks
involved, the requirements that needed to be met, and how they
collaborated with design teams to address the emotional issues.
The pros and cons of emotion-related technologies were also mentioned. Towards the end of the interview, we encouraged engineers
to envision possible resources for emotion-aware applications and
actions or tools to support designers in early-stage cockpit design,
as well as their thoughts on how the development of emotion-aware
systems might evolve in the future.
3.3.3 Developer Protocol. In the interview with AI experts, we
asked participants to describe their contributions towards the development of AI systems for intelligent cockpits, with a particular
focus on emotion-related applications. Then the experts were asked
to walk through projects they had been involved in, detailing the
AI technologies they developed, the types of emotion data they
used, and how they collaborated with designers and engineers to
integrate AI systems into existing cockpit environments. At the
end of the interview, we explored with AI experts their views on
challenges and opportunities in AI-driven emotion recognition, as
well as potential improvements in AI design practices that could
enhance the efectiveness of emotion-aware technologies in automotive contexts.
3.4 Data Analysis
We recorded all interview sessions, accumulating approximately 31
hours of audio footage. With initial auto-transcription, these recordings were subsequently manually verifed for accuracy. Five authors
participated in and continuously and collaboratively discussed the
codes and themes throughout the data analysis process, in which
the key steps were summarised in Figure 1 and the complete process
was provided in Figure 3 in Appendix.
We employed a deductive/inductive hybrid thematic analysis
approach [41, 120]: First, a code manual was created based on our
research questions and interview guide, using a top-down deductive
coding approach. Six domain categories were formed, namely: (1)
workfow for designing emotion-aware systems; (2) design methods,
principles and frameworks used; (3) emotion-related practices; (4)
challenges and opportunities in emotion-aware design; (5) future
of emotion-aware design; (6) strategies and resources needed for
future.
Everyone familiarised themselves with the transcripts and generated hundreds of initial open codes from transcripts independently
where these broad categories served as a guidebook to help us retrieve data from the transcripts. In our codes-to-theme process, the
fve researchers frst converted their own coded data using Miro 1 into digital statement cards which included the original quote
from participants, participant ID (D1-D17, E1-E7, AE1-AE7), and a
one-sentence summary or keyword of the quotes on the card.
After this, we held a half-day online workshop to discuss and
compare all the statement cards, and determined on which cards to
retain, discard, or merge based on their similarity.
Given that large amounts of data and broad categories, using
the afnity diagramming technique [53], we then clustered the
codes and statementsto surface underlying sub-themes and develop
broader themes that emerged from the participants’ insights. The
fve researchers reached consensus over the course of several weeks
through both asynchronous discussion and daily meetings, during
which we further clustered those initialstatement cards and labelled
each cluster.
Another workshop was then held to identify themes by discussing and connecting these clusters. After further review of all
the statement cards underthe identifed themes and sub-themes, the
analysis was consolidated into four major themes and we present
our results in detail in the next section (Figure 2).
4 Findings
This section provides a detailed presentation of the fndings under
four themes summarised from the interviews (Figure 2). Throughout the fndings, we emphasised the pivotal roles of whom are
involved in early-stage design and refnement of emotion-aware
design within intelligent cockpits in automobiles, providing them
with guidelines to inform and inspire future design practices.
4.1 User-centred Design in Emotion-aware
Systems
Following user-centred design approaches in which all participants
agreed with, this section synthesises how to address user needs
related to emotional states and responses, in the role of designers.
As D2 described,“Principle of user-centred design remains unchanged,
as the product is always designed to serve end-users, and the needs of
potential users are always the key.”
4.1.1 Defining "Afective Scenario". The process of designing an
emotion-aware system begins with exploring in-car user behaviours
and experiences to gather requirements from users. This provides
a starting point for design practitioners to build upon: “Investigating users’ in-car experiences can help us be prepared for future
design in cockpit usage” [D17]. Additionally, D11 mentioned that
understanding of user needs ensures the efectiveness of emotion
features. However, participants indicated the lack of in-depth user
research at the early-stage of defning research goals (D1, D8, D11,
D13, D16, D6, D12). Most participants noted that design briefs
and objectives were always stated precisely by management-level
stakeholders, rather than user researchers or human factor experts.
D16 mentioned, “Conducting user research on a larger scale is crucial to fnd out user needs in common, to understand why human
emotions change.” In-depth user research enables organisations to
defne more relevant and efective research goals, ensuring that the
fnal design is aligned with both user expectations and business
objectives.
With in-depth user research, we can sweep up vast quantities
of data about customers’ in-car experiences and accumulate user
behaviours, to identify the regularity and variability in data. Human
emotions can be identifed later: “Identifying the situations that
emotions happen in helps fguring out emotions. For example, [fatigue2
after long drives,] anxiety in trafc congestion, or pleasure during a
journey” [D5]. Once sufcient behavioural data has been gathered,
the designer can defne specifc use case scenariosthatrefect typical
emotional fuctuations in in-vehicle environments. The importance
of analysing user data was also emphasised by the AE7: “We can
fnd use cases from user data. After analysing and modelling, use cases
for afective systems can be identifed and so that [we can] know what
user’s actual needs are, in so-called emotional needs.” Throughout the
analysis of these diverse data across diferent users and scenarios,
emotional needs can be accurately defned and understood.
4.1.2 Touchpoint Analysis in Customer Journey. Customer journey
mapping helps organisations understand in detail what customers
experience, feel, expect, and may encounter at each step from the
customer’s perspective [68]. This allows the design of systems
to respond to user needs accurately and promptly: “By analysing
high frequency scenarios and pain points in the user journey, we can
efectively enhance the user experience” [D8]; “using the customer
journey map, breaking it down to the indivisible atomiclevel to analyse
functional requirements” [D1]. This was echoed by D5, “Accurate
scene recognition with addressing pain points plays a signifcant role
in the overall user experience.” These identifed scenes or moments
are what we call “touchpoints”—the essence of improving user
experience and providing personalised service [27].
Many participants made it clear that defning and visualising the
contact points where the user interacts with the provider within
vehicles is important. This includes all moments of human-vehicle
interaction, such as touchscreen operation, voice control response,
and cabin environment adjustment. By clearly identifying and visualising these touchpoints, designers are able to better understand
and address users’ emotional needs, under certain scenarios, during
the journey.
During the early stages of design and development in vehicles,
some participants pointed out that high-fdelity and real-world
prototype testing were not feasible due to cost and confdentiality
constraints. D3 (a senior designer and product manager) mentioned
that the “virtual evaluation” approach can efectively simulate and
evaluate the design, performance and user experience in vehicles.
Specifcally, this utilises virtual simulation capabilities for 3D modelling and rendering of in-car environments and building VR/AR
virtual scenarios, which can ft in with physical prototypes:
“I think with mixed reality, you just put on AR or VR
glasses and you can pick any styles in interiors and surrounding environments [...] we can immediately make
changes based on your feedback. Because usually only
when users realise what’s on the market they know what
they need.” [D3]
4.1.3 Inclusive Design. Many practitioners agreed that user needs
always depend on users’ background and abilities, which can be
seen as “inclusive design” [D16] [85]. For instance, “Female drivers,
particularly those who have children, face signifcant stresses and
strains in balancing life and work. Therefore, crafting a cabin that is
friendly to women with emotional release is necessary” [D8]. Inclusive design aims to address issues of age, gender, economic status,
geographic location, language, ethnicity and accessibility and so
on. Some practitioners mentioned that, the needs of diferent user
groups, such as infants and the elderly, should be given special
consideration in design. They strongly recommended the use of
emotion recognition technology, for example, which enables vehicles to automatically make environmental adjustments by detecting
and analysing passengers’ physiological reactions, such as tackling
low arousal. These adjustments include modifying the seat tilt, adjusting the air-conditioning temperature, or playing soothing music,
thereby enhancing passengers’ comfort and sense of security.
4.2 Emotional Experience with the Role of AI
4.2.1 The Role of AI in Emotion-aware Systems. With wide availability of foundation models, AI agents, classifed as Level 3 based
on OpenAI defnition [35], have gained interests among automotive
practitioners, far beyond capabilities of large language models: they
can proactively interact through sensors, prompt itself to complete
tasks; update with any API in real-time; use multiple models or
agents (AE2, AE3, AE6). AE4 provided a ftting analogy, “If AI evolving into such an independent and autonomous intelligent agent in
vehicles, then it becomes particularly important to monitor users’ emotional states and understand human emotions.” Furthermore, AE5
emphasised the multimodal capabilities in AI agents: “AI agents are
able to recognise text, speech and process images.” For in-car agents,
“AI needs to analyse the behaviours and actions of drivers to accurately
predict their emotions and needs, which rely on rich data inputs and
[are] more passive while interacting” [D7]. Thus, collecting rich
and reliable data is essential. Data analysis can also help defne
scenarios with regularity and repeatability in data.
“The key to achieving this is a vast amount of user data,
especially the data generated during user interactions
with vehicle interfaces or systems. By analysing this
data, AI can be more proactive to fulfll users’ emotional
needs, providing a more personalised experience.” [A7]
Meanwhile, AI ability in generalisation3 was highlighted in the
interviews, suggesting that the scenario-based design method may
no longer be the solution:
“"We’re currently adapting to specifc scenarios, but
what we aim for in the future is designing without scenarios, where services are automatically tailored to any
situation with minimal human intervention, making
the experience smarter and more personalised.” [D4]
4.2.2 Definition and Classification of Human Emotions. Automotive practitioners were critical of discrete emotion theory, saying
that “currently the understanding of emotions is not sufcient” [AE5],
and “even the basic question of ‘what is emotion’ remains unclear”
[AE7]. One participant highlighted concerns about emotion defnition on a deeper level:
"The frst challenge is that the amount of data is far
from enough and the classifcation method needs to be
more specifc. Not just simply dividing into six or eight
categories, but more complex. For example, ‘crying while
laughing’, generally is not taken into consideration in
normal datasets. Another thing is the subtle diferences,
such as, between a genuine smile and a forced smile.
Although they look very similar, emotionally, these two
states are completely diferent and need to be distinguished." [D4]
In addition to exploring complexity in emotions, D8 and D1 expressed doubts about the diversity that infuences emotions, such
as “personal past experiences, cultural backgrounds” or “human behaviours across diferent countries and regions". Thus, a more practical and context-sensitive classifcation method is needed to account
for these behavioural and psychological variations across diverse
contexts and backgrounds among users. Otherwise, the design
would be limited: “current classifcation and grading of emotions are
limited, so we can only provide limited emotional feedback” [D15],
and “we need a method that can quantify these diferences” [D11].
To address these issues, the signifcance of human factors and ergonomics, especially in the areas of psychology and sociology, was
pointed out by many participants: “We hope these expertscan provide
theoretical models to help us identify which factors infuence emotions
the most and ofer some design guidance. This will make emotional
design more controllable and targeted” [D13]; “[This] further enriches
our research perspectives and design methods to better apply emotion
features” [D15].
4.2.3 Emotion Predict Actions. People naturally understand that
emotion predicts actions. That is, under certain scenarios, the understanding of emotions could delineate the type of human behaviours.
In the simplest term, angry people aggress. With advanced defnition and classifcation methods of emotions, practitioners believed
that AI can provide more precise and personalised decision-making,
rather than meeting comfort criteria (D1, D8, D13, D15, AE5). Therefore, in-car systems could provide diferent solutions adapted to
diferent emotions: “When people look angry, the system can automatically remove disruptive content, such as advertisements, to
prevent further triggering their negative emotions. Similarly, when
users feel sad, the system can chill the mood by playing relaxing music”[D15]. By monitoring passengers’ emotional states and changes,
current practices have proven that predicting actions can improve
the design of safety measures in cars where potential risks and
unexpected behaviours can be predicted to efectively prevent accidents.
Some participants mentioned that mental models can help recognise patterns in identifying emotional triggers and responses, to
predict behaviours based on these emotions. D16 believed that understanding the elements involved in a problem is necessary when
solving it. For instance, “when analysing trust issues between users
and autonomous driving technology, mental framework can assist in
understanding reasons for distrust” [AE2]. Thus, it allows the system to efectively analyse emotional triggers and accurately predict
user behaviours, thereby designing solutions that better meet user
needs.
4.2.4 Emotion Feedback Mechanisms. Most participants shared the
belief that auditory-vocal system would be the mainstream form of
emotion-aware interaction in intelligent cockpits. As AE7 stated,
“Almost every carmaker regards voice interaction as an essential functional module.” Some systems use voice recognition to understand
the user’s emotional state. This allows them to provide customised
feedback: “Emotion-aware interaction inside the vehicle is done by
voice” [E3]. In contrast, visual systems are mainly associated with
the design of physical spaces in cockpits, where user interfaces,
ambient displays and lighting stand out: “The hardware in future
cockpits might change dramatically, like the shift in holographic projection technology will completely change the way of how we interact
with screens” [D9], or “traditional displays might be replaced by more
fexible screens or projections as automation level increases” [D11].
To provide engaging emotion feedback, the role of virtual characters was highlighted by many practitioners, where imagination
turns into tangible art: “it bridges the gap between a machine and
a person, turning the relationship in cars from one between a tool
and its user to something more like friendship” [D17]. This anthropomorphic design (D9, D17, AE1) transforms the system from merely
a tool into something akin to a friend which strongly enhances
emotional connection between the user and the cabin.
However, D4, E1 andAE6 were critical about the lack of applicationdriven system design and insufcient details in design. E1 questioned whether “even if all these technologies come true, but is it
worth doing so much?” D6, a senior interaction designer, expressed
that while the technical solutions exist, the implementation of emotion features without application scenarios would be challenged
by the team leader of whether this feature is genuinely necessary.
Thus, D3 and D7 suggested that the introduction of intelligent recommendation and planning features can address this defciency:
“AI helps me plan my routes and recommend suitable places whenever
and wherever possible. It should recommend not just top spots from
the internet, but places based on my personal needs. This kind of
intelligent recommendation and planning is what I really need.”
Rather than virtual voice or ambient light, associated applications can be provided to users, in a proactive behaviour. As D1
mentioned, “With emotion data inputting into AI models, then it
changes from doing what it says to doing what it thinks. This change
is personalisation.” With this driving force, emotion-aware systems
can interoperate with real-world applications and create practical
values. Recently, many applications like TikTok, YouTube, or Instagram, analyse thousands of signals by users to fnd out what kind
of content they mostly want to see, and make recommendations.
Similarly, with so many signals collected from users within vehicles,
personalised recommendations and planning can be achieved and
collaborated with a variety of applications and platforms (D2, D3,
D14, D16). As D2 summarised, everyone desires for a “Perfect for
themselves” experience. And D3 commented that the vehicle will
be like a close friend, always caring for user’s emotions, providing comfort during the drive, and even adjusting the driving style
based on the user’s preferences. For instance, “after analysis, the
navigation system would recommend the nearest McDonald’s and
ofer your favourite Big Mac meal for pre-order” [D16].
In the long term, despite basic emotions, more complex emotional states or responses, like “sense of achievement”, or “sense of
belonging”, cannot be defned but deeply impact on the way of
in-car interaction: “as future vehicles increasingly evolve into multifunctional spaces, emotional experiences extend far beyond comfort
or satisfaction provided by functionality, like sense of belonging. To
address these advanced emotional needs, future design needs to create
new scenarios and experiences” [D8].
4.3 Challenges in In-car Emotion-aware
Systems
4.3.1 Prioritising Safety. Safety is still the priority, with emotionaware system coming next, serving as the support to reduce users’
negative emotions. AE7 and D3 stated plainly that safety is the
primary emotional need for everyone. D14 further emphasised,
“When the driver is in stress, the system should prioritise activating
safety assistance features to ensure the vehicle’s stability and safety.
Later it will also provide emotional feedback based on the driver’s
mood changes.” This emotion-aware functionality is primarily used
to alleviate the driver’s negative emotions, with the ultimate goal
remaining safe driving. Meanwhile, safety design includes internal
and external environment, particularly the safety of pedestrians and
road environments. E1 highlighted the importance of strict safety
standards in vehicle design: “these safety designs are mandatory
and aimed at protecting drivers and passengers in the event of an
accident, while also ensuring the safety of pedestrians.” This means
that the safety design inside the vehicle is as important as external
protective measures, focusing not only on those inside the vehicle
but also on external safety factors.
Managing the complexity of an emotion-aware system is a challenge both from the design and technical perspective. For an in-car
system, it is a massive system with multiple inputs but always ends
up with only one output to users. Thus, the selection of information
is the crux. For instance, “if the system keeps bombarding the user
with emotional notifcation elements like holiday greetings, weather
alerts, or vehicle condition warnings, it might cause information overload and annoy the user” [D11]. To avoid this, the system should
use a priority mechanism to ensure that crucial information is prioritised—like alerting the user that the vehicle needs recharging is
more important than holiday reminders, reducing the interference
of irrelevant information on the driving experience.
4.3.2 Privacy and Ethical Considerations. Participants pointed out
that privacy is a signifcant concern within the realm of emotionaware systems. Modern vehicles are no longer just a means of transportation, but highly complex data centres on wheels equipped with
cameras, sensors, IoT devices, and connectivity features. They can
easily obtain any personal data, including the driver’s behaviour
and habits, location, preferences, conversations, and sensitive information that users did not intend to share. E2 was worried about that
this often triggers user’s privacy concerns: “people might feel they
are being overly monitored as the car is equipped with many cameras
and microphones.” Although the smart system requested for authorization in advance, “users are still wary of potential leakage of their
personal data” [D12]. However, the risk to privacy increases because vehicles not only collect data but may also share it with third
parties such as insurance companies, advertisers, business partners
and government agencies. In the context of autonomous driving,
many participants pointed out that sharing with cloud platforms
carries the risk of privacy breaches: “as the automotive ecosystem integrates with cloud platforms, users’ data could potentially be shared
across diferent systems” [D17]. However, several participants noticed there was a lack of data privacy laws and regulations, as well
as technologies for prevention, like decentralisation: “decentralised
technology can reduce data exposure on public networks, allowing
data to be stored locally to further protect user privacy” [D15].
Some participants stressed the importance of training systems
on unbiased data and ensuring equal treatment for users. This is
due to concerns about social disparity but the system might be
applied in mass production vehicles, as well as the possibility of
biased data leading to ofensive responses: “The intelligent assistant
might respond in ways that leave users feeling disappointed or even
ofended” [E2].
4.3.3 Serious Consequences of Bad User Experience. Complexity
in emotions causes unanticipated consequences that may not meet
user requirements. For example, misinterpreting subtle emotional
cues leads to inappropriate responses: “The system’s understanding
of my facial expressions or feedback wasn’t accurate enough and its
responses were not fast enough. This made me confused, and I had to
try several times to make the system understand my intentions.” As a
result, user expectations may be lowered when the emotion-aware
system cannot correctly identify or respond to their emotional
states. As D3 noted, “the challenge in emotional design is mental instability and users’ emotions are easily infuenced by external factors,
making them elusive and hard to predict.” This instability and variability make it more difcult for emotion-aware systems to respond
accurately to the user’s real-time feelings. D11 added, “If emotional
interaction cannot meet user expectations, it can cause signifcant
frustration for users. For instance, the system may recommend music
at a wrong timing.” These inappropriate feedback and poor-quality
interaction experiences can further trigger user dissatisfaction, and
ultimately this immature product may ruin their trust.
While much of the research focuses on eforts to understand
emotions, many participants were doubted about the reliability
of AI/ML models, such as the explainability and black box efect
of AI models, or lack of high-quality and comprehensive afective
annotations.
"When we develop an AI model, we are often dealing
with a black box. [...] we can only rely on driving experiments and data analysis to attempt to understand
the model’s behaviour." [AE3]
Thus, whether the adaptation of AI defne user needs precisely
remains an open question. D2, as a product manager, thoughtfully
refected on the impact of AI on implementations, “Many automakers have already started integrating AI. I think the frst thing we need
to clarify is whether consumers need these features.” Similarly, D7
supported this view and pointed out, “The product team needs to understand the technology, create scenarios, and analyse user needs” to
ensure that the technological solution aligns with user expectations.
4.3.4 Designing Solutions that Satisfy Both Business Values and
Market Needs. Automotive industries are mainly cost-oriented. D1
and D14 noted that when selecting technical solutions, putting specifc demands might require a high-cost solution where the “return
on investment (ROI)” for such an approach is not feasible. This highlights the trade-of between cost and return as a common challenge
for the industry. With high complexity in emotion-aware system
development, car manufacturers and related industrial partners
might opt out because of high budgets but low return on investment: “Quotation is the primary. Diferent cars, depending on their
positioning, have strict cost limitations from the very beginning of
the project. When it comes down to the smart cabin, it will certainly
have a cost constraint” [D5]. This sensitivity to cost makes it cautious when adopting new technologies, especially those high-cost
technologies but difcult to achieve in a short term. Thus, solutions
that balance user needs and market needs are challenging. Another
limitation is that carmakers tend to merge pre-existing solutions
from industrial suppliers: “While it ofers convenience, this "of-theshelf" solution is general and often come with pre-packaged attributes
that may unintentionally incorporate unnecessary elements, afecting
the end-user experience” [D1].
To gain buy-in from management-level stakeholders, how to
validate the efectiveness of systems and gain market acceptance
is what they focus on. “Convincing investors or bosses to trust the
outcome from emotionless machines is not easy. We usually lack a
bottom-up plan to accurately validate the value of the system” [D16].
Therefore, a well-designed validation process is crucial to ensure
that the actual benefts from technology can be demonstrated, gaining the trust of decision-makers and market acceptance. Satisfying
business values can carry implications for brand ownership, equity,
and loyalty, all of which impact adoption. Also, good products will
not succeed without marketing:
"[For Waymo robotaxi service] Although obviously it is
a marketing tactic, it efectively dispelled public anxiety
about autonomous driving and enhanced their confdence and acceptance towards AV." [D8]
4.4 The Future of Emotion-aware Systems in
Vehicles
4.4.1 Emotion Care and Support. Some participants envisioned a
caring and support system that involves real-time emotional inputs
from users. They proposed that understanding emotions in humanvehicle interaction can bring many benefts, which contribute to
well-being and efectiveness in life: “The system can ofer emotional
care, especially in quiet environments or being alone” [D11]; the
system could provide instant emotional care and positive feedback
when drivers feel “stressed or frustrated” (D11, D14), thus increasing
the enjoyment of their journey and the willingness to drive (citing
D4, D8, D10).
Additionally, more than assistance, the system provides emotional support. Several practitioners highlighted the positive impact
on support to advanced emotional requirements: “sense of belonging”, “social attributes", “companionship”, or even “enhancing social
relationship between occupants in the car”. It’s really machine augmenting human (D13, D17, E3, AE1): “The role of AI in cars is an
assistant, and it might eventually become a partner or even a friend.”
This refectsthe possibility that future in-vehicle systems could take
human emotions as a core input, integrating them into advanced
AI models to efectively provide emotional support.
4.4.2 Supply Chain of Information. Given the vast amount of user
data, participants expressed hope from two perspectives. First, the
concept of shared AI assistant between vehicles and all the smart
devices, like the fctional character of “Jarvis” in Iron Man. As E1
mentioned, “Smartphones and Vehicles connect together to share the
same memory network.” This integration enables cross-device interaction, from the smart devices to vehicles, enhancing the synergy
between devices. It also allows the vehicle to provide customised
interactive experiences based on the user’s daily behaviours and
habits. Secondly, the emotion-aware system can communicate with
other systems in vehicles, including human-machine interfaces,
navigation systems, and automated driving systems. Eventually,
the integration ofers a comprehensive solution, allowing users to
share data and information among diferent systems in vehicles, as
well as diferent external intelligent devices: “If in the future, the
entire ecosystem is interconnected, and they all share with a cloud
platform, then I wouldn’t have to switch between diferent platforms
or AIs.” This integration solution provides users with a more personalised and seamless interactive experience.
4.4.3 Service Design in Emotion-aware Systems. Emotion-aware
systems seem to provide service more than assistance in the future.
As D5 stated, “The opportunity for emotion-related technologies is
ofering services.” This refects a shift in emotion-aware design, moving from merely monitoring and responding to proactively ofering
services that improve passengers’ emotions and experiences. As the
autonomous driving technologies was highlighted, the in-vehicle
experience of drivers was considered occupants engaging in their
choice of activities such as working, chatting, or relaxing during the
journey. In this context, the need of emotion-aware systems is to
provide customised journey experience and service for passengers.
The “RoboTaxi” service was highlighted by D11 with its comfort
and efciency for passengers on the move, where emotion-aware
systems can act like traditional taxi drivers to enhance acceptance
in AV. D5 pointed out that in the absence of a human driver, robotaxis tend to rely on advanced emotion-aware and scenario-aware
abilities to meet user service needs. Thus, Private automobiles are
widely considered an unsustainable future personal mobility solution. “Without traditional driving behaviour, vehicles shift towards
trends of sharing. This shift means that vehicles will no longer focus
on driving convenience or safety, but may instead pay more attention
to the value that the cabin can bring in the mobility experience itself”
[D17].
4.4.4 Being Proactive but with Boundaries. Many participants indicated their interest in proactive behaviours from emotion-aware
systems. Currently the interaction mainly relies on user commands:
“With triggers atcertain conditions, voice assistantcan respond to users
using a pre-installed and pre-defned emotion database” [D7]. Compared to passive interaction, with the lack of commands, proactive
system needs to recognise emotional states through inputs from
users, vehicles and external environments. “Proactive behaviour is
the key” [AE7] to develop AI’s potential to reduce cognitive load
and anticipate user needs. However, some participants expressed
their concern about AI being too intelligent or aggressive in responses. AE2, D11 and D7 all recommended that “boundary” is
good word to describe the assistance provided by emotion-aware
systems, which helps to defne what you are comfortable with and
how you would like to be treated by systems.
5 Discussion
In this section, we frst discuss research opportunities and ways
to rethink the role of the user in emotion-aware design based on
human-centred design approaches. Then, we discuss two main conficts in the fndings and how reconciliation eforts may be required
during the design process to improve emotion-aware design in the
future.
5.1 Rethinking the Role of “User”
Recognizing the limited perspectives of industrial practitioners
and researchers involved in vehicle design, user-centred design
approaches are widely accepted in automotive user interface and
systems [42, 44, 59, 96, 101, 105, 109]. In our research, we found
that most of the industrial design practitioners suggested involving
members of the public, potentially target user groups, and of course
domain experts in the design and evaluation of emotion-aware
systems. However, with some exceptions at the discovery stage,
involving users or anyone who is not part of the design team seems
impossible: automotive industries must protect their intellectual
property to remain competitive. This requirement may also raise
safety and ethical concerns in physical user testing. Thus, this
poses new challenges and research questions for both industrial
practitioners and the HCI community, including how to simulate
realistic user scenarios and user behaviours, how to gather feedback
from potential users without exposing technologies, as well as what
alternatives can be used during the testing and validation.
Our fndings suggest opportunities to rethink how user-centred
practitioners defne and visualise users’ behavioural and emotional
experience during early-stage design and refnement of in-vehicle
systems. In this phase, experience touchpoints, pain points and
opportunity points highlighted in use case scenarios are examined. For instance, prior work on user journey map and touchpoint
analysis in the context of in-car environments (e.g., [31, 60, 121])
suggests that the efectiveness and accuracy may be less critical for
practitioners than what those mappings reveal about people’s orientations towards the system. In our fndings, carmakers’ concerns
about unpleasant user experience may suggest opportunities (e.g.,
more personalised design) to reveal the capabilities and limitations
of both emotion-aware design and AI system development.
In addition, our fndings suggest the need to reconsider how to
understand human emotions efectively and respond to end-users
accurately with just the right way. For instance, we see technical practitioners proposing a more practical and context-sensitive
classifcation in emotions and applying mental models to predict
human behaviours. Prior literature has suggested dimensional or
categorical emotion theory to classify emotions [3]. Additionally,
emotional expressions, driving context and road information are
used to recognise emotional states. [2, 36, 71]. However, these approaches are still at the frst step of emotion-aware systems, and
automotive research and praxis have not yet developed comprehensive or standardised frameworks, principles, or practices for
practitioners to incorporate emotional features with functionalities
in human-vehicle interaction.
The participation of experts in human factors and ergonomics,
like psychology and sociology, was highlighted in the interpretation of human emotions and behaviours, collaborated closely with
the development of afective computing. Rather than defning user
needs at the outset, their expertise is more remarkable in measuring how well the system performs in adapting to diverse user
needs during the testing and evaluation of the system design. By
putting themselves in the users’ shoes, the role of "user" can be
flled and new tools or methods can also be found for standardising the evaluation process. While many recognise the necessity
of accurate emotion recognition and detection to improve driving
safety and user experience in cars, how to address user needs and
provide appropriate assistance or service remains an open question. Further research should thus explore ways to interpret and
respond to real-time emotional states in a manner that enhances
user experience.
5.2 Personalisation versus Generalisation
In our study, an inherent adversarial relation was found in the design and development of emotion-aware systems in vehicles that
users expect personalised service but AI aims for systematic generalisation capabilities. This new practice of adapting AI in emotionaware systems poses new challenges for both designers and AI
experts on how to satisfy the need for personalisation while building AI models that transcend the limitations of training datasets
and excel in diverse real-world scenarios.
Apparent from the interviews was that the goal of emotionaware systems was natural interactions tailored to users’ emotional
states, individual preferences and real-time needs, in a word, personalisation. This aspect is critical for enhancing user engagement
and satisfaction [38, 106, 117]. Personalisation begins with defning
afective scenarios within vehicles. These unique scenarios obtain
emotional states in real-time, such as anxiety during trafc congestion in a long-time driving. Meanwhile, inclusive design principles
encourage the system to accommodate users with diverse needs,
backgrounds and abilities. Indeed, both scenario-based and inclusive design cater for the concept of personalisation.
A key theme in this study was how participants envision automotive emotion-aware design in the near future. This, in the view
of automotive practitioners, involves the system’s ability to provide
service in a proactive behaviour and to care for and support users’
emotional needs during the journey. Additionally, high levels of
vehicle automation introduce the realm of service design where
the vehicle itself becomes a provider of personalised experiences
for passengers [65]. Beyond the role of transportation, users tend
to establish long-term social relationships (e.g., partnership [103],
companionship [102], guardianship [104]) with vehicles that acknowledge their individuality and address it based on their inner
feelings and thoughts [34]. This deep emotional connection transforms vehicles from mere driving cabins to trusted companions, as
the advent of autonomous vehicle technologies facilitates the shift
toward a demand-based system [84].
In contrast, AI/ML systems, especially those applied in emotion recognition [66, 94, 98, 125] and human-vehicle interaction
[11, 82, 107], always tend to converge across wide availability of
training and datasets and improve the model prediction performance (force neural networks or other ML algorithms to grasp
valuable information and concepts in existing scenarios to perform
better on new ones). Through our study, we saw AI experts motivating the creation of datasets containing emotional expressions in
diferent modalities for accurate emotion recognition. Vast amounts
of data are processed and interpreted to ensure system consistency
across diferent users and scenarios, as well as fne performance
on new, unseen data [66]. For instance, we saw technical experts
improving AI performance to recognise basic emotions, like happiness or anger, across diferent demographic groups and within
diferent use case scenarios. To that end, these eforts highlight
the importance of the concept in generalisation with emerging AI
capabilities for scalability, signifying opportunities in wide deployment of emotion-aware systems to users, vehicles, and automotive
companies [30]. However, potential implications in fndings have
revealed several limitations of generalisation. These include the
failure to detect nuances in emotions and meet individual’s expectations which can lead to a loss of user trust and acceptance. Also,
an incomplete database of emotional expressions across diferent
genders, cultures, or age groups may inadvertently raise biases.
Drawing upon this user-centred values and AI-driven techniques,
researchers, designers and additional practitioners need to make
a trade-of between personalisation and generalisation, considering both user needs and business values. Substantial prior work
has suggested domain generalisation [92, 116], transfer learning
[39], data fusion and ensemble techniques [66], or using "contextsensitive" AI [93] to potentially mitigate the limitations posed by
data scarcity. However, these approaches also acknowledge the
challenges of balancing personalisation and generalisation. HCI
and human-centred researchers and practitioners are called upon to
contribute a human-centred perspective to identifying and addressing what users’ general and distinctive needs. Further research
should explore how emotion-aware systems can be designed to
cater for these needs without compromising scalability or fairness.
This human-centred approach will be crucial, particularly at the
early-stage design in defning user needs, in ensuring that emotionaware systems provide both personalised and adaptable experiences
for end-users in real-world automotive applications.
5.3 User Needs versus Business Values
One substantial barrier to embedding emotion-aware systems into
real-world applications within vehicles, as pointed out by many
practitioners during the interviews, is questioning whether the
benefts outweigh the costs for car manufacturers. Moving from
conceptual designs into full-scale production, the high initial investment required to research, develop and implement emotion-aware
systems is not always seen as justifable, particularly when AI remains "black box" [54, 113]. Existing afective systems for in-car
usage have proven to be promising in emotion detection and emotion regulation for both drivers and occupants, as having serious
impact on road safety [38]. Indeed, despite its prevalence in road
safety and mental health [15, 124], its value proposition in business
and practical utility in real-world scenarios has not yet been defned. Similar concerns were raised regarding customers’ attitudes
towards emotion-aware systems. Unlike tangible vehicle features
such as comfort in ride or enjoyment in infotainment, emotional
interactions and implications are more implicit and our customers
may not be aware of their positive efects. Nevertheless, negative
user experiences, such as unresponsive or irrelevant reactions, tend
to underscore system shortcomings more prominently. On the fip
side, most of the customers fnd it difcult to articulate their preferences as they only know what they want until experiencing a
product frsthand.
Under the transition from driver assistance to autonomous driving, our study sheds light on the challenges and opportunities
for traditional vehicle manufacturers and related industrial and
business partners from the emerging design practices of in-car
emotion-aware systems with adaptation of AI. In our interviews,
while many practitioners mentioned that safety is still the priority,
as automation progresses, practitioners also pointed out that this
current form is not truly aligned with the concept of full autonomy. The focus will shift towards how vehicles emotionally engage
with their users in a way that enhances user experience. Emotional
states, in turn, can be a critical index for earning human acceptance and trust for both real-time and long-term engagements with
autonomous vehicles [26, 108].
Essentially, we contend that short-term agendas and long-term
strategies for emotion-aware design in automotive industries are
required to ensure smooth transition from manual driving to autonomous driving for human-vehicle interaction.
In the short term, we call upon further research in and beyond
automated driving to continue placing the human at the centre of
technology development. Importantly, new tools and methods are
needed that can ground the often speculative work of identifying
emotional states and addressing user expectations in their specifc
contexts of use. Previousstudiesin driver emotion research,same as
mentioned by practitioners, were mainly conducted in a controlled
and laboratory environment with driving simulators [32, 57]. Nevertheless, the problems persist even if the most advanced driving
simulators lead to situations where the experiment results are not
aligned with user expectations (especially when imitating highly
complex and realistic driving situations) [7]. Worse still, ethical
concerns or safety issues can arise, even when researchers employ
Wizard-of-Oz or on-road testing in real vehicles [123]. However,
although practitioners acknowledged in their interviews that such
approaches are not ideal, it is often the most feasible option they
have, given limited time and cost, as well as high confdentiality by
carmakers. As such, more research should focus on understanding
how—in scenario paradigms where human behaviours are identifed with user journey and experience mapping—emotional states
are associated with user needs in practices.
In the long term, a path forward is to continue drawing on AI
agents to create emotion-aware systems for social good. This is especially important for those exceptional, allowing personalisation
should human emotions difer amongst themselves but reconciliation eforts should ethical values difer amongst groups.
5.4 Future Work
In this study, eliciting practitioner perspectives across diferent
job roles and automotive companies enables the capture of a wide
range of perspectives and considerations within emotion-aware
cockpit design. This opens up opportunities for future work to
delve into the complex and massive scales of in-vehicle systems to
provide a more comprehensive analysis of emotion-aware design.
Future research can build upon our insights to examine a specifc
user group, such as female driver or elderly passenger, or a specifc
functional demand, such as head-up display or voice assistant, to
pinpoint end-users’ perceptions about and solutions to the realworld situations. Also, vehicle automation should be considered and
included in future studies to further promote emotion-aware design
solutions. Second, the design process in our study appears to be
primarily designer-dominated, refecting the typical structure of a
core design team that incorporates additional specialists and crossfunctional members as needed. This might be infuenced by the
sample size limitation, suggesting the possibility of other forms of
collaboration, such as involving AI experts in developing AI models
prior to designing specifc functionalities. Future research should
explore these alternatives or seek to improve existing patterns,
which may currently be ill-equipped to adapt the dynamic qualities
of emotion features and AI/ML systems. After this, specifc action
points, suggestions or guidelines could be tailored to particular job
roles.
5.5 Limitations
In this paper, we opted for a breadth-frst approach to better understand user-centred design of emotion features in vehicles, which
obviously comes at the cost of depth—we did not delve deeper
into any particular functional elements, automation levels, or user
groups. Also, despite our eforts to include as diverse roles involved
in the design of emotion-aware systems as possible, there are many
other actors we were unable to recruit in this study, such as product
planners. Given the qualitative approach we applied, we cannot
claim generalisability beyond our sample. Also, our study focused
on designers undertaking the leading work in creating emotionaware systems, in which functionalities, including AI models, were
developed after designing and defning how the system works. As
such, the questions were mainly explored through the perspectives
of design practitioners, rather than engineers, AI experts or other
specialists. We acknowledge that there are other forms of collaboration, and our fndings might be more or less transferable to these
processes based on human-centred design approaches.
6 Conclusion
This paper reports on in-depth interviews with 31 automotive practitioners, to understand how to address emotion features in humancentred design for efective human-vehicle interaction in real-world
practices. Through the interviews, we refne user-centred design in
emotion-aware systems in which user emotional needs are identifed within "afective scenarios" through user journey and experience mapping and situate the role of AI within emotion-aware
design. We also identify the challenges and opportunities that automotive practitioners envision for the future of emotion-aware
design in human-vehicle interaction. Based on our fndings, we
discuss research opportunities and ways to rethink the role of the
user in emotion-aware design based on human-centred design approaches. We draw implications for developing AI-driven emotionaware design for in-vehicle systems, and moving from conceptual
designs into full-scale production. We call on eforts from both
researchers and practitioners to explore emotion-aware design for
efective human-vehicle interaction, and to move towards best practices for enhancing user experience and maximising the capability
of AI.
Acknowledgments
This study was funded by the University’s Research Centre for
Future (Caring) Mobility (Project ID: P0042701), the Collaborative
Research with World-leading Research Groups (P0039528), and
the Projects of RIAIoT (P0043542) of The Hong Kong Polytechnic
University. And the Smart Trafc Fund (PSRI/64/2305/PR) funded by
the Transport Department of Hong Kong. We thank our interview
participants for their time and feedback.
