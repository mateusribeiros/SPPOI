Title: Dual data mapping with fine-tuned large language models and asset administration shells toward interoperable knowledge representation

Abstract
In the context of Industry 4.0, ensuring the compatibility of digital twins (DTs) with existing software systems in the manufacturing sector presents a significant challenge. The Asset Administration Shell (AAS), conceptualized as the standardized DT for an asset, offers a powerful framework that connects the DT with the established software infrastructure through interoperable knowledge representation. Although the IEC 63278 series specifies the AAS metamodel, it lacks a matching strategy for automating the mapping between proprietary data from existing software and AAS information models. Addressing this gap, we introduce a novel dual data mapping system (DDMS) that utilizes a fine-tuned open-source large language model (LLM) for entity matching. This system facilitates not only the mapping between existing software and AAS models but also between AAS models and standardized vocabulary dictionaries, thereby enhancing the model's semantic interoperability. A case study within the injection molding domain illustrates the practical application of DDMS for the automated creation of AAS instances, seamlessly integrating the manufacturer's existing data. Furthermore, we extensively investigate the potential of fine-tuning decode-only LLMs as generative classifiers and encoding-based classifiers for the entity matching task. To this end, we establish two AAS-specific datasets by collecting and compiling AAS-related resources. In addition, supplementary experiments are performed on general entity-matching benchmark datasets to ensure that our empirical conclusions and insights are generally applicable. The experiment results indicate that the fine-tuned generative LLM classifier achieves slightly better results, while the encoding-based classifier enables much faster inference. Furthermore, the fine-tuned LLM surpasses all state-of-the-art approaches for entity matching, including GPT-4 enhanced with in-context learning and chain of thoughts. This evidence highlights the effectiveness of the proposed DDMS in bridging the interoperability gap within DT applications, offering a scalable solution for the manufacturing industry.
Previous article in issue
Next article in issue
Keywords
InteroperabilityLarge language modelAsset administration shellDigital twinEntity matchingKnowledge representation
1. Introduction
Digital twin (DT), as a key enabler of Industry 4.0, blends the physical and virtual worlds, where physical systems acquire virtual representations, enabling them to communicate and cooperate with other components, environments, and humans, thus pushing forward a trend toward more interconnected and interactive manufacturing systems [1,2]. The increasing volume and diversity of digital data and information necessitate a more comprehensive knowledge representation of physical assets across their entire lifecycle, which should be acquired from disparate subsystems in various phases of the product lifecycle [2,3].
A fundamental challenge lies in appropriately representing knowledge in a way that is compatible with the established software infrastructure in the manufacturing sector, such as Enterprise Resource Planning (ERP), Product Lifecycle Management, and Manufacturing Execution Systems (MES). This ensures information consistency rather than the creation of isolated data silos [4]. However, much of the existing research has concentrated on proposing new DTs and ontologies to represent specific domain knowledge, often overlooking the need for integration with established software systems [[5], [6], [7], [8], [9]]. In this context, the Asset Administration Shell (AAS) emerges as a promising alternative for knowledge representation and the development of DTs, which is more aligned with the principles of Industry 4.0 and unifies the concept of both DT and ontology [10].
AAS, initiated by German industry stakeholders, has gained acceptance as the IEC Standard series 63278. Its metamodel defined in IEC 63278-2 [11] delineates rules and formats for information modeling. It also points out the need for matching strategies, enabling automatic mapping between proprietary data from existing software and properties defined in AAS models, ensuring seamless data integration. However, the standard primarily specifies an exact matching algorithm for data mapping that relies on literal matches of properties’ identifiers, leaving a gap in intelligent semantic matching capabilities [11]. In response to this gap, we propose a dual data mapping system (DDMS). This system not only enables data mapping between existing software and AAS models but also enhances the model's semantic interoperability by mapping between AAS models and standardized vocabulary dictionaries. It leverages AAS for interoperable knowledge representation and a fine-tuned open-source large language model (LLM) for semantic matching.
Given that AAS is a relatively new standard for information modeling and knowledge representation, its principles may not be widely recognized. To aid in understanding, we illustrate the concept of DDMS within the AAS framework in Fig. 1. The first mapping aims at seamless data integration of proprietary data from existing software into AAS submodel templates (SMTs) for the automatic creation of AAS instances for knowledge representation. For instance, knowledge of a simulation model created by a specific Computer Aided Engineering (CAE) software includes both the model file and its meta-information, which ensures users other than the original creator can understand and utilize the model effectively. Traditionally, this would require the creation of separate documentation for the model. With the AAS framework, e.g. through an SMT “Provision of Simulation Models,” this goal is achievable via a combination of diverse Submodel Elements (SMEs) in a hierarchical structure within the SMT to represent knowledge in a structured data format. Here, data and information from the proprietary document are mapped to corresponding SMEs. To further enhance the interoperability of this SMT, the second mapping introduces semantics to individual SMEs by matching them with a vocabulary dictionary, such as ECLASS [12]. This dictionary describes each entry with multiple attributes, including name, definition and unit. Referencing the corresponding entries ensures that each AAS property is defined in a neutral and universally understandable way, enriched with semantics. This facilitates a common understanding across different systems and users. An example of such a mapping triplet, which describes an aspect of simulation models, is illustrated at the bottom of Fig. 1. A more comprehensive explanation of the AAS language is provided in Section 2.
Fig 1
Download: Download high-res image (526KB)
Download: Download full-size image
Fig. 1. Proposed data mapping concept for seamless data integration (mapping 1) and interoperable semantic reference (mapping 2).

The proposed DDMS aims to enhance the practical utility of AAS as cognitive DT for knowledge representation and data integration throughout the product lifecycle within manufacturing systems. The core of the DDMS is a fine-tuned LLM designed to discern whether two entities are semantically matched. This leverages the capabilities of recent Natural Language Processing (NLP) technologies to process text representations of proprietary data, properties within AAS information models, and vocabularies from external dictionaries. Industry-specific knowledge is encoded and incorporated into the LLMs through a fine-tuning process. Furthermore, we follow the setting of the entity matching task and explore two distinct approaches for fine-tuning decoder-only LLMs: generative classification and encoding-based classification. Our main contributions are summarized as follows:
1)
For industry DT applications, we propose an innovative DDMS that leverages a fine-tuned LLM entity matcher. This is the first attempt to bridge the gap in intelligent semantic matching capabilities of AAS that is conceptualized in IEC 63278-2. A case study in the injection molding domain demonstrates the practical application of DDMS for the automated creation of AAS instances by integrating the manufacturer's existing data.
2)
For the domain of entity matching, this paper pioneers the fine-tuning and evaluation of open-source LLMs. An extensive experimental comparison between fine-tuned LLMs, GPT4 with prompt engineering, and state-of-the-art (SOTA) BERT-variant classifiers is conducted across five entity matching benchmark datasets and two AAS-specific datasets. The latter is established in this work by collecting and compiling AAS-related resources.
3)
For the NLP community, we empirically investigate the potential of fine-tuning decoder-only LLMs as encoding-based classifiers and compare it with the generative classification approach and the pre-trained LLMs with advanced prompt engineering. The results provide valuable insights for researchers and industry users into the optimal deployment strategies of LLMs in specific domains.
The remainder of this paper is organized as follows: Section 2 presents the background of knowledge representation and AAS in the manufacturing industry, and reviews SOTA semantic matching techniques and recent work related to LLM-based entity matching and text classification. Section 3 describes the proposed DDMS along with the fine-tuned LLM, which is demonstrated by a case study and evaluated on various datasets in Section 4. Section 5 concludes.
2. Background and related work
2.1. Knowledge representation
Knowledge representation involves how knowledge is structured in a machine-readable form, enabling computers to utilize knowledge for specific tasks. A fundamental approach of model-based engineering (MBE) for knowledge representation is the creation of information models, e.g., using semantic technologies, especially ontologies [13]. Ontology provides a structured framework for organizing information by defining a set of concepts and categories in a subject domain, along with their properties and the relationships between them [14]. In manufacturing, ontologies are standard to represent knowledge related to products, processes, and resources (PPR) [[5], [6], [7]]. Ontology may serve as the backbone of DTs. For instance, notable efforts by Dai et al. [8] and Göppert et al. [9] have showcased the creation and deployment of ontology-based DTs for machining tasks and general production planning and control (PPC), respectively.
However, the implementation of ontologies often results in isolated data silos due to their organization-specific, ad-hoc nature and lack of integration with existing software systems [15,16]. For instantiating ontologies in practice, one has to manually identify the source data in existing software systems such as ERP and populate the ontology instance accordingly. This raises a significant challenge, i.e. the effective mapping of data from existing software systems into an ontology instance. Additionally, ontologies typically represent knowledge through a combination of numeric and string data and often reference rather than directly include large or complex data types like images or real-time sensor data [16]. This approach cannot fully address the manufacturing sector's needs where the exchange of engineering models, drawings, and sensor data is frequent and crucial for various operational scenarios. Therefore, industrial DTs must address knowledge representation being able to represent these diverse aspects, carrying information about assets in a detailed and modular structure. A modular structure is particularly important as it allows tailoring the DT to specific use cases, ensuring only the necessary and relevant information is accessible to a particular user [10].
In comparison, AAS offers a more industry-centric approach to ontology formulation. On the one hand, AAS allows the integration of heterogeneous data sources, such as images and CAE model files, and dynamic data, such as operational data in the form of binary large objects. Additionally, AAS provides a more enriched semantic framework for describing each concept within its structure. While ontologies typically define classes through names and their relationships, AAS enables a more detailed semantic description. On the other hand, AAS is designed to be compatible with Resource Description Framework (RDF) formulation of ontology, facilitating a seamless transition between these formats [17]. Sapel and Hopmann [18] developed an AAS-oriented ontology for PPC in the area of injection molding and also converted it into the reusable AAS SMT “Technical Data for Injection Molding” [19]. Submodels (SMs) are the main building blocks within the AAS. Each SM captures specific asset aspects relevant to particular use cases. For instance, SM “Technical Data” provides a structured technical datasheet of industrial equipment. SM “Maintenance” and SM “Operational Data” aim to capture the information of maintenance and condition monitoring data respectively [20,21], which may contain administrative information, metadata, sensor data, and documents.
2.2. Asset administration shell
In the concept of AAS, interoperable information modeling is mainly achieved by the standardized metamodel, standardized SMTs, and concept descriptions for semantic referencing. Unlike ontologies or proprietary models often seen in MBE, the AAS metamodel standardized in IEC 63278-2 [11] specifies not only the syntax but also all building blocks of AAS using an object-oriented language. As depicted in Fig. 2, an AAS represents an asset mainly through SMs in a modular manner, enabling users to tailor the AAS by including SMs relevant to specific use cases. An SM comprises hierarchically organized Submodel Element Collections (SMCs) and SMEs. The variety of SME types, including e.g., Property, Multi-Language Property (MLP), File, and Reference Element (RE), serve as the basic building units of AAS, with each type fulfilling a unique role. For example, SMCs facilitate logical groupings of SMEs, Files provide a means to attach data files, and Properties offer asset descriptors for distinction and characterization. The attributes of these SMEs are precisely defined within the AAS standard, ensuring a uniform structure. These syntactic constraints and the rich diversity of the metamodel provide the basis for interoperable knowledge representation. Fig. 6 presents an exemplary AAS.
Fig 2
Download: Download high-res image (565KB)
Download: Download full-size image
Fig. 2. Illustration of Asset Administration Shell.

Furthermore, an essential idea of the AAS concept is establishing standardized SMTs for every relevant aspect of an asset. An SMT determines the data structure and individual properties through a combination of SMCs and SMEs. These pre-defined SMTs allow users to input values, creating AAS instances tailored to their unique application needs while maintaining overall data consistency across different systems. For instance, by utilizing the standardized SMT “Technical Data for Injection Molding”, manufacturers of injection molding machines can uniformly present technical specifications. This standardization ensures that customers can understand and compare machine specifications from different manufacturers on a like-for-like basis. Since these data models are maintained in a machine-readable format, they can be seamlessly integrated into various software systems, like PLM, facilitating data integration across disparate software platforms. Today, the development and standardization of SMTs is an ongoing effort within the AAS community, primarily promoted by the Industrial Digital Twin Association (IDTA) [19].
Last but not least, the semantics of a property is defined by a concept description, adhering to the schema defined by IEC 61360 [22]. This schema offers a detailed semantic description for each property, encapsulating aspects such as preferred name, definition, unit, and data type. The primary goal of these concept descriptions is to ensure that properties defined by the SMT developers are universally comprehensible, facilitating their interpretation across diverse systems and applications. While organizations can host their own repository of concept descriptions, it is highly recommended to reference external standardized vocabulary dictionaries, such as ECLASS [12] or the IEC 61987 Common Data Dictionary (CDD) [23]. Leveraging these external repositories guarantees that the properties incorporated into an AAS model are consistent with widely recognized industry terms and definitions. This reduces ambiguities and inconsistencies, enhancing the interoperability and clarity of data exchange within the manufacturing ecosystem.
2.3. Related work in semantic matching
The seamless and accurate data integration between existing software systems and AAS models, as well as the automatic semantic referencing of an AAS property to a standardized entry in a dictionary, relies on the algorithm of semantic matching. Semantic matching was originally proposed to identify whether two graph-like structured texts (e.g., database schemas or ontologies) have semantic correspondences [24]. It operates on two levels of granularity: element level and structure level. At the element level, semantic matching focuses on computing the similarities of individual elements represented at the nodes of these structures and comparing their concepts and attributes. At the structure level, it compares the structural information or schema of the data elements. This concept has been widely adopted in ontology matching and entity matching, as presented in the subsequent review of relevant literature.
Ontology matching targets the issue of semantic heterogeneity by creating correspondences that define relationships between entities of different ontologies [25]. Given that both the source and target in the matching process are ontologies, the matching considers lexical information at the element level and the structure of both ontologies. Meta-matching strategies aim to optimize the combination of various lexical and structural similarity measures to maximize the matching performance. This combination is searched by optimization algorithms such as genetic algorithms or particle swarm optimization [26]. From the NLP perspective, traditional similarity measures in ontology matching were based on statistical learning, guided by rules set by ontology experts [27]. However, recent advancements have shifted towards using neural language models for ontology representation learning. Khoudja et al. [28] built an autoencoder based on statistical similarity measures to learn sparse representations of ontologies. Chen et al. [29] introduced OWL2Vec, a technique inspired by word2vec and knowledge graph embeddings, for static semantic embedding of ontologies, enabling neural networks to learn dynamic contextual representations. Hao et al. [30] constructed a Siamese network with two multi-layer perceptrons to learn the lexical information and a graph convolutional network (GCN) to learn the structural information. This approach initiated the application of language models in ontology representation that incorporates both lexical and structural aspects. Afterward, pre-trained language models (PLM) such as BERT (and its variants known for the transformer architecture) have been introduced for lexical embedding, which can encode structural information [31]. To fine-tune BERT for paraphrase detection, a Siamese network that adopts the BERT architecture was constructed to take a pair of synonym terminological descriptions for unsupervised learning of sentence embeddings. This network architecture is well known as Sentence-BERT [32]. Wu et al. [33] adopted it for learning lexical embeddings in ontologies, coupled with a graph attention network (GAT) for structural embeddings. The combined embeddings, representing the entirety of the ontology, are then used for similarity calculations using various distance measures.
When the task is to identify all pairs across different data sources that refer to the same entity, this falls within the scope of entity matching. Entity matching concerns more than direct pair similarity comparison at the elemental level. It also involves additional steps like schema alignment and blocking to manage the complexity and enhance efficiency at the structural level [34]. Schema alignment aims to mitigate the impact of differences in data schemas and identify semantically related attributes of entities that should be compared for matching. GCNs and GANs are frequently used for structural embedding in entity matching [35]. Given the potential enormity of entities in data sources, comparing every possible pair is not feasible. This is where blocking occurs. It is a process to select a subset of candidates by efficient approximate nearest neighbor (ANN) searches in high-dimensional embedding spaces. Nevertheless, the central step in entity matching is the similarity comparison of entity pairs. In this step, deep learning-based NLP techniques have become the SOTA approach [[35], [36], [37], [38], [39], [40], [41]]. Its evolution also showcases a transition from statistical feature learning to neural language models and finally to PLM. The latest progression is the adoption of PLMs to learn the embeddings from the comparison pairs [38,39]. These models often incorporate an additional dense layer for binary classification to determine if a pair matches. To further enhance the performance of matches, additional strategies can be employed such as active learning and generative neural networks [39]. Both et al. [40] adopted a PLM for data mapping within the AAS framework.
The evolution of semantic matching approaches closely aligns with the progression of language modeling techniques in NLP [41]. This evolution began with statistical language models, like n-gram models, and then advanced to neural language models, such as recurrent neural networks. The advent of PLMs, such as BERT-variants, marked a significant leap forward, as they are trained on vast corpora of text and can be fine-tuned for diverse tasks. Currently, LLMs, which are essentially large-sized PLMs, have shown remarkable abilities in semantic comprehension and solving complex tasks [41]. A notable example of LLM applications is ChatGPT, which has significantly transformed the perception and application of AI algorithms. Users interacting with LLMs are not required to have an in-depth understanding of the model's internal work or to structure their tasks in specific ways to fit the model. Instead, they can directly interact with the LLM through prompts. A novel concept that has emerged in this context is in-context learning (ICL) [42], where LLMs generate predictions based solely on the context provided.
In the entity matching domain, recent research [43] tested the performance of GPT4 combined with ICL as a generative classifier. It can surpass the performance of fine-tuned SOTA BERT-variant models on several entity-matching benchmark datasets. It was also noted that open-source LLMs did not achieve similar levels of success without fine-tuning. Xia et al. [44] proposed a PLM-based prompt-tuning approach that generates paraphrased prompts of entity pairs as the inputs to pre-trained LLMs for generative classification. Wang et al. [45] investigated three prompting strategies: pairwise matching, tripletwise comparison of two candidates for a given query entity, and listwise selection of the best candidate for a given query and several candidates. These strategies were tested on various open-source LLMs, but their performance in terms of F1-score could not compare to a fine-tuned PLM.
In the AAS community, Xia et al. [46] developed a retrieval-augmented generation system that generates AAS instances from given unstructured input data. Within this system, the data mapping between AAS and ECLASS properties was conducted by a pre-trained LLM. The experiments revealed that GPT-3.5 outperformed open-source LLMs such as Llama2 and Mistral. However, only 20 samples of 5 different properties were evaluated.
In the broad area of text classification, prior research has mainly concentrated on prompt tuning and prompt engineering techniques, such as ICL and Chain-of-Thought (CoT), to achieve classification objectives [[47], [48], [49]]. Some studies have advanced the capabilities of LLMs to generate classification labels through supervised fine-tuning [[50], [51], [52]]. However, there has been a noticeable lack of focus on fine-tuning LLMs as traditional encoding-based classifiers. This oversight may be attributed to the fact that existing LLMs are primarily built on decoder-only transformer architectures, which are inherently not designed for text encoding.
2.4. Research gaps
Previous research in knowledge representation has largely focused on the development of new information models such as ontologies and AAS SMTs. However, instantiating these models often requires manual engineering effort to define dedicated data parsers. This involves identifying the data available in various software systems and assigning it to information models. This manual process is very labor intensive. To address this gap, our proposed data integration strategy using semantic mapping between proprietary entities and information models provides a solution to automate this process.
Furthermore, some researchers attempted to enhance model interoperability by referencing domain standards and external vocabulary repositories [15,18,21,40]. There is a notable absence of automated methods that allow users to easily locate referenceable knowledge. This is mainly addressed by the second mapping within the DDMS.
In the entity matching area, fine-tuned BERT-variant models and pre-trained LLMs with prompt engineering are the SOTA. There is growing interest in adopting LLMs for semantic matching, such as GPT variants [[43], [44], [45], [46]]. In terms of performance, GPT with ICL significantly outperforms other pre-trained LLMs and is comparable to fine-tuned BERT-variant models. However, the potential of fine-tuned open-source LLMs has not yet been investigated.
For a determined task, using ICL and CoT can result in lengthy and costly prompt texts that must be repeated for each input sample, leading to inefficiency. Additionally, the output text may lack control, even if a template is provided in the prompt instruction. In the broader field of text classification, LLMs are typically fine-tuned as generative classifiers rather than encoding-based classifiers. Our research aims to address this gap through extensive empirical studies, exploring the efficacy of fine-tuning open-source LLMs for the entity matching task, thus providing a more efficient approach to leveraging LLMs in specific tasks.
3. Dual data mapping system leveraging fine-tuned large language model
3.1. Dual data mapping system
The proposed DDMS, as illustrated in Fig. 3, introduces an integrated framework designed to facilitate seamless data integration from existing software into AAS-based DT for interoperable knowledge representation. This system comprises four functional components: Entity Extraction Parser, AAS Server and Registry, Semantic Matching Engine, and Concept Description Repository.
Fig 3
Download: Download high-res image (440KB)
Download: Download full-size image
Fig. 3. Proposed dual data mapping system and its functional components.

The entity extraction parser is tasked with extracting relevant entities with their attributes and values from the raw data exported from existing software systems. The raw data can be presented in structured or unstructured formats such as PDF, Excel, or JSON. Depending on the specific requirements of the target software systems, the entity extraction parser may need to be implemented using different techniques, including optical character recognition (OCR), database queries and dedicated file parsers. In this work, a pre-trained LLM is employed to automatically analyze unformatted strings from PDF files, extracting entities with attributes and values for storage in a database and preparing them for subsequent matching processes.
The matching targets of proprietary entities refer to the properties within the standardized SMTs, which are hosted on the AAS server and are accessible for querying supported by the AAS registry. The registry maintains records of identifiers and endpoints for all SMTs, facilitating the lookup of properties within SMTs. Properties within hosted SMTs are thus queryable via APIs such as REST-API. The combination of the AAS server and registry forms the basic IT infrastructure, supporting the operational framework of AAS in the production environment.
The matching task is enabled by the semantic matching engine, which is central to the data mapping system. This engine iteratively matches each proprietary entity with properties predefined in an SMT. Successful matches allow for the automatic population of entity values into corresponding AAS properties, thus instantiating the AAS instance. Additionally, to improve interoperability, the engine also matches AAS properties with entries defined in standardized dictionaries. These entries are typically structured according to a data specification template like IEC 61360-1 and implemented as ontologies.
The collection of these standardized entries is hosted in the concept description repository. It facilitates easy querying of the entry's attributes such as name, identifier, definition, and unit. Therefore, this concept description repository serves as a resource for the semantic matching engine to draw upon standardized definitions and terms, enhancing the semantic interoperability of the SMTs.
3.2. Semantic matching engine
The semantic matching engine is the core component of DDMS. It is designed with the flexibility and capability to address both data mapping tasks, namely matching proprietary entities with AAS properties and matching AAS properties with standardized entries. Proprietary entities are stored in a database table, AAS properties are structured according to the AAS metamodel, and standardized entries, e.g. in ECLASS, are presented as the ontology. To achieve consistent and effective matching across these varied data structures, the semantic matching engine resolves the differences in data representation by converting the attributes of proprietary entities, AAS properties and standardized entries into concatenated sentence-like texts. This design aligns with the elemental-level ontology and entity matching task.
Given that the matching target, such as the ECLASS repository, contains tens of thousands of properties, the semantic matching engine incorporates a preliminary step known as blocking in the entity matching domain [34]. This technique is similar to the concept of retrieval in the field of information retrieval. Blocking aims to narrow down the pool of potential candidates for the subsequent pairwise entity matching. By filtering out irrelevant candidates, it ensures that only the most relevant entities are considered in the pairwise matching phase.
In this paper, we mainly focus on pairwise entity matching, aligning with the general entity matching task, where matching is considered a binary classification problem. Let 
 be a matching source consisting of 
 data elements, while 
 be a matching target consisting of 
 data elements. Each data element is characterized by a string of concatenated attributes. The matching task is to find a LLM-based matcher 
 that predicts the binary label 
 for each pair of 
 and 
 guided by a matching prompt 
. The formulation of the LLM matcher, denoted as 
 is expressed as follows:
(1)
At the heart of the semantic matching engine is this LLM matcher. In this work, we investigate the fine-tuning of LLM as a generative classifier and as an encoding-based classifier, respectively.
3.3. Fine-Tuning open-source large language models as classifiers
Fig. 4 illustrates the investigated generative classifier and encoding-based classifier with different prompting and modeling options. The base language model refers to an LLM, which generates hidden states from tokenized inputs. The primary distinction between the generative classifier and the encoding-based classifier lies in the model head.
Fig 4
Download: Download high-res image (376KB)
Download: Download full-size image
Fig. 4. Investigated generative classifier and encoding-based classifier with different prompting and modeling options. The dark blue blocks indicate a model component, while the light blue blocks indicate inputs, intermedia outputs or final outputs.

The generative classifier has a causal language modeling head that predicts the next token from the given dictionary based on the previous tokens. The output logits represent the likelihood of each token in the vocabulary being the next token. This setup is the default modeling configuration for existing autoregressive LLMs. In our entity matching task, the generated labels are defined within the set 
, indicating whether two entities are matched or not.
The encoding-based classifier follows a conventional classification modeling approach. It employs a pooling layer to aggregate the sequence embeddings into a fixed-size representation for each input batch. These pooled sequence embeddings are then fed into a classification head to generate the final class predictions. In our case, the label set is 
.
We investigate different options for input prompts and pooling operations. The unstructured prompt refers to the concatenated text of the instruction and the entity attributes. Structured prompting uses span typing to preprocess the input texts by adding special tokens that categorize spans of text. Table 1 provides an illustrative example of how a prompt can be organized using span typing. Angle brackets are used to demarcate formatting spans, providing a syntactical cue to the model regarding the structural composition of the prompt. Attributes’ name of a data element further clarifies the nature of the information being presented to the model.
Table 1. Exemplary prompt structured by span typing.

<instruction>
You will read two sentence-like entities to be matched. Each entity has several attributes, such as name and description. Your task is to decide whether the two entities are matched (they refer to the same entity). If matched, please answer 'yes'. If not, answer 'no'.
</instruction>
<input>
<entity1> name: Architecture. definition: Construction type of a building or product. 〈/entity1〉
<entity2> name: Structure. description: Collection describing the model architecture 〈/entity2〉
</input>
<response> </response>
For the pooling operation, the last token embedding is the default setting of an autoregressive LLM, which only uses the last token embedding to represent the entire sequence for the classification. Additionally, we implement a mean pooling layer that calculates the mean values of all token embeddings within a sequence as its representation.
3.4. System implementation
Fig. 5 depicts the implementation of the overall DDMS, involving Python and Java components. The entity extraction parser implemented as a Python program utilizes a pre-trained LLM to automatically extract entities from raw data sources. For instance, the LLM is prompted to parse PDF documents such as technical datasheets of industrial equipment with the following instruction: “Extract all the technical specification data of a product from the PDF into a list of properties in a format of '<property name>, 〈value〉, <unit>', such as 'Nominal_Voltage, 250, V', without any explanation texts. For a missing value, use 'N/A'. Each row must only contain these three elements with two ','.” This method converts data into a structured table format, ready for storage in a database.
Fig 5
Download: Download high-res image (349KB)
Download: Download full-size image
Fig. 5. Implementation of the data mapping system. The dark blue blocks indicate the model or tool used.

It is important to note that this approach is a demonstrative implementation specific to the given raw data such as PDF files in our case study. In practice, the entity extraction parser should be tailored to the specific application and data format. It does not necessarily have to utilize an LLM, especially when the raw data is already presented in a structured form. For unstructured raw data, OCR may be also needed depending on data characteristics.
The infrastructure for the AAS server and registry, along with the concept description repository, is constructed using the Eclipse Basyx Java V2 SDK [53], which is an open-source AAS SDK and offers pre-implemented backend and frontend components as docker images. The standardized REST-APIs are provided for data querying, enabling communication between Java and Python components. The hosted AAS SMTs stem from both IDTA [19] and those developed within this work. The developed SMTs are published on our project website [54] and have been submitted to IDTA for standardization [19]. To compile the dataset for the matching task, a total of 44 SMTs encompassing 2376 unique properties are collected, providing a foundation to train and test an LLM matcher. As shown in the right side of Fig. 6, an AAS property contains multiple attributes according to the AAS metamodel, such as “idShort”, “description”, “semanticId”, “qualifiers”, etc. Among others, “idShort” and “description” that refer to the name and explanation of a property are queried for semantic matching. “semanticId” of a property is used to reference the corresponding standardized entry. As the dictionary of 27,423 unique entries, ECLASS V12 Advanced is utilized and hosted by the concept description repository in this work. As shown in Table 2, each ECLASS entry is presented in the form of ontology with the attributes such as “id”, “preferred name”, “definition”, “unit”, “domain”, “name scope”, etc. The attributes “preferred name” and “definition” of 27,423 ECLASS entries are used to compile the dataset for the matching task. The AAS-specific entity-matching datasets will be introduced in Section 4.
Fig 6
Download: Download high-res image (2MB)
Download: Download full-size image
Fig. 6. Exemplary SMT “Technical Data for Injection Molding Machine” with its model structure on the left side and the attributes of the selected property “MaxInjectionFlowRate”.

Table 2. Referenceable ECLASS entry “max. flow rate” for the AAS property “MaxInjectionFlowRate”.

<ontoml:property xsi:type=ontoml:NON_DEPENDENT_P_DET_Type id=0173–1#02-AAI739#002 guid=1e20666211d44aef9784a2ac835db882>
  <name_scope class_ref=0173–1#01-RAA001#001/>
  <date_of_original_definition>2011–02–11Z</date_of_original_definition>
  <date_of_current_version>2019–06–11Z</date_of_current_version>
  <date_of_current_revision>2019–06–11Z</date_of_current_revision>
  <revision>1</revision>
  <status>66</status>
  <source_ language_code=en country_code=US/>
  <preferred_name>
    <label language_code=en country_code=US>max. flow rate</label>
  </preferred_name>
  <definition>
    <text language_code=en country_code=US>largest possible volume which can be moved per unit time through the section of the device</text>
  </definition>
  <det_classification>K30</det_classification>
  <domain xsi:type=ontoml:REAL_MEASURE_TYPE_Type>
    < unit_ref=0173–1#05-AAA668#003/>
    < quantity_ref=0173–1#Z4-BAJ276#002/>
  </domain>
  <is_multivalent>true</is_multivalent>
</ontoml:property>
To address the inefficiency inherent in comparing each source element 
 directly with every element of the target set 
, a blocking mechanism is implemented to select the top-N candidates from 
 for pairwise entity matching. In this work, the blocking first leverages domain knowledge to filter a subset of 
. In case of data mapping between proprietary entities and AAS properties, the relevant SMTs are first selected from the total 44 SMTs according to the domain of proprietary entities. Similarly, for data mapping between AAS properties and ECLASS entries, the matching scope is refined according to the ECLASS classification system, which can be accessed on the ECLASS website [12]. To further narrow down the candidates, a pre-trained embedding model is employed to transform both the target subset and each source element 
 into embedding vectors. Top-N cosine ANNs are searched for a given 
 as the candidates. As the embedding model, we utilize an open-source LLM embedding model “SFR-Embedding-Mistral" [55]. This selection is based on the experiment in Section 4.3. The indexing and search processes are implemented using the Faiss library [56]. The source entity and the selected top-N candidates form the N entity pairs for subsequent pairwise entity matching.
For entity matching, we implement a unified experimental pipeline primarily based on the HuggingFace transformers library [57], ensuring consistent modeling and evaluation conditions across experiments. The generative classifier is implemented using the “LlamaForCausalLM” class, while the encoding-based classifier is implemented by modifying the “LlamaForSequenceClassification” class. To fine-tune the LLMs, the low-rank adapter (LoRa) [58] implemented by the PEFT library is employed [59]. Detailed experimental settings are presented in 4.4 Exp.2 AAS-specific entity matching, 4.5 Exp.3 general entity matching.
Given that each source entity involves N inferences performed on the retrieved N entity pairs, it is common for the model to predict more than one positive pair. This scenario is permissible and aligns with practical applications. The predicted results are used to rerank the candidates and presented as suggestions to human users, who ultimately make the final decision. This is particularly relevant when mapping between AAS properties and ECLASS entries, as more than one ECLASS entry may be referencable for a single AAS property. ECLASS entries are often defined with varying scopes and levels of detail, allowing for multiple valid mappings for a given AAS property.
4. Case study and statistical evaluation
This section delves into a practical application of the proposed data mapping system through a use case. It showcases how manufacturers can utilize this approach to create AAS instances by integrating their proprietary data. Additionally, extensive numerical experiments are performed to evaluate the LLM matcher by fine-tuning open-source LLMs.
4.1. Case study in the domain of injection molding
The case study showcases the utilization of the proposed DDMS in the injection molding domain, a crucial area in the mass production of plastic products. In the European plastics and rubber machinery industry, the organization EUROMAP plays a significant role by developing and disseminating technical recommendations for injection molding equipment. These standards benefit both equipment manufacturers and buyers through standardized specifications. In practice, manufacturers have to implement technical documents for their machines according to EUROMAP standards, often resulting in data inconsistency and challenges in knowledge management and data reusability. This can be observed by comparing technical data sheets from different suppliers for the same type of machine. In this context, AAS can serve as a single point of truth through standardized SMTs. By implementing standardized technical specifications, e.g., as recommended by EUROMAP, into SMTs, AAS facilitates the dissemination of structured and standardized machine data. This provides a reference to distributed software systems across different user groups and enables direct provision of structured data to customers, bypassing the traditional reliance on unstructured PDF documents. In our research project [54], several SMTs for the main injection molding equipment were developed according to EUROMAP standards and professional textbooks, containing pre-defined technical properties for injection molding machines, molds, temperature control units, and hot runner devices. An example SMT is shown in Fig. 6. More detailed information can be found in [18,54]. Manufacturers aim to map their proprietary machine descriptions to these SMTs, thereby creating AAS instances of their machinery, which support diverse internal and external applications, enhancing interoperability and accessibility of machine data.
The proposed DDMS functions in both the development and application phases of SMTs. The data mapping between AAS properties and ECLASS entries usually occurs during the development stage. Property definitions are initially specified by developers according to standards, professional literature, domain knowledge, or practical conventions. These proprietary definitions, characterized by "idShort" and "description," may be highly specific to particular use cases or domains. For instance, the property “MaxInjectionFlowRate” in the demonstration example in Fig. 7 is specifically defined for the injected molding process. Subsequently, developers seek to link these proprietary properties to standardized entries as concept descriptions, enhancing the property's interoperability and semantic clarity. There could be an exact entry for the specific property being queried. However, ECLASS entries are more often defined in general terms, independent of a specific use case or domain. As shown in Fig. 7, the DDMS begins with the blocking function identifying the ten most semantically similar ECLASS candidates for the query property, ranking them based on similarity scores. Despite the correct ECLASS entry being "max. flow rate" with the semantic ID “0173-1#02-AAI739#002”, it might not be ranked highly due to its broader definition, which does not explicitly mention the injection molding context. This situation underscores a common challenge: finding the most appropriate ECLASS entry when none may perfectly match the specific domain-defined property. Next, the block function forwards the ten candidates to the semantic matching engine for pairwise matching. In this case, despite the existence of multiple ECLASS entries with the preferred name “max. flow rate”, their definitions pertain to different domains, and therefore they are unsuitable for the "MaxInjectionFlowRate" property. Here, only the entry with the ID “0173-1#02-AAI739#002” is considered relevant by the fine-tuned LLM, showcasing the DDMS's capability to navigate through complex matching scenarios.
Fig 7
Download: Download high-res image (1011KB)
Download: Download full-size image
Fig. 7. Example results of the proposed mapping method between AAS properties and ECLASS entries, including blocking and the fine-tuned LLM matcher.

The data mapping between proprietary entities and AAS properties occurs when SMTs are prepared, and users aim to create AAS instances for their assets. Instead of developing application-specific data integration parsers, the proposed DDMS utilizes a pre-trained LLM as a universal entity extraction parser and a fine-tuned LLM for precise entity matching. Fig. 8 highlights the process and outcomes of applying DDMS to technical datasheets of injection molding machines provided by two different manufacturers. These datasheets, initially in PDF format, are processed by the entity extraction parser, resulting in the structured tabulation of technical entities. Taking "MaxInjectionFlowRate", as an example, this property is described by the two manufacturers as „Injection_Rate_PS“ in 
 and „Injection_flow“ in 
 respectively. In addition, the entity „Injection_Speed“ appears to be a related entity. These three proprietary entities are forwarded to the fine-tuned LLM matcher. Upon evaluation by the LLM matcher, "Injection_flow" is identified as the matched entity most closely aligning with "MaxInjectionFlowRate", corresponding to its definition.
Fig 8
Download: Download high-res image (1MB)
Download: Download full-size image
Fig. 8. Example results of the proposed mapping method between AAS properties and proprietary entities, including entity extraction parser and the fine-tuned LLM matcher.

This demonstration illustrates the challenges posed by semantic ambiguity within the manufacturing domain. Despite the presence of domain standards that define professional terminologies, manufacturers frequently choose to use their proprietary descriptions. This variance in terminology creates a significant hurdle for customers attempting to compare and select products, as the lack of consistency complicates understanding and decision-making processes. The adoption of AAS, with its rich semantic framework, for unified knowledge representation introduces a potent solution to bridge this gap. It enables a “common language” and understanding across the industry. The proposed DDMS facilitates this transition by mapping manufacturers‘ existing descriptions to standardized definitions, thus mitigating the issue of semantic ambiguity.
4.2. Experiment design and research questions
The effectiveness of the DDMS and especially the fine-tuned LLM matcher is further assessed through three distinct statistical experiments across various datasets from different domains, as presented in Table 3. Each experiment is designed to address several key research questions.
Table 3. Meta-information of the evaluation datasets in three experiments.

Datasets	Domain	Pairs	Pos.	Neg.
Exp.1 Mechanism of the Overall DDMS
AAS-ECLASS	Industry	–	491	2946
Exp.2 AAS-specific entity matching
PropEntity-AAS	Industry	2254	322	1932
AAS-ECLASS	Industry	3437	491	2946
Exp.3 General entity matching
Abt-Buy	E-comm	9575	1028	8547
Walmart-Amazon	E-comm	10,242	962	9280
Amazon-Google	Software	11,460	1167	10,293
DBLP-Scholar	Bibliography	28,707	5347	23,360
DBLP-ACM	Bibliography	12,363	2220	10,143
The first experiment evaluates the mechanism of the overall DDMS, specifically focusing on the blocking function for candidate selection and the entity matcher for reranking. This experiment addresses two research questions:
•
RQ1: Can the blocking effectively include the true positive candidate in the selected top-N candidates for a given query entity?
•
RQ2: Is the entity matcher necessary and effective in improving the retrieval results of blocking?
In this experiment, we consider the worst-case scenario where the referencable ECLASS entry is searched for each AAS property among the total 27,423 unique ECLASS entries. Notably, this scenario excludes human intervention with domain knowledge to pre-select relevant classes within the ECLASS repository.
The second experiment assesses the performance of the fine-tuned LLM matcher on two AAS datasets specifically established for this work. The PropEntity-AAS dataset is used to evaluate the entity matching between proprietary entities and AAS properties, while the AAS-ECLASS dataset is for the matching between AAS properties and ECLASS entries. This experiment aims to answer:
•
RQ3: Which fine-tuned LLM classifier performs better for entity matching, the generative classifier or the encoding-based classifier?
•
RQ4: Is it worthwhile to fine-tune open-source LLMs compared to using pre-trained LLMs with prompt engineering and fine-tuned BERT variants?
The third experiment extends the evaluation from the AAS-specific datasets to widely recognized entity-matching benchmark datasets [43]. The objective is to answer:
•
RQ5: Is our fine-tuned LLM matcher competitive against SOTA BERT-variant-based approaches [[36], [37], [38], [39]] and SOTA approaches based on pre-trained LLMs with prompt engineering [[43], [44], [45]] across general entity matching tasks?
In addition, the results for RQ3 and RQ4 can be validated in the context of a general entity matching task. All experiments are conducted within a consistent development environment on an NVIDIA DGX A100 System equipped with 8xA100 GPUs.
4.3. Exp.1 mechanism of the overall ddms
4.3.1. Dataset
This experiment is performed on the AAS-ECLASS dataset, containing 491 positive entity pairs. They represent the true mappings between AAS properties and ECLASS entries. The AAS properties in these pairs are part of the total 2376 properties extracted from existing AAS SMTs. The ground truth for their paired ECLASS entries can be determined based on the attribute "semanticID", as depicted in Fig. 6. These IDs were manually assigned by the SMT developers using the official ECLASS search engine. Out of the total AAS properties, 491 have been assigned corresponding ECLASS semantic IDs (IRDI beginning with “0173-1#02”), identifying them as positive pairs. The remaining AAS properties, which do not have matched ECLASS entries, were assigned private concept descriptions. 2946 negative samples are generated in two groups for training the entity matcher. Half of the negative samples are randomly chosen from ECLASS properties, likely not similar to the selected AAS properties. The remaining half consists of similar yet incorrect pairings. For each AAS property, the blocking function identifies the Top-N ECLASS candidates. negative entries are randomly selected from these candidates, simulating close but incorrect matches.
Table 4 showcases two representative positive pairs from the AAS-ECLASS dataset. Entity1 refers to AAS properties characterized by the attributes "idShort" and "description". "idShort" is typically an abbreviation of a property name, while "description" aims to describe the property meaning in its specific application. However, the "description" attribute can sometimes be empty, as seen in the first Entity1. This could be because the developers think "idShort" is sufficient to express the meaning or they provide only a general definition in the concept description, leaving "description" empty. The „description” attribute can also contain noisy information such as “'aas:langString': [{'@lang': 'en', '#text':” in the second Entity1. This noise is caused by the current AAS parser not completely cleaning the XML metadata.
Table 4. Representatives of positive pairs from the AAS-ECLASS dataset.

AAS property (Entity1)	ECLASS entry (Entity2)
idShort	description	preferred name	definition
Atex2Gas	None	approval according to ATEX II / gas present	information on whether a part is tested and approved according to ATEX II / gas or not
RemainingUsfulLifeDateTime	{'aas:langString': [{'@lang': 'en', '#text': 'Date and time when remaining useful life will be expected to be exceeded'}	time stamp (date and time)	mark attributed to an instant by means of a specified timescale, expressed as a date and a time
In comparison to the "dirty" AAS properties, ECLASS entries as Entity2 are mostly well-defined with the attributes "preferred name" and "definition". However, ECLASS properties may be defined in more general terms and, therefore, may not be semantically consistent with the corresponding AAS properties defined for a specific use case or domain. his poses a significant challenge for matching, as it requires the identification of semantic equivalence across general and specific terminologies. For instance, the second positive pair "RemainingUsefulLifeDateTime" and "timestamp (date and time)" are not exactly semantically matched. Entity2 provides a more general definition for Entity1.
4.3.2. Experiment setup and evaluation metric
The dataset undergos a train-valid-test split in a 6–2-2 ratio, resulting 98 positive test samples. The DDMS primarily involves two components: blocking for candidate selection and an entity matcher for reranking. The blocking function relies on a pre-trained embedding model and does not involve a fine-tuning process. It is directly evaluated on the positive test samples, where Entity1 is used as the query entity to search for the top-N candidates from the total 27,423 unique ECLASS entries. The selected candidates are ranked based on their similarity scores, ranging between 0 and 1. Among these candidates, only one may correspond to the positive match 
, which is the paired Entity2, serving as the ground truth for calculating the evaluation metrics: mean reciprocal rank at rank 10 (MRR@10), mean hit rate at rank 10 (MHR@10) and MHR@1. MRR@10 is a standard statistical measure used to evaluate the effectiveness of a retrieval system. For the entire query source set 
, the MRR is calculated as:
(2)
Where 
 in our case is the position of the true positive Entity2. For MRR@10, 
, when the true positive Entity2 does not appear within the returned top-10 candidates.
HR indicates whether the true positive Entity2 appears within the returned candidates, regardless of its rank. If MHR@10 is high enough, it means that in most cases, the true positive entity is found within the top 10 candidates, suggesting that the blocking is effective up to this level. MHR@1 aims to check if the blocking is good enough to directly identify the true positive Entity2. If MHR@1 is high, it indicates that the blocking often ranks the true positive entity as the top candidate and the pairwise entity matching step is not necessary.
The entity matcher relies on a fine-tuned LLM-based classifier. The detailed training and evaluation of the matcher are presented in Exp. 2 and 3. In Exp. 1, an entity matcher is first fine-tuned on the training and validation set of the AAS-ECLASS dataset. The fine-tuned matcher is then applied to the top-N candidates retrieved by the blocking function. For each pair of the query entity and the candidate entity, the matcher determines whether they are matched or not. If matched, the score of that candidate is set to 1. If not matched, the score remains unchanged. Finally, the top-N candidates are reranked with the updated scores, and the evaluation metrics are recalculated. An improvement in the metrics would indicate the effectiveness of the entity matcher.
4.3.3. Evaluated models
For the blocking function, several SOTA pre-trained embedding models are evaluated to determine the most effective one for the DDMS. The indexing and search processes are kept consistent in the experiment. The evaluated models include the commercial off-the-shelf model "text-embedding-ada-002–2″ deployed by Azure OpenAI Service [60] as well as "text-embedding-3-large" provided by OpenAI [60], the representative open-source LLM-based embedding model “SFR-Embedding-Mistral” [55], and the representative sentence-transformer-based embedding model “all-mpnet-base-v2” [61]. “SFR-Embedding-Mistral” was the top-1 model on the Massive Text Embedding Benchmark (MTEB) leaderboard [62], while “all-mpnet-base-v2” is the top model on the leaderboard of pre-trained sentence transformers models [63]. For the entity matcher, the best fine-tuned matcher “Llama2–13B-Encoding-str.” identified in Experiment 2 is employed.
4.3.4. Results and discussions
Table 5 presents the evaluation results. From the metrics, it is evident that SFR-Embedding-Mistral has achieved the best performance, outperforming even the latest embedding model "text-embedding-3-large" provided by OpenAI. The conventional sentence-transformer-based all-mpnet-base-v2 falls behind the LLM-based ones.
Table 5. Results of Exp.1. The metrics are presented in percentile. The first four models are embedding models, serving the blocking function. The last one combines the best-performing embedding model for blocking with the fine-tuned entity matcher model.

Model	MRR@10	MHR@10	MHR@1
SFR-Embedding-Mistral	68.6	88.8	59.2
text-embedding-3-large	64.6	87.8	54.1
text–embedding–ada–002–2	63.3	86.7	53.1
all-mpnet-base-v2	45.5	64.3	35.7
SFR-Embedding-Mistral + Llama2–13B-Encoding-str	73.6	88.8	64.3
RQ1

Can the blocking mechanism effectively include the true positive candidate in the selected top-N candidates for a given query entity? We have evaluated the worst scenario by excluding the pre-selection based on human domain knowledge. Under this condition, SFR-Embedding-Mistral achieved 88.8% MHR@10 for searching a query entity from 27,423 ECLASS entries. Although there is room for improvement in the embedding model itself, the inclusion of domain knowledge, such as filtering by specific attributes and domain classes, can further enhance the overall blocking ability to retrieve the true positive candidate in the selected top-N candidates in practice.
Given these findings, we opted to use the "SFR-Embedding-Mistral" embedding model with a top-10 candidate setting for the current implementation. This configuration feeds the top 10 candidates to the pairwise entity matcher, while also providing the remaining candidates to the user for consideration, ensuring that the true positive candidate is likely included in the selected top-N candidates for further refinement.
RQ2

Is the entity matcher necessary and effective in improving the retrieval results of blocking? Regarding the necessity, the MHR@1 results indicate that even the best model cannot often rank the true positive Entity2 at the top-1 position. The significant difference between MHR@1 and MHR@10 underscores the necessity of the subsequent entity matching step for refinement. Regarding the effectiveness, the entity matcher, based on the fine-tuned Llama2–13B-Encoding-str model, has improved the blocking results. MRR@10 improved from 68.6 to 73.6, while MHR@1 increased to 64.3. This indicates that the matcher significantly refines the ranking of the candidates, enhancing the likelihood that the true positive candidate is placed higher in the final ranking. However, the matcher has not achieved the expected high rate of MHR@1. This shortfall is partly due to the matcher often identifying more than one positive candidate among the top-10 candidates. In such cases, the first identified positive candidate is ranked at the top position, which may not always be the true positive candidate.
4.4. Exp.2 AAS-specific entity matching
4.4.1. Dataset
This experiment is performed on the PropEntity-AAS and AAS-ECLASS datasets, respectively. The statistics for these datasets are presented in Table 3. Each sample is a pair of Entity1 and Entity2, accompanied by a binary label indicating whether the pair is positive or negative. In the AAS-ECLASS dataset, Entity1 refers to AAS properties, while Entity2 refers to ECLASS entries. The construction process for the AAS-ECLASS dataset and exemplary samples have been previously introduced. Below, we briefly introduce the PropEntity-AAS dataset.
In the PropEntity-AAS dataset, Entity1 refers to proprietary entities, while Entity2 refers to AAS properties. Acquiring a comprehensive set of proprietary entities as the mapping source is a significant challenge, given that AAS is a relatively new standard and has not been widely applied in the industry. To mitigate this, three data sources have been identified. The first data source emanates from the demonstrated injection molding domain. Due to the widespread adoption of relevant domain standards such as EUROMAP, a certain degree of uniformity in equipment descriptions among manufacturers has been achieved in this domain. Manufacturers’ technical datasheets can serve as a primary source of proprietary entities to be matched with the aforementioned four injection-molding-related SMTs. The second data source is provided by one of the IDTA members who is a manufacturer of electrical connection and electronic components for automation. This company has created AAS instances of its products for customers. The related SMTs are “Contact Information”, “Technical Data”, “Digital Nameplate” and “Provision of 3D Models” [19]. The third data source stems from existing ERP software platforms such as SAP. Terms related to “purchase” processes within the software are identified as potentially alignable with SMTs focused on procurement activities, including “Purchase Order Creation,” “Purchase Request Notification,” and “Purchase Request Response” [54]. Across these three domains, 322 positive pairs were annotated manually. The approach to generating negative pairs mirrors that used in the AAS-ECLASS dataset. Instead of ECLASS entries, the blocking mechanism now targets AAS properties within the corresponding SMTs for each data source.
An example in PropEntity-AAS is shown in Table 6. Proprietary entities originate from existing software or documents and typically lack descriptive or definitional content. In this context, the semantic information is mainly carried by the property name, supplemented by “value” and “unit” attributes. This complicates the matching task due to minimal contextual information and absent domain knowledge.
Table 6. Representatives of positive pairs from the PropEntity-AAS dataset.

Proprietary entities (Entity1)	AAS property (Entity2)
name	value	unit	idShort	description
PurchaseUnit	Box	nan	OrderQuantityUnit	Unit in which the product can be ordered; only multiples of this unit can be used/ordered
4.4.2. Experiment setup and evaluation metric
The datasets undergo a train-valid-test split in a 6–2-2 ratio. The models are fine-tuned across ten epochs. The performance on evaluation sets determines the best-performing model for testing. Given the inherent imbalance between positive and negative samples typical in entity matching tasks—where negative samples vastly outnumber positive ones—the F1 score is used as the performance measure. The F1 score is the standard evaluation measure for the entity matching task. For experimental runs involving pre-trained LLMs without fine-tuning, the models are directly evaluated on the test sets.
4.4.3. Evaluated models and hyperparameters
In this experiment, the evaluated models are grouped into three categories. First, we fine-tune an open-source LLM as the generative classifier and the encoding-based classifier with different prompting and modeling options, respectively. The pre-trained Llama2 is chosen as the base model due to its widespread acceptance and proven performance [64]. Its variants like Llama2–7B and Llama2–13B, as well as chat and foundation model versions, are assessed. Key hyperparameters include the LoRa rank of weight matrices 
, the scaling factor 
 and the imbalance ratio of the training dataset 
. The rank 
 balances training efficiency with the fine-tuned model's performance. Optimal settings vary based on dataset size. For both small-scale AAS datasets, a rank of 128 is preferred for fine-tuning generative classifiers, while the rank is set to 32 for encoding-based classifiers. The scaling factor 
 acts as the scaling of the learning rate and is empirically set to 1, with an initial learning rate of 
 and a cosine learning rate scheduler. The remaining parameter settings of the trainer follow the recommendations from [58]. The imbalance ratio 
 controls the proportion of negative to positive samples, set at 3 for both AAS datasets, indicating that negative samples are three times more than positive ones. The batch size is set to 32. The maximum sequence length for the tokenizer is dynamically adjusted according to the dataset and the prompting strategy, ranging from 512 to 2048.
The second model category includes the pre-trained LLMs with prompt engineering. The Llama2-Chat-13B, Llama2-Chat-70B and the commercial GPT4-Turbo "gpt-4–1106-preview" are employed. As prompting strategies, zero-shot learning (ZSL), ICL and CoT prompting are evaluated for each model, respectively. CoT builds upon ICL by introducing intermediate reasoning steps into the prompts, emulating human-like problem-solving processes.
The third model category includes RoBERTa [65] as the representative BERT-variant model. The pre-trained RoBERTa-large with 355 M parameters stored in the Huggingface model hub is fine-tuned. The learning rate is set to 
 with a cosine learning rate scheduler. The training epoch and batch size are kept the same as those used for fine-tuning the LLMs.
The designed prompts are presented in Table 7. For fine-tuning RoBERTa, the inputs are direct Entity1 and Entity2 separated by special token 〈s〉 without any instruction, denoted as “w.o.Intr.-sptk”. For fine-tuning encoding-based LLM classifiers, there are three prompt options: without instruction (w.o.Intr.-str.), unstructured prompt with instruction (Encoding-unstr.), and structured prompt with instruction (Encoding-str.). There are two prompt options for fine-tuning the generative LLM classifier, i.e. “unstr.” and ”str”. To evaluate pre-trained LLMs with prompt engineering, the prompts for ZSL, ICL, and CoT are designed. The prompt for ZSL is the same as that of “str.”. For ICL and CoT, three matched and unmatched examples are randomly selected from the training dataset.
Table 7. Different prompting strategies.

Prompting strategy	Prompts
w.o.Intr.-str.	<entity1> … 〈/entity1〉 〈entity2〉 … </entity2>
w.o.Intr.-sptk	<s> entity1: … 〈/s〉 〈s〉 entity2:… </s>
unstr.	instruction: You will read two sentence-like entities to be matched….
entity1: …. entity2:… Response:
str./ZSL	<instruction> You will read two sentence-like entities to be matched…. 〈/instruction〉
<input> 〈entity1〉 … 〈/entity1〉 〈entity2〉 … 〈/entity2〉 〈/input〉
<response> 〈/response〉.
CoT	<instruction> You will read two sentence-like entities to be matched…. Below are several examples…. 〈/instruction〉
<input> 〈entity1〉 … 〈/entity1〉 〈entity2〉 … 〈/entity2〉 〈/input〉
<response> 〈/response〉.
CoT	<instruction> You will read two sentence-like entities to be matched….
Think step by step. First, entities may be professional terminologies in specific domains, you should consider the domain knowledge. Second, the entity names and descriptions or definitions are most important. should consider synonyms. Third, entity1 may be defined for a specific use case or domain, while entity2 may be defined in more general terms. If the scope of Entity1 belongs to that of Entity2, they should be considered matching. However, if the scope of Entity2 belongs to that of Entity1, they should be considered not matching. Below are several examples
Below are several examples…. 〈/instruction〉
<input> 〈entity1〉 … 〈/entity1〉 〈entity2〉 … 〈/entity2〉 〈/input〉
<response> 〈/response〉.
4.4.4. Results and discussions
The experimental results are selectively presented in several comparison groups in Table 8, providing insights into the performance of different models and configurations. First, we investigate which base model is most suitable. For training a generative classifier, the chat model significantly outperforms the foundation model. In contrast, for training an encoding-based model, the foundation model performs slightly better than the chat model.
Table 8. Comparison results of different models with different settings on the AAS-specific datasets.

Comparison	Model	PropEntity-AAS	AAS-ECLASS
Chat vs. foundation model	Llama2-Chat-13B-Gen.-str	80.3	76.5
Llama2–13B-Gen.-str	26.0	32.3
Llama2-Chat-13B-Encoding-str.	76.8	75.0
Llama2–13B-Encoding-str.	77.5	76.7
Str. vs. unstr.	Llama2-Chat-13B-Gen.-str	80.3	76.5
Llama2-Chat-13B-Gen.-unstr	67.3	72.4
Llama2–13B-Encoding-str.	77.5	76.7
Llama2–13B-Encoding-unstr.	66.2	58.5
Llama2–13B-Encoding-w.o.Intr-str	74.4	72.5
Last token vs. mean pooling	Llama2–13B-Encoding-str.	77.5	76.7
Llama2–13B-Encoding-str.-mean_pooling	38.9	47.5
13B vs. 7B	Llama2-Chat-13B-Gen.-str.	80.3	76.5
Llama2-Chat-7B-Gen.-str.	79.0	75.5
Llama2–13B-Encoding-str	77.5	76.7
Llama2–7B-Encoding-str	65.4	69.7
Fine-tuned vs. Pre-trained	Llama2-Chat-13B-Gen.-str	80.3	76.5
Llama2-Chat-13B-ZSL	25.3	25.4
ZSL vs. ICL. vs. CoT	GPT4-ZSL	28.9	55.5
GPT4-ICL	68.4	71.9
GPT4-CoT	72.5	75.5
Llama2–70B-ZSL	39.3	29.0
Llama2–70B-ICL	34.2	30.8
Llama2–70B-CoT	58.1	46.5
Fine-tuned LLM vs prompt engineering vs. RoBERTa	Llama2-Chat-13B-Gen.-str.	80.3	76.5
Llama2–13B-Encoding-str.	77.5	76.7
GPT4-CoT	72.5	75.5
RoBERTa-large	71.9	65.2
Next, we compare the influence of prompt options for both the generative classifier and the encoding-based classifier. The structured prompt improves the performance of both classifiers. For the encoding-based classifier, we also test the influence of the instruction by comparing Llama2–13B-Encoding-str. and Llama2–13B-Encoding-w.o.Intr. The latter performs slightly worse. Additionally, we test two types of pooling layers and find that using the last token embedding as the sentence representation significantly overperforms the mean pooling. Based on these findings, we identify the best generative classifier Llama2-Chat-13B-Gen.-str. and the best encoding-based classifier Llama2–13B-Encoding-str. When comparing model sizes, the performance drop is not substantial for the generative model when reducing from 13B to 7B, but it is significant for the encoding-based model.
To evaluate the necessity of fine-tuning an LLM, we first compare the fine-tuned Llama2 with the pre-trained version using the same prompt, i.e. ZSL in a structured format. Without fine-tuning, Llama2-Chat-13B-ZSL cannot perform well for classification. We further enhance the pre-trained model with ICT and CoT prompting. The designed CoT prompt significantly improves the performance of both Llama2 and GPT-4, with GPT-4 performing substantially better than Llama2. Additionally, GPT-4 consistently follows the desired output format provided in the prompt (e.g., "yes" or "no"), requiring no further post-processing. However, Llama2 often produces inconsistent outputs such as „yes/no 〈/response〉 Yes, the two entities are matched“, which complicates the extraction of predicted labels and costs more output tokens.
Finally, we compare the fine-tuned generative classifier Llama2-Chat-13B-Gen.-str and the encoding-based classifier Llama2–13B-Encoding-str with GPT-4-CoT and the fine-tuned RoBERTa-large model. Llama2-Chat-13B-Gen.-str achieves the highest F1-score on the PropEntity-AAS dataset, while Llama2–13B-Encoding-str is slightly better on the AAS-ECLASS dataset. Both fine-tuned Llama2 classifiers overperform GPT-4-CoT and RoBERTa-large. The fine-tuned RoBERTa-large performs significantly worse in comparison.
We also assess inference time: Llama2-Chat-13B-Gen.-str (0.75 s/query), Llama2–13B-Encoding-str (0.12 s/query), GPT-4-CoT (1.19 s/query), and RoBERTa-large (0.15 s/query). Notablely, the inference time of Llama2-Chat-13B-Gen.-str highly depends on the max. number of output tokens. In our case, for the fine-tuned models, the max. token number is set to 10. The inference time of GPT4-CoT depends on the latency of the internet connection to the LLM server (in our case, the Azure OpenAI service). The encoding-based classifier is 6 times faster than the generative one and 10 times faster than GPT4. Surprisingly, it is even faster than RoBERTa-large. This may be caused by different usage of GPU devices on the server and may cause misleading results.
RQ3

Which fine-tuned LLM classifier performs better for entity matching, the generative classifier or the encoding-based classifier? In terms of performance, the LLM generative classifier achieves slightly better results. However, the inference efficiency of the encoding-based classifier is much better.
RQ4

Is it worthwhile to fine-tune open-source LLMs compared to using pre-trained LLMs with prompt engineering and fine-tuned BERT variants? The experimental results indicate that fine-tuning open-source LLMs yields superior performance compared to pre-trained LLMs with prompt engineering and fine-tuned RoBERTa. If a specific task has to be repeated many times, we recommend fine-tuning an LLM for this task. Otherwise, GPT4-CoT is also a good option.
4.5. Exp.3 general entity matching
4.5.1. Dataset
In this experiment, five entity-matching benchmarking datasets are included: Abt-Buy and Walmart-Amazon (W.-A.) from the e-commerce domain, Amazon-Google (A.-G.) from the software product domain, DBLP-Scholar and DBLP-ACM from the bibliographic domain [66], as presented in Table 3.
Table 9 showcases three representative pairs from these datasets. Abt-Buy features entity pairs with the attribute "description" that can range from lengthy textual descriptions to being completely absent. Despite this variability, positive entity pairs in Abt-Buy often have similar product names, which may provide some leverage in matching. Entities in W.-A. and A.-G. are structured with multiple attributes, each described by a few terms. Even positive pairs can have significantly different attribute values, such as in "category" and "modelno". This noisy information makes these two datasets particularly challenging for entity matching. DBLP-Scholar and DBLP-ACM contain pairs of academic papers. The presence of attributes like “paper title” that are mostly identical in positive pairs simplifies the matching process compared to the other datasets.
Table 9. Representatives of positive pairs from entity-matching datasets.

Abt-Buy	Attributes and exemplary values
Entity 1	<name> lg washer dryer white stacking kit wstk1 〈des〉 stack dryer on top of the washer free up much-needed space in the laundry room white finish <price>186
Entity 2	<name> lg electronics lg wstk1 washer/dryer stacking kit: white 〈des〉 〈price〉
W.-A.	Attributes and exemplary values
Entity 1	<title> sumdex slr camera sling pack 〈cat〉 mp3 accessories 〈brand〉 sumdex 〈modelno〉 144,952 〈price〉 39.99
Entity 2	<title> slr camera sling pack 〈cat〉 cases bags 〈brand〉 sumdex 〈modelno〉 poc-484bk 〈price〉 44.59
DBLP-ACM	Attributes and exemplary values
Entity 1	<title> comprehension syntax 〈author〉 limsoon wong…<venue> sigmod record 〈year〉 1994
Entity 2	<title> comprehension syntax 〈author〉 peter buneman…<venue> acm sigmod record 〈year〉 1994
4.5.2. Experiment setup and evaluation metric
The train-valid-test split is pre-given in all the datasets [66], undergoing a 6–2-2 ratio for fine-tuning LLMs across ten epochs. For experimental runs without fine-tuning, the models are directly evaluated on the test sets. The F1 score is the evaluation measure.
4.5.3. Evaluated models and hyperparameters
The performance of the fine-tuned Llama2 models is compared with SOTA BERT-variant-based approaches, including DeepMatcher+ (DM+) [36], Ditto [37], RoBERTa [38], and DAEM [39], and SOTA pre-trained LLM with prompt engineering, including GPT4-ICL [43] APrompt4EM [44], COMEM [45]. Except for GPT4-ICL [43], we use the best F1 scores reported in the original papers of these SOTA approaches, as they claimed to use the same train-validation-test split. Given that GPT4-ICL [43] used a sub-sampled test set, we re-implemented both GPT4-ICL and GPT4-ZSL. It is worth noting that the results achieved by BERT-variant-based approaches involve data augmentation and other pre-processing tricks that enrich the training datasets, especially positive samples, and thus boost the model performance. In this context, we implement RoBERTa using the same pipeline as fine-tuning LLMs. Furthermore, we tested Llama2-Chat-70B-CoT and Llama2-Chat-13B-ZSL on all the datasets to evaluate the impact of fine-tuning.
For fine-tuning Llama2, the LoRa rank 
 is set to 512 for the generative classifier and 32 for the encoding-based classifier. The imbalance ratio 
 is set to 1 for the DBLP-Scholar dataset and set to 3 for other datasets. The scaling factor 
 is set to 1, with an initial learning rate of 
 and a cosine learning rate scheduler.
4.5.4. Results and discussions
RQ5

Is the fine-tuned LLM matcher competitive against SOTA BERT-variant-based approaches [[36], [37], [38], [39]] and SOTA approaches based on pre-trained LLMs with prompt engineering [[43], [44], [45]] across general entity matching tasks? Table 10 presents the F1 scores for five datasets achieved by different approaches. Across all the datasets, Llama2-Chat-13B-Gen consistently outperforms other models. Notably, its performance on the most challenging A.-G. dataset stands out, achieving an F1 score 5.6% higher than the best-performing BERT-variant-based approach and 3.4% higher than GPT4-ICL. On the other hand, Llama2-Chat-13B-Encoding is weaker than GPT-4-ICL on the Abt-Buy dataset.
Table 10. Comparison of the fine-tuned Llama2-Chat-13B-Gen and Llama2-Chat-13B-Encoding with SOTA approaches in terms of F1 score in percentile. Bold highlights the best results.

Model	Abt-Buy	W.-A.	A.-G.	DBLP-Scholar	DBLP-ACM
Llama2-Chat-13B-Gen	95.8	90.7	81.0	96.0	99.1
Llama2-Chat-13B-Encoding	91.7	88.6	79.1	95.3	99.0
Llama2-Chat-13B-ZSL	17.6	16.1	16.3	29.9	28.9
Llama2-Chat-70B-CoT	31.3	36.5	36.2	40.7	72.2
DM+ [36]	62.8	73.6	70.7	94.7	98.5
DAEM [39]	–	87.7	73.1	97,3	98,9
Ditto [37]	89.3	86.8	75.6	95.6	99.0
RoBERTa [38]	90.9	85.9	70.2	96.0	98.8
RoBERTa-large (our)	83.8	83.5	70.9	95.1	98.1
COMEM[45]	67.9	65.6	53.8	67.4	77.4
APrompt4EM [44]	–	80.2	67.1	–	–
GPT4-ZSL (our)	82.2-	79.6	64.0	–	–
GPT4-ICL (our)	95.1-	87.2	77.6	–	–
This experiment also confirms our observations for RQ3 and RQ4. Without fine-tuning, the open-source LLM cannot perform the entity matching task effectively, even when enhanced with CoT prompting. For easier datasets, where fine-tuned BERT variants can already achieve high F1 scores, it may not be worth fine-tuning LLMs. However, the fine-tuned LLM demonstrates its advantage on the challenging datasets. Even GPT-4-ICL falls short of the performance achieved by Llama2-Chat-13B-Gen. This emphasizes the impact of fine-tuning on enhancing model performance on specific tasks, especially considering that GPT-4 may have much more parameters than Llama2–13B
5. Conclusions
This work presents a novel DDMS that leverages a fine-tuned open-source LLM for the automated creation of AAS instances as DTs. This system is designed to achieve interoperable knowledge representation through AAS-based DTs It facilitates data integration from established software systems within the manufacturing sector, ensuring data consistency across AAS instances and their originating software sources. Additionally, an essential aspect of DDMS is the enhancement of semantic interoperability, achieved by automatically aligning AAS properties with standardized vocabulary dictionaries such as ECLASS.
The core component of DDMS is an LLM-based entity matcher, where the data mapping problem was formulated as a binary classification problem. We empirically investigated the potential of fine-tuning LLMs as generative classifiers and encoding-based classifiers, respectively. Their performance was analyzed through comparison with pre-trained LLMs using advanced prompt engineering techniques and classical fine-tuned BERT-variant models. The experimental findings offer meaningful insights for deploying LLMs within domain-specific applications.
Overall, fine-tuned open-source LLMs demonstrate superior performance compared to fine-tuned BERT-variant models and pre-trained LLMs with prompt engineering for entity matching. Even the smaller variant of the open-source LLM, Llama2–7B, showcases remarkable potential after fine-tuning in domain-specific tasks. This result is particularly impressive, considering it surpassed the performance of the powerful GPT-4 with trillions of parameters. However, the process of fine-tuning LLMs demands substantial resources and expertise. In contexts where fine-tuning LLMs is impractical due to resource constraints, GPT-4 with advanced prompting strategies like CoT prompting, emerges as a viable alternative. Without this prompt engineering work, GPT-4 alone with ZSL cannot perform very well on domain-specific tasks.
Although the current decode-only LLMs are designed for generative tasks, they can still be fine-tuned as encoding-based classifiers. Their performance is slightly weaker than generative classifiers but enables much faster inference.
In the current work, entity matching is formulated as a pairwise binary classification task. However, we observe that, when the matcher is applied on the similar candidates retrieved by the blocking function, the matcher cannot always accurate identify the single positive one out of all the similar candidates. This may be due to the lack of contrastive learning during the training stage so that the matcher cannot effectively distinguish the similar candidates. Furthermore, the current pairwise classification setting is inefficient because the query entity must be repeatedly compared to each retrieved candidate entity, leading to increased computational overhead. In future work, we will explore the potential of fine-tuning LLM as the encoding-based selective classifier to directly select the best candidate from the given query and all the candidates. In addition, the proposed DDSM will be developed further into a practical tool. The AAS-related datasets will be expanded to enrich the resources available to the AAS community.

